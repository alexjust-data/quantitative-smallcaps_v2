# Contexto Completo del Proyecto - Trading Smallcaps ML Pipeline

**Para:** Asistente de IA (creación de scripts de análisis/verificación)
**Fecha:** 2025-10-22
**Proyecto:** Pipeline cuantitativo ML para trading de small caps US
**Alcance:** 2020-01-03 a 2025-10-21 (5.8 años)

---

## RESUMEN EJECUTIVO

Pipeline completo de 4 fases que procesa:
- **1,906 tickers** únicos (small caps US)
- **11,054 ticker-days** (solo días "info-rich": RVOL≥2.0, |%chg|≥15%, $vol≥$5M)
- **77,382 archivos** totales
- **~4 GB** de datos (raw + processed)
- **1,622,333 eventos ML** listos para entrenar

---

## ESTRUCTURA COMPLETA DE DATOS

### FASE A: Universo y Referencia

#### 1. `raw/polygon/reference/`
**Descripción:** Datos de referencia de tickers desde Polygon API
**Script creador:** `scripts/fase_A_Universo/ingest_reference_universe.py`
**Estructura:**
```
raw/polygon/reference/
├── tickers_cs_arcx.parquet       # Tickers NASDAQ/NYSE Arca Common Stock
├── splits_cs_arcx.parquet        # Histórico de splits
├── dividends_cs_arcx.parquet     # Histórico de dividendos
└── ticker_details/               # Detalles por ticker (opcional)
```

**Schema `tickers_cs_arcx.parquet`:**
```
ticker: String               # Symbol (ej: "AAPL")
name: String                 # Nombre compañía
market: String               # "stocks"
locale: String               # "us"
primary_exchange: String     # "XNAS", "ARCX", etc.
type: String                 # "CS" (Common Stock)
active: Boolean              # Si está activo
currency_name: String        # "usd"
last_updated_utc: DateTime   # Última actualización
```

**Schema `splits_cs_arcx.parquet`:**
```
ticker: String
execution_date: Date
split_from: Float64         # Numerador del split
split_to: Float64           # Denominador (ej: 2:1 → from=2, to=1)
```

**Schema `dividends_cs_arcx.parquet`:**
```
ticker: String
ex_dividend_date: Date
cash_amount: Float64
declaration_date: Date
pay_date: Date
record_date: Date
frequency: Int64
```

---

#### 2. `processed/universe/info_rich/`
**Descripción:** Universo de tickers que tuvieron al menos 1 día info-rich
**Script creador:** `scripts/fase_C_ingesta_tiks/extract_info_rich_tickers.py`
**Estructura:**
```
processed/universe/info_rich/
└── info_rich_tickers_20200101_20251021.csv   # 1,906 tickers
```

**Schema `info_rich_tickers_*.csv`:**
```
ticker: String              # Un ticker por línea
```

**Origen:** Derivado de watchlists diarias (días con RVOL≥2.0, |%chg|≥15%, $vol≥$5M)

---

### FASE B: OHLCV Daily e Intraday

#### 3. `raw/polygon/daily_ohlcv/` (VACÍO en tu setup actual)
**Descripción:** OHLCV diario de Polygon
**Script creador:** `scripts/fase_B_ingesta_Daily_minut/ingest_ohlcv_daily.py`
**Estado:** No utilizado en pipeline actual (se usa `ohlcv_daily` en su lugar)

---

#### 4. `raw/polygon/ohlcv_daily/`
**Descripción:** OHLCV diario optimizado (datos mensuales + ZSTD)
**Script creador:** `scripts/fase_B_ingesta_Daily_minut/ingest_ohlcv_daily.py`
**Estructura:**
```
raw/polygon/ohlcv_daily/
└── {ticker}/
    └── month={YYYY-MM}/
        └── daily.parquet        # Barras diarias del mes
```

**Schema `daily.parquet`:**
```
t: DateTime(ms)            # Timestamp de la barra
o: Float64                 # Open
h: Float64                 # High
l: Float64                 # Low
c: Float64                 # Close
v: Int64                   # Volume (shares)
vw: Float64                # VWAP
n: Int64                   # Number of trades
```

**Ejemplo:** `raw/polygon/ohlcv_daily/AAPL/month=2024-10/daily.parquet`

---

#### 5. `raw/polygon/ohlcv_intraday_1m/`
**Descripción:** OHLCV intradía 1-minuto optimizado (rangos mensuales + ZSTD + adaptive rate limiting)
**Script creador:** `scripts/fase_B_ingesta_Daily_minut/ingest_ohlcv_intraday_minute.py`
**Estructura:**
```
raw/polygon/ohlcv_intraday_1m/
└── {ticker}/
    └── month={YYYY-MM}/
        └── 1m.parquet           # Barras 1-min del mes
```

**Schema `1m.parquet`:**
```
t: DateTime(ms)            # Timestamp de la barra (1-min)
o: Float64                 # Open
h: Float64                 # High
l: Float64                 # Low
c: Float64                 # Close
v: Int64                   # Volume
vw: Float64                # VWAP
n: Int64                   # Number of trades
```

**Optimizaciones aplicadas:**
- Descarga por rangos mensuales (evita límites de API)
- ZSTD compression level 2
- Adaptive rate limiting (starts 0.15s, speeds up to 0.08s)

---

### FASE C: Ticks/Trades e Info-Rich Universe

#### 6. `processed/daily_cache/`
**Descripción:** Cache diario de OHLCV para construcción rápida de universo dinámico
**Script creador:** `scripts/fase_C_ingesta_tiks/build_daily_cache.py`
**Estructura:**
```
processed/daily_cache/
└── date={YYYY-MM-DD}/
    └── cache.parquet            # OHLCV de TODOS los tickers para ese día
```

**Schema `cache.parquet`:**
```
ticker: String
date: Date
o: Float64
h: Float64
l: Float64
c: Float64
v: Int64
vw: Float64
prev_close: Float64              # Close del día anterior
```

**Uso:** Permite calcular RVOL, %chg, $vol para todos los tickers en un solo scan (vs millones de archivos individuales)

**Total:** 2,857 archivos (días de trading 2020-2025)

---

#### 7. `processed/universe/dynamic/`
**Descripción:** Watchlists diarias de tickers "info-rich"
**Script creador:** `scripts/fase_C_ingesta_tiks/build_dynamic_universe.py` (versión optimizada)
**Estructura:**
```
processed/universe/dynamic/
└── date={YYYY-MM-DD}/
    └── watchlist.parquet        # Tickers que cumplen filtros ese día
```

**Schema `watchlist.parquet`:**
```
ticker: String
date: Date
close: Float64
volume: Int64
rvol: Float64                    # Relative volume vs EMA(20)
pct_change: Float64              # % de cambio vs prev_close
dollar_volume: Float64           # close × volume
market_cap: Float64 (optional)   # Si está disponible
```

**Filtros aplicados:**
- RVOL ≥ 2.0 (2x volumen normal)
- |%chg| ≥ 15% (movimiento significativo)
- $vol ≥ $5M (liquidez mínima)

---

#### 8. `raw/polygon/trades/` ⭐
**Descripción:** Ticks raw (tick-level trades) solo para días info-rich
**Script creador:** `scripts/fase_C_ingesta_tiks/download_trades_optimized.py`
**Retry script:** `scripts/fase_C_ingesta_tiks/retry_failed_trades.py` (con timestamp range splitting)
**Estructura:**
```
raw/polygon/trades/
└── {ticker}/
    └── date={YYYY-MM-DD}/
        ├── trades.parquet       # Ticks del día
        └── _SUCCESS             # Marcador de descarga completa
```

**Schema `trades.parquet`:**
```
t: Datetime(us)                  # Timestamp (microsegundos, convertido de nanosegundos)
p: Float64                       # Price
s: Int64                         # Size (shares)
c: List(Int64)                   # Conditions (Polygon flags)
exchange: Int64                  # Exchange ID
id: String                       # Trade ID único
participant_timestamp: Int64     # Timestamp participante (nanosegundos)
sequence_number: Int64           # Número de secuencia
tape: Int64                      # Tape (A/B/C)
trf_id: Int64                    # TRF ID
trf_timestamp: Int64             # TRF timestamp (nanosegundos)
```

**Métricas:**
- **1,906 tickers**
- **11,054 ticker-days**
- **11,054 archivos** `trades.parquet`
- **11,054 marcadores** `_SUCCESS`
- **Tamaño total:** ~2.6 GB
- **Promedio:** ~42,101 trades/archivo, 0.68 MB/archivo

**Optimizaciones aplicadas:**
- Descarga solo días info-rich (del watchlist)
- Timestamp range splitting en retry (4 rangos de 6h para evitar cursor bugs de Polygon API)
- Idempotencia con `_SUCCESS` markers

**Fix crítico aplicado:** Conversión de timestamps de nanosegundos a microsegundos (línea 57-63 de `build_bars_from_trades.py`)

---

### FASE D: Barras Informacionales + ML

#### 9. `processed/bars/` ⭐
**Descripción:** Dollar Imbalance Bars (DIB) construidas desde ticks
**Script creador:** `scripts/fase_D_creando_DIB_VIB/build_bars_from_trades.py`
**Estructura:**
```
processed/bars/
└── {ticker}/
    └── date={YYYY-MM-DD}/
        ├── dollar_imbalance.parquet
        └── _SUCCESS
```

**Schema `dollar_imbalance.parquet`:**
```
t_open: Int64                    # Timestamp apertura barra (microsegundos)
t_close: Int64                   # Timestamp cierre barra (microsegundos)
o: Float64                       # Open price
h: Float64                       # High price
l: Float64                       # Low price
c: Float64                       # Close price
v: Int64                         # Volume (shares acumulado)
n: Int64                         # Number of trades en la barra
dollar: Float64                  # Dollar flow acumulado (Σ p×s)
imbalance_score: Float64         # Promedio de tick signs (+1 uptick, -1 downtick, 0 equal)
```

**Parámetros de construcción:**
- **Target USD:** $300,000 por barra
- **EMA window:** 50 (suavizado adaptativo del umbral)
- **Tick rule:** `sign = +1` si `p > p_prev`, `-1` si `p < p_prev`, `0` si igual

**Métricas:**
- **1,906 tickers**
- **11,054 ticker-days**
- **~1,193,095 barras totales** (~108 barras/ticker-day promedio)
- **Tamaño total:** ~77 MB
- **Tiempo de construcción:** 12.5 minutos (14.7 ticker-days/seg)

**Relación con trades:**
- Cada barra agrupa N trades hasta que `Σ(p×s) ≥ threshold`
- Threshold adaptativo vía EMA para suavizar ritmo

---

#### 10. `processed/labels/` ⭐
**Descripción:** Etiquetas de Triple Barrier (PT/SL/Vertical) por cada barra
**Script creador:** `scripts/fase_D_creando_DIB_VIB/triple_barrier_labeling.py`
**Estructura:**
```
processed/labels/
└── {ticker}/
    └── date={YYYY-MM-DD}/
        └── labels.parquet
```

**Schema `labels.parquet`:**
```
anchor_ts: Int64                 # Timestamp del evento ancla (= t_close de la barra)
t1: Int64                        # Timestamp cuando se tocó alguna barrera
pt_hit: Boolean                  # ¿Se tocó profit-target primero?
sl_hit: Boolean                  # ¿Se tocó stop-loss primero?
label: Int64                     # +1 (PT), -1 (SL), 0 (vertical barrier)
ret_at_outcome: Float64          # Retorno real al momento del outcome
vol_at_anchor: Float64           # Volatilidad estimada en el momento del ancla
```

**Lógica Triple Barrier:**
- **Profit Target:** `price_anchor × (1 + 3.0 × σ)`
- **Stop Loss:** `price_anchor × (1 - 2.0 × σ)`
- **Vertical Barrier:** 120 barras hacia adelante
- **Volatilidad (σ):** `EMA(|log_returns|, 50)`

**Labels:**
- `+1`: Profit-target se tocó primero
- `-1`: Stop-loss se tocó primero
- `0`: Ninguno se tocó en 120 barras (vertical barrier)

**Métricas:**
- **11,054 archivos labels**
- **Tiempo construcción:** 3.9 minutos (47.2 archivos/seg)

**Join con barras:** `labels.anchor_ts = bars.t_close` (1:1)

---

#### 11. `processed/weights/` ⭐
**Descripción:** Pesos de muestra para ML (unicidad temporal + magnitud + time-decay)
**Script creador:** `scripts/fase_D_creando_DIB_VIB/make_sample_weights.py`
**Estructura:**
```
processed/weights/
└── {ticker}/
    └── date={YYYY-MM-DD}/
        └── weights.parquet
```

**Schema `weights.parquet`:**
```
anchor_ts: Datetime(us)          # Timestamp del evento (para join)
weight: Float64                  # Peso normalizado del sample
```

**Fórmula de cálculo:**
```
weight[i] = (|ret_at_outcome[i]| / concurrency[i]) × decay[i]

Donde:
- concurrency[i] = # de ventanas [anchor_ts, t1] que contienen evento i
- decay[i] = 0.5 ^ (age_days / 90)  [time-decay, half-life 90 días]
- Normalización: Σ(weights) = 1.0 dentro de cada archivo (por ticker-day)
```

**Objetivo:**
- **Unicidad temporal:** Eventos solapados (no independientes) tienen menos peso
- **Magnitud:** Movimientos grandes aportan más señal
- **Recencia:** Datos recientes son más relevantes

**Métricas:**
- **11,054 archivos weights**
- **Tiempo construcción:** 3.7 minutos (49.9 archivos/seg)

**Join con labels:** `weights.anchor_ts = labels.anchor_ts` (1:1)

---

#### 12. `processed/datasets/daily/` ⭐
**Descripción:** Datasets ML por ticker-day (bars + labels + weights + features)
**Script creador:** `scripts/fase_D_creando_DIB_VIB/build_ml_daser.py`
**Estructura:**
```
processed/datasets/daily/
└── {ticker}/
    └── date={YYYY-MM-DD}/
        └── dataset.parquet      # 23 columnas: labels + features + weight
```

**Schema `dataset.parquet` (23 columnas):**

**Labels (de `labels.parquet`):**
```
anchor_ts: Int64                 # Timestamp del evento
t1: Int64                        # Timestamp del outcome
pt_hit: Boolean                  # ¿Tocó PT?
sl_hit: Boolean                  # ¿Tocó SL?
label: Int64                     # +1 / -1 / 0
ret_at_outcome: Float64          # Retorno real
vol_at_anchor: Float64           # Volatilidad estimada
```

**Features (engineered desde `dollar_imbalance.parquet`):**
```
c: Float64                       # Close price de la barra
ret_1: Float64                   # log(c / c_prev)
range_norm: Float64              # (H - L) / |C_prev|
vol_f: Float64                   # Volume (shares)
dollar_f: Float64                # Dollar flow
imb_f: Float64                   # Imbalance score
ret_1_ema10: Float64             # EMA(10) de ret_1
ret_1_ema30: Float64             # EMA(30) de ret_1
range_norm_ema20: Float64        # EMA(20) de range_norm
vol_f_ema20: Float64             # EMA(20) de volume
dollar_f_ema20: Float64          # EMA(20) de dollar flow
imb_f_ema20: Float64             # EMA(20) de imbalance
vol_z20: Float64                 # Z-score volume (ventana 20)
dollar_z20: Float64              # Z-score dollar (ventana 20)
n: Int64                         # Number of trades en la barra
```

**Weight:**
```
weight: Float64                  # Sample weight (de weights.parquet)
```

**Join logic:**
```python
dataset = bars.join(labels, left_on="t_close", right_on="anchor_ts")
              .with_features(...)  # Feature engineering
              .join(weights, on="anchor_ts")
```

**Métricas:**
- **11,054 archivos dataset.parquet**
- **Tiempo construcción:** 4.7 minutos (daily files)

---

#### 13. `processed/datasets/global/dataset.parquet` ⭐
**Descripción:** Dataset ML global (concatenación de todos los daily)
**Script creador:** `scripts/fase_D_creando_DIB_VIB/build_ml_daser.py` (fase concat)
**Schema:** Idéntico al daily (23 columnas)

**Métricas:**
- **1 archivo único**
- **1,622,333 rows** (eventos ML)
- **Tamaño:** 183.2 MB
- **Tiempo construcción:** ~4.3 minutos (concatenación + sort)

**Ordenamiento:** Por `anchor_ts` (cronológico)

---

#### 14. `processed/datasets/splits/` ⭐
**Descripción:** Train/Valid splits con walk-forward temporal
**Script creador:** `scripts/fase_D_creando_DIB_VIB/build_ml_daser.py` (fase split)
**Estructura:**
```
processed/datasets/splits/
├── train.parquet                # 1,297,816 rows (79.9%)
└── valid.parquet                # 324,467 rows (20.1%)
```

**Schema:** Idéntico al global (23 columnas)

**Parámetros del split:**
- **Método:** Walk-forward (temporal)
- **Folds:** 5
- **Purge gap:** 50 barras entre train/valid (evita leakage)
- **Train:** Primeros 4 folds (80% - purge)
- **Valid:** Último fold (20%)

**Walk-Forward Logic:**
```python
# Divide timeline en 5 segmentos uniformes
n_rows = 1,622,333
segment_size = n_rows / 5 = 324,467

# Train = segmentos 1-4, menos purge
train_end = (4 × segment_size) - 50
train = dataset[0 : train_end]       # 1,297,816 rows

# Valid = segmento 5
valid_start = 4 × segment_size
valid = dataset[valid_start : ]      # 324,467 rows
```

**Tamaños:**
- `train.parquet`: 146.9 MB
- `valid.parquet`: 36.0 MB

---

#### 15. `processed/datasets/meta.json`
**Descripción:** Metadata del pipeline completo
**Script creador:** `scripts/fase_D_creando_DIB_VIB/build_ml_daser.py`

**Contenido:**
```json
{
  "created_at": "2025-10-22T20:02:06",
  "bars_root": "processed/bars",
  "labels_root": "processed/labels",
  "weights_root": "processed/weights",
  "outdir": "processed/datasets",
  "bar_file": "dollar_imbalance.parquet",
  "tasks": 11054,
  "daily_files": 11054,
  "global_rows": 1622333,
  "split": "walk_forward",
  "folds": 5,
  "purge_bars": 50,
  "train_rows": 1297816,
  "valid_rows": 324467,
  "feature_columns_example": [
    "ret_1", "range_norm", "vol_f", "dollar_f", "imb_f",
    "ret_1_ema10", "ret_1_ema30", "range_norm_ema20",
    "vol_f_ema20", "dollar_f_ema20", "imb_f_ema20",
    "vol_z20", "dollar_z20", "n"
  ],
  "label_column": "label",
  "weight_column": "weight",
  "time_index": "anchor_ts"
}
```

---

## RELACIONES Y FLUJO DE DATOS

### Flujo completo (de ticks a ML dataset):

```
1. raw/polygon/trades/{ticker}/date={day}/trades.parquet
   ↓ (build_bars_from_trades.py)
2. processed/bars/{ticker}/date={day}/dollar_imbalance.parquet
   ↓ (triple_barrier_labeling.py)
3. processed/labels/{ticker}/date={day}/labels.parquet
   ↓ (make_sample_weights.py)
4. processed/weights/{ticker}/date={day}/weights.parquet
   ↓ (build_ml_daser.py - feature engineering + join)
5. processed/datasets/daily/{ticker}/date={day}/dataset.parquet
   ↓ (build_ml_daser.py - concat)
6. processed/datasets/global/dataset.parquet
   ↓ (build_ml_daser.py - split)
7. processed/datasets/splits/[train.parquet, valid.parquet]
```

### Relaciones 1:1 (Primary/Foreign Keys):

```sql
-- Barras → Trades (N:M implícita por timestamp range)
bars.t_open <= trades.t <= bars.t_close

-- Labels → Barras (1:1 exacta)
labels.anchor_ts = bars.t_close

-- Weights → Labels (1:1 exacta)
weights.anchor_ts = labels.anchor_ts

-- Dataset → Barras + Labels + Weights (inner join)
dataset.anchor_ts = bars.t_close = labels.anchor_ts = weights.anchor_ts
```

---

## SCRIPTS DEL PIPELINE (por fase)

### FASE A: Universo
```
scripts/fase_A_Universo/
├── ingest_reference_universe.py     # Descarga tickers, splits, dividends
├── build_tickers_dim_scd2.py        # SCD-2 de ticker dimension (histórico)
├── ingest_ticker_details.py         # Detalles por ticker
└── tools/
    ├── validate_reference.py        # Valida datos de referencia
    └── verify_universe_filters.py   # Verifica filtros de universo
```

### FASE B: OHLCV
```
scripts/fase_B_ingesta_Daily_minut/
├── ingest_ohlcv_daily.py            # OHLCV diario (mensuales + ZSTD)
├── ingest_ohlcv_intraday_minute.py  # OHLCV 1-min (mensuales + adaptive RL)
└── tools/
    ├── analyze_ohlcv_daily_download.py
    ├── audit_intraday_completeness.py
    └── batch_intraday_wrapper.py   # Wrapper para descarga masiva
```

### FASE C: Ticks
```
scripts/fase_C_ingesta_tiks/
├── build_daily_cache.py             # Cache diario de OHLCV
├── build_dynamic_universe.py        # Universo dinámico (watchlists)
├── build_dynamic_universe_optimized.py  # Versión optimizada
├── download_trades_optimized.py     # Descarga ticks (días info-rich)
├── retry_failed_trades.py           # Retry con timestamp range splitting
└── extract_info_rich_tickers.py     # Extrae lista única de 1,906 tickers
```

### FASE D: DIB + ML
```
scripts/fase_D_creando_DIB_VIB/
├── build_bars_from_trades.py        # DIB desde ticks (FIX: timestamps ns→μs)
├── triple_barrier_labeling.py       # Labels triple barrier
├── make_sample_weights.py           # Sample weights
├── build_ml_daser.py                # Dataset ML (FIX: write_parquet syntax)
└── tools/
    └── audit_bars.py                # Audita barras generadas
```

---

## FIXES CRÍTICOS APLICADOS

### Fix 1: Timestamps nanosegundos → microsegundos
**Archivo:** `scripts/fase_D_creando_DIB_VIB/build_bars_from_trades.py` (líneas 57-63)
**Problema:** Polygon API devuelve timestamps en nanosegundos, pero Polars `iter_rows()` los interpreta como microsegundos → "year 56767 out of range"
**Solución:**
```python
t_sample = df["t"].head(1).cast(pl.Int64).item()
if t_sample > 32503680000000000:  # Jan 1, 3000 in microseconds
    df = df.with_columns((pl.col("t").cast(pl.Int64) // 1000).cast(pl.Datetime(time_unit="us")).alias("t"))
```

### Fix 2: write_parquet syntax error
**Archivo:** `scripts/fase_D_creando_DIB_VIB/build_ml_daser.py` (líneas 271-272)
**Problema:** `(Path / "file.parquet").write_parquet(df, ...)` → AttributeError
**Solución:**
```python
# ANTES:
(splits_outdir / "train.parquet").write_parquet(train, ...)

# DESPUÉS:
train.write_parquet(splits_outdir / "train.parquet", ...)
```

### Fix 3: Future dates filter
**Archivo:** `scripts/fase_C_ingesta_tiks/download_trades_optimized.py` (líneas 78-83)
**Problema:** Script intentaba descargar fechas futuras del watchlist → API 400 errors
**Solución:**
```python
today = date.today()
for day in date_range:
    if day > today:
        continue
```

---

## MÉTRICAS GLOBALES

### Archivos totales: 77,382
| Directorio | Archivos |
|------------|----------|
| `raw/polygon/trades` | 22,108 (11,054 × 2: parquet + _SUCCESS) |
| `processed/bars` | 22,108 (11,054 × 2: parquet + _SUCCESS) |
| `processed/labels` | 11,054 |
| `processed/weights` | 11,054 |
| `processed/datasets/daily` | 11,054 |
| `processed/datasets/global` | 1 |
| `processed/datasets/splits` | 2 |
| `processed/datasets/meta.json` | 1 |

### Tamaños aproximados:
- **Raw trades:** ~2.6 GB
- **Processed bars:** ~77 MB
- **Processed labels:** ~166 MB
- **Processed weights:** ~11 MB
- **Processed datasets/daily:** ~818 MB
- **Processed datasets/global:** 183 MB
- **Processed datasets/splits:** 183 MB (train+valid)
- **TOTAL:** ~4.0 GB

### Tiempos de ejecución (100% éxito):
| Etapa | Tiempo | Velocidad |
|-------|--------|-----------|
| DIB Bars | 12.5 min | 14.7 ticker-days/seg |
| Labels | 3.9 min | 47.2 archivos/seg |
| Weights | 3.7 min | 49.9 archivos/seg |
| ML Dataset | ~9 min | ~20.5 archivos/seg |
| **TOTAL** | **~29 min** | **~6.3 archivos/seg** |

---

## CASOS DE USO PARA SCRIPTS DE VERIFICACIÓN

### 1. Verificación de inventario
**Objetivo:** Contar archivos, verificar `_SUCCESS` markers, validar fechas
**Inputs:** Paths de directorios
**Checks:**
- Todos los 11,054 ticker-days tienen `trades.parquet` + `_SUCCESS`
- Todos los 11,054 ticker-days tienen `dollar_imbalance.parquet` + `_SUCCESS`
- Todos los 11,054 ticker-days tienen `labels.parquet`
- Todos los 11,054 ticker-days tienen `weights.parquet`
- Todos los 11,054 ticker-days tienen `dataset.parquet`
- Fechas están en rango [2020-01-03, 2025-10-21]
- No hay fechas futuras

### 2. Verificación de schemas
**Objetivo:** Validar columnas, tipos, nulos
**Inputs:** Sample de 30-50 archivos de cada tipo
**Checks:**
- `trades.parquet`: 11 columnas esperadas, tipos correctos
- `dollar_imbalance.parquet`: 10 columnas, sin nulos en `t_open`, `t_close`, `c`
- `labels.parquet`: 7 columnas, `label` ∈ {-1, 0, +1}
- `weights.parquet`: 2 columnas, `weight` > 0, sum(weight) ≈ 1.0 por archivo
- `dataset.parquet`: 23 columnas, sin nulos en columnas críticas

### 3. Conservación de masa (volumen/dólar)
**Objetivo:** Verificar equivalencia trades → bars
**Inputs:** Sample de ticker-days
**Checks:**
```python
# Para cada ticker-day:
trades_df = pl.read_parquet(f"trades/{ticker}/date={day}/trades.parquet")
bars_df = pl.read_parquet(f"bars/{ticker}/date={day}/dollar_imbalance.parquet")

# Volume conservation
assert abs(trades_df['s'].sum() - bars_df['v'].sum()) < 0.01

# Dollar conservation
dollar_trades = (trades_df['p'] * trades_df['s']).sum()
dollar_bars = bars_df['dollar'].sum()
assert abs(dollar_trades - dollar_bars) / dollar_trades < 0.001  # <0.1% error
```

### 4. Verificación de lógica de labels
**Objetivo:** Validar consistencia de triple barrier
**Inputs:** Sample de labels
**Checks:**
```python
labels_df = pl.read_parquet("labels.parquet")

# Solo uno de pt_hit/sl_hit puede ser true simultáneamente
assert not any(labels_df['pt_hit'] & labels_df['sl_hit'])

# label coherente con hits
assert all(labels_df.filter(pl.col('pt_hit'))['label'] == 1)
assert all(labels_df.filter(pl.col('sl_hit'))['label'] == -1)
assert all(labels_df.filter(~pl.col('pt_hit') & ~pl.col('sl_hit'))['label'] == 0)

# t1 >= anchor_ts
assert all(labels_df['t1'] >= labels_df['anchor_ts'])
```

### 5. Distribuciones estadísticas
**Objetivo:** Validar proporciones de labels, pesos, features
**Inputs:** Global dataset
**Checks:**
```python
df = pl.read_parquet("global/dataset.parquet")

# Distribución de labels (no degenerada)
label_dist = df['label'].value_counts()
print(label_dist)
# No debe haber >90% de una sola clase

# Gini de weights (no súper concentrado)
gini = compute_gini(df['weight'])
assert gini < 0.9

# Features sin nulos extremos
assert df['ret_1'].null_count() / len(df) < 0.01
```

### 6. Verificación de lineage
**Objetivo:** Validar que todos los ticker-days vienen del universo info-rich
**Inputs:** `info_rich_tickers_*.csv` + archivos generados
**Checks:**
```python
info_rich_tickers = set(pl.read_csv("info_rich_tickers_*.csv")['ticker'])
tickers_in_trades = set([d.name for d in Path('raw/polygon/trades').iterdir()])

# Todos los tickers en trades deben estar en info_rich
assert tickers_in_trades.issubset(info_rich_tickers)

# Total debe ser 1,906
assert len(info_rich_tickers) == 1906
```

---

## CONVENCIONES DE NOMENCLATURA

### Particionamiento Hive-style
```
{dataset}/{ticker}/date=YYYY-MM-DD/{filename}.parquet
```

### Timestamps
- **Formato:** Int64 (microsegundos desde epoch)
- **Conversión:** `datetime.fromtimestamp(ts / 1_000_000)`
- **Timezone:** UTC (implícito)

### Marcadores de éxito
- **`_SUCCESS`:** Archivo vacío que indica procesamiento completo
- **Uso:** Scripts verifican `_SUCCESS` antes de reprocesar (idempotencia)

### Compresión
- **Formato:** Parquet con ZSTD level 2
- **Statistics:** Disabled (para velocidad de escritura)

---

## DEPENDENCIAS DEL PROYECTO

```
Python 3.10+
polars >= 0.20.0
requests
python-dateutil
pyyaml
```

---

## DOCUMENTACIÓN RELACIONADA

- `01_DayBook/fase_01/D_creando_DIB_VIB/07_Ejecucion_Pipeline_DIB_Labels_Weights.md` - Ejecución detallada de Fase D
- `DATA_STRUCTURE_SCHEMA.md` - Esquema exhaustivo para BBDD (solo Fase D)
- `DATA_STRUCTURE_MAP.json` - Mapa completo verificado con schemas reales
- `processed/datasets/meta.json` - Metadata del pipeline ML

---

## NOTAS PARA CREACIÓN DE SCRIPTS

1. **Paths absolutos vs relativos:** Todos los scripts usan paths relativos desde `D:\04_TRADING_SMALLCAPS\`

2. **Manejo de errores:** Usar try/except con logging detallado. Exit codes ≠ 0 si falla algún check.

3. **Sampling vs full scan:** Para verificaciones rápidas, samplear 30-50 archivos. Para verificación final, scan completo.

4. **Formato de output:** JSON con timestamp, checks pasados/fallados, y métricas. Ejemplo:
```json
{
  "timestamp": "2025-10-22T21:00:00",
  "check_type": "schema_validation",
  "total_checked": 50,
  "passed": 50,
  "failed": 0,
  "details": {...}
}
```

5. **Performance:** Usar Polars para I/O de Parquet (10-100x más rápido que Pandas).

6. **Paralelismo:** Para checks sobre 11K archivos, usar `ProcessPoolExecutor` con 8-12 workers.

---

**FIN DEL CONTEXTO COMPLETO**

Este documento contiene TODA la información necesaria para crear scripts de verificación, análisis, o extensión del pipeline sin necesidad de contexto adicional.
