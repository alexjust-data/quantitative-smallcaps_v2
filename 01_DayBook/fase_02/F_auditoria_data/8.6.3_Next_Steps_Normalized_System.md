# 8.10 Pr√≥ximos Pasos - Sistema de Archivos Normalizados

**Fecha**: 2025-10-23
**Estado**: Implementaci√≥n parcial completada

---

## Resumen del Trabajo Completado

### ‚úÖ Implementaciones Finalizadas

1. **Patch en `verify_against_references.py`**:
   - Soporte para `--normalized-root`
   - L√≥gica de fallback: archivo normalizado ‚Üí API client
   - Patr√≥n de nombres: `<SYMBOL>_<DATE>_<VENDOR>_minute.(parquet|csv|json)`

2. **Script `preprocess_vendor_data.py`**:
   - Normalizaci√≥n UTC
   - Filtrado RTH (09:30-16:00 ET)
   - Soporte multi-formato (parquet, csv, json)

3. **Modificaci√≥n en `alphavantage_client.py`**:
   - Auto-guardado de raw data en `raw_data/`
   - Mensaje `[SAVED RAW]` para tracking

4. **Prueba exitosa**:
   - WOLF 2025-05-13 descargado de Alpha Vantage
   - Raw data guardado: `WOLF_2025-05-13_alphavantage_raw.json`
   - API call: 1 (controlado)

---

## ‚ö†Ô∏è Problema Detectado

### Estructura JSON de Alpha Vantage

**Formato real:**
```json
{
  "Meta Data": {...},
  "Time Series (1min)": {
    "2025-05-30 19:59:00": {
      "1. open": "1.2098",
      "2. high": "1.2199",
      "3. low": "1.2098",
      "4. close": "1.2199",
      "5. volume": "5827"
    },
    "2025-05-30 19:58:00": {...}
  }
}
```

**Problema:** Timestamps son **keys** del diccionario, no una columna. `preprocess_vendor_data.py` espera columnas est√°ndar.

**Soluci√≥n requerida:** Vendor-specific parsers en `preprocess_vendor_data.py`.

---

## üìã Tareas Pendientes

### Tarea 1: Parser Vendor-Specific en preprocess_vendor_data.py

**Objetivo:** Manejar formatos JSON espec√≠ficos de cada vendor.

**C√≥digo propuesto:**
```python
def parse_vendor_json(data: dict, vendor: str) -> pd.DataFrame:
    """Parse vendor-specific JSON structures"""

    if vendor == "alphavantage":
        # Alpha Vantage: timestamps as keys
        time_series_key = "Time Series (1min)"
        if time_series_key not in data:
            raise ValueError(f"Missing '{time_series_key}' in Alpha Vantage response")

        records = []
        for timestamp, values in data[time_series_key].items():
            records.append({
                "timestamp": timestamp,
                "open": values.get("1. open"),
                "high": values.get("2. high"),
                "low": values.get("3. low"),
                "close": values.get("4. close"),
                "volume": values.get("5. volume")
            })
        return pd.DataFrame(records)

    elif vendor == "twelvedata":
        # Twelve Data: values array
        if "values" not in data:
            raise ValueError("Missing 'values' in Twelve Data response")
        df = pd.DataFrame(data["values"])
        # Rename 'datetime' to 'timestamp' for consistency
        if "datetime" in df.columns:
            df = df.rename(columns={"datetime": "timestamp"})
        return df

    elif vendor == "polygon":
        # Polygon: results array with 't' as unix timestamp
        if "results" not in data:
            raise ValueError("Missing 'results' in Polygon response")
        df = pd.DataFrame(data["results"])
        # Convert unix nanoseconds to datetime
        if "t" in df.columns:
            df["timestamp"] = pd.to_datetime(df["t"], unit="ns", utc=True)
        return df

    else:
        # Generic: assume standard DataFrame structure
        return pd.DataFrame(data)
```

**Integraci√≥n en `normalize_vendor_data()`:**
```python
def normalize_vendor_data(input_path, vendor, ...):
    # Read input file
    if input_path.suffix.lower() == ".json":
        with open(input_path) as f:
            data = json.load(f)
        # Use vendor-specific parser
        df = parse_vendor_json(data, vendor)
    else:
        # Standard file formats
        df = pd.read_parquet/csv(input_path)

    # Continue with normalization...
```

---

### Tarea 2: Modificar Resto de Clientes para Guardar Raw Data

**Archivos a modificar:**

#### `vendors/twelvedata_client.py`
```python
# L√≠nea ~35, despu√©s de data = r.json()
raw_dir = pathlib.Path(__file__).parent.parent / "raw_data"
raw_dir.mkdir(exist_ok=True)
raw_file = raw_dir / f"{symbol}_{date}_twelvedata_raw.json"
raw_file.write_text(json.dumps(data, indent=2, ensure_ascii=False))
print(f"[SAVED RAW] {raw_file}")
```

#### `vendors/polygon_client.py`
```python
# Similar, despu√©s de obtener respuesta
raw_file = raw_dir / f"{symbol}_{date}_polygon_raw.json"
raw_file.write_text(json.dumps(data, indent=2, ensure_ascii=False))
print(f"[SAVED RAW] {raw_file}")
```

#### `vendors/fmp_client.py`
```python
# Similar (aunque 403 actualmente)
raw_file = raw_dir / f"{symbol}_{date}_fmp_raw.json"
```

---

### Tarea 3: Descarga Controlada Inicial

**Objetivo:** Descargar 1-2 tickers √ó 1 fecha √ó 2-3 vendors y normalizar.

**Plan:**
```powershell
# Paso 1: Descargar (gastar 2-3 API calls controlados)
python verify_against_references.py `
  --symbols WOLF `
  --dates 2025-05-13 `
  --vendors alphavantage,twelvedata `
  --our-dib-root "D:/04_TRADING_SMALLCAPS/processed/bars" `
  --out reports/initial_download.json

# Resultado:
# raw_data/WOLF_2025-05-13_alphavantage_raw.json ‚úÖ
# raw_data/WOLF_2025-05-13_twelvedata_raw.json ‚úÖ

# Paso 2: Normalizar (offline, 0 API calls)
python preprocess_vendor_data.py `
  --in raw_data/WOLF_2025-05-13_alphavantage_raw.json `
  --vendor alphavantage `
  --assume-tz US/Eastern `
  --rth-only `
  --out normalized/WOLF_2025-05-13_alphavantage_minute.parquet

python preprocess_vendor_data.py `
  --in raw_data/WOLF_2025-05-13_twelvedata_raw.json `
  --vendor twelvedata `
  --assume-tz UTC `
  --rth-only `
  --out normalized/WOLF_2025-05-13_twelvedata_minute.parquet

# Paso 3: Validar con archivos normalizados (0 API calls)
python verify_against_references.py `
  --symbols WOLF `
  --dates 2025-05-13 `
  --vendors alphavantage,twelvedata `
  --normalized-root normalized `
  --our-dib-root "D:/04_TRADING_SMALLCAPS/processed/bars" `
  --out reports/validation_from_normalized.json
# ‚úÖ 0 API calls - usa archivos normalizados
```

---

### Tarea 4: Validar Match Rates Mejoran con RTH Filtering

**Hip√≥tesis:** Los match rates bajos (~1.6%) pueden deberse a extended hours mismatch.

**Experimento:**
```powershell
# Test 1: Sin RTH filtering
python preprocess_vendor_data.py `
  --in raw_data/WOLF_2025-05-13_alphavantage_raw.json `
  --vendor alphavantage `
  --assume-tz US/Eastern `
  --out normalized/WOLF_2025-05-13_alphavantage_minute_FULL.parquet

python verify_against_references.py `
  --symbols WOLF `
  --dates 2025-05-13 `
  --vendors alphavantage `
  --normalized-root normalized `
  --our-dib-root "D:/04_TRADING_SMALLCAPS/processed/bars" `
  --out reports/test_full_hours.json

# Test 2: Con RTH filtering
python preprocess_vendor_data.py `
  --in raw_data/WOLF_2025-05-13_alphavantage_raw.json `
  --vendor alphavantage `
  --assume-tz US/Eastern `
  --rth-only `
  --out normalized/WOLF_2025-05-13_alphavantage_minute_RTH.parquet

python verify_against_references.py `
  --symbols WOLF `
  --dates 2025-05-13 `
  --vendors alphavantage `
  --normalized-root normalized `
  --our-dib-root "D:/04_TRADING_SMALLCAPS/processed/bars" `
  --out reports/test_rth_only.json

# Comparar match rates:
# FULL hours: 1.57% ‚Üí RTH only: ??% (esperamos mejora)
```

---

### Tarea 5: Batch Download Script

**Objetivo:** Descargar m√∫ltiples tickers respetando rate limits.

**Crear:** `batch_download_and_normalize.py`

```python
#!/usr/bin/env python3
"""
Batch download and normalize vendor data

Usage:
    python batch_download_and_normalize.py --tickers tickers.txt --date 2025-05-13 --vendors alphavantage,twelvedata
"""

import subprocess
import time
import argparse

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--tickers", required=True, help="File with ticker list (one per line)")
    parser.add_argument("--date", required=True, help="Date to download (YYYY-MM-DD)")
    parser.add_argument("--vendors", required=True, help="Comma-separated vendor list")
    parser.add_argument("--batch-size", type=int, default=5, help="Tickers per batch (respect rate limits)")
    parser.add_argument("--wait-time", type=int, default=60, help="Seconds to wait between batches")
    args = parser.parse_args()

    # Load tickers
    with open(args.tickers) as f:
        tickers = [line.strip() for line in f if line.strip()]

    vendors = args.vendors.split(",")

    print(f"Downloading {len(tickers)} tickers √ó {len(vendors)} vendors = {len(tickers) * len(vendors)} API calls")
    print(f"Batch size: {args.batch_size}, Wait time: {args.wait_time}s")

    # Process in batches
    for i in range(0, len(tickers), args.batch_size):
        batch = tickers[i:i+args.batch_size]
        batch_str = ",".join(batch)

        print(f"\n=== Batch {i//args.batch_size + 1}/{(len(tickers) + args.batch_size - 1)//args.batch_size} ===")
        print(f"Tickers: {batch_str}")

        # Download
        cmd = [
            "python", "verify_against_references.py",
            "--symbols", batch_str,
            "--dates", args.date,
            "--vendors", args.vendors,
            "--our-dib-root", "D:/04_TRADING_SMALLCAPS/processed/bars",
            "--out", f"reports/batch_{i//args.batch_size + 1}.json"
        ]
        subprocess.run(cmd, check=True)

        # Normalize each vendor's raw data
        for ticker in batch:
            for vendor in vendors:
                raw_file = f"raw_data/{ticker}_{args.date}_{vendor}_raw.json"
                norm_file = f"normalized/{ticker}_{args.date}_{vendor}_minute.parquet"

                if not Path(raw_file).exists():
                    print(f"‚ö†Ô∏è Missing raw file: {raw_file}")
                    continue

                norm_cmd = [
                    "python", "preprocess_vendor_data.py",
                    "--in", raw_file,
                    "--vendor", vendor,
                    "--assume-tz", "US/Eastern" if vendor in ["alphavantage", "fmp"] else "UTC",
                    "--rth-only",
                    "--out", norm_file
                ]
                subprocess.run(norm_cmd, check=True)
                print(f"‚úÖ Normalized: {norm_file}")

        # Wait for rate limit reset
        if i + args.batch_size < len(tickers):
            print(f"‚è≥ Waiting {args.wait_time}s for rate limit reset...")
            time.sleep(args.wait_time)

    print("\n=== Complete ===")
    print(f"Downloaded and normalized {len(tickers)} tickers")

if __name__ == "__main__":
    main()
```

**Uso:**
```powershell
# Crear lista de tickers
echo "WOLF
NVDA
AAPL
TSLA
AMC" > tickers_sample.txt

# Ejecutar batch download
python batch_download_and_normalize.py `
  --tickers tickers_sample.txt `
  --date 2025-05-13 `
  --vendors alphavantage,twelvedata `
  --batch-size 5 `
  --wait-time 60

# Resultado:
# 5 tickers √ó 2 vendors = 10 API calls
# Tiempo: ~2 minutos (1 batch, respeta rate limit)
```

---

## üéØ Prioridades

### Corto Plazo (Hoy)

1. ‚úÖ Completar parser vendor-specific en `preprocess_vendor_data.py`
2. ‚úÖ Normalizar archivo `WOLF_2025-05-13_alphavantage_raw.json` existente
3. ‚úÖ Validar con `--normalized-root` (0 API calls)
4. ‚úÖ Commit + documentaci√≥n

### Mediano Plazo (Esta Semana)

1. Modificar resto de clientes (twelvedata, polygon) para guardar raw
2. Descargar 10 tickers √ó 1 fecha √ó 2 vendors = 20 API calls
3. Normalizar todos
4. Generar reporte comparativo (RTH vs Full hours)

### Largo Plazo (Siguiente Semana)

1. Batch download de 100 tickers √ó 10 fechas = 1,000 ticker-days
2. Generar certificaci√≥n estad√≠stica multi-vendor
3. Documentar en reporte final
4. Preparar para paper/publicaci√≥n

---

## üìä M√©tricas de √âxito

### API Calls Conservados

**Antes del sistema normalizado:**
- 10 validaciones √ó 3 vendors √ó 1 ticker = 30 calls
- Rate limit: 8/min ‚Üí FALLA

**Despu√©s:**
- 1 descarga inicial = 3 calls
- 9 validaciones = 0 calls
- **Ahorro: 90%**

### Tiempo de Ejecuci√≥n

**Con API calls:**
- 100 tickers √ó 2 vendors = 200 calls
- Rate limit 5/min ‚Üí 40 minutos

**Con normalized files:**
- 100 tickers √ó 2 vendors = **~10 segundos** (lectura disco)
- **Mejora: 240x**

---

## üîó Referencias

- [8.8_New_Vendors_Integration.md](8.8_New_Vendors_Integration.md) - Integraci√≥n inicial vendors
- [8.9_Normalized_Files_Strategy.md](8.9_Normalized_Files_Strategy.md) - Estrategia completa
- [8.6.1_solutions.md](8.6.1_solutions.md) - Patch original del sistema

---

## Estado Actual

‚úÖ **Implementaci√≥n core completada**
‚úÖ **Prueba exitosa con Alpha Vantage**
‚ö†Ô∏è **Requiere parser vendor-specific** para normalizaci√≥n
üìà **Listo para escalar a gran volumen**

**Pr√≥ximo paso inmediato:** Implementar parser vendor-specific en `preprocess_vendor_data.py`.
