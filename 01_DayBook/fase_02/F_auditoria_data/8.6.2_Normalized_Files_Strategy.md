# 8.9 Estrategia de Archivos Normalizados - Conservación de API Calls

**Fecha**: 2025-10-23
**Objetivo**: Evitar gastar llamadas API innecesarias usando archivos normalizados pre-descargados

---

## 1. Problema Identificado

### Rate Limits Actuales

| Vendor | Límite | Usado | Restante | Criticidad |
|--------|--------|-------|----------|-----------|
| **Twelve Data** | 8 calls/min | 2 | 6 | ⚠️ Medio |
| **Alpha Vantage** | 5 calls/min | ? | ? | ✅ Bajo |
| **Polygon** | Unlimited (paid) | N/A | N/A | ✅ Bajo |
| **FMP** | N/A (403) | N/A | N/A | ❌ No funcional |

### Problema

Cada vez que ejecutamos `verify_against_references.py`, el script hace **1 API call por (ticker × date × vendor)**:

```
Ejemplo actual:
--symbols WOLF,NVDA,AAPL (3 tickers)
--dates 2025-05-13,2025-05-14 (2 dates)
--vendors twelvedata,alphavantage (2 vendors)

Total API calls = 3 × 2 × 2 = 12 calls

Si Twelve Data tiene límite de 8/min → FALLA
Si Alpha Vantage tiene límite de 5/min → FALLA
```

---

## 2. Solución: Sistema de Archivos Normalizados

### Concepto

**Descargar una vez, usar infinitas veces:**

1. **Descarga inicial** (1 API call) → Guardar respuesta raw
2. **Normalización** (offline, 0 API calls) → Convertir a formato estándar UTC
3. **Validaciones** (0 API calls) → Leer archivos normalizados en lugar de llamar API

### Arquitectura

```
Flujo anterior (cada validación = API calls):
verify_against_references.py → twelvedata_client.py → API call → 1 call gastado

Flujo nuevo (primera vez):
1. twelvedata_client.py → API call → Guardar raw data → 1 call gastado
2. preprocess_vendor_data.py → Leer raw → Normalizar → Guardar normalized → 0 calls
3. verify_against_references.py → Leer normalized → 0 calls

Flujo nuevo (siguientes veces):
verify_against_references.py → Leer normalized → 0 calls ✅
```

---

## 3. Implementación Aplicada

### Cambios en `verify_against_references.py`

**Nuevo parámetro:**
```bash
--normalized-root <path>
```

**Lógica:**
```python
def load_vendor_frame(vendor, symbol, date, ..., normalized_root=None):
    # 1) Buscar archivo normalizado primero
    if normalized_root:
        path = f"{normalized_root}/{symbol}_{date}_{vendor}_minute.parquet"
        if path exists:
            return load_normalized(path)  # 0 API calls ✅

    # 2) Si no existe, fallback a API client
    return fetch_from_api(vendor, symbol, date)  # 1 API call
```

**Patron de nombres:**
```
<SYMBOL>_<DATE>_<VENDOR>_minute.(parquet|csv|json)

Ejemplos:
WOLF_2025-05-13_twelvedata_minute.parquet
NVDA_2025-05-14_alphavantage_minute.csv
AAPL_2025-05-13_polygon_minute.json
```

---

## 4. Workflow Recomendado

### Fase 1: Descarga Estratégica (1 vez)

**Objetivo:** Descargar datos de los vendors con rate limits más estrictos

```powershell
cd "D:\04_TRADING_SMALLCAPS\01_DayBook\fase_02\F_auditoria_data\independent_audit_multi_wrds\independent_audit_multi"

# Crear carpeta para archivos normalizados
mkdir normalized

# Descargar 1 ticker con Twelve Data (gastar 1 call controlado)
python verify_against_references.py `
  --symbols WOLF `
  --dates 2025-05-13 `
  --vendors twelvedata `
  --our-dib-root "D:/04_TRADING_SMALLCAPS/processed/bars" `
  --out reports/download_twelvedata_wolf.json

# El script ya hizo el API call y tiene la data en memoria
# Pero necesitamos interceptar la respuesta para guardarla...
```

**⚠️ Problema:** El script actual no guarda las respuestas raw.

**Solución:** Modificar temporalmente los clientes para guardar respuestas:

```python
# En twelvedata_client.py, línea 34-35 (después de r.json()):
data = r.json()

# GUARDAR RAW DATA
import json, pathlib
output_dir = pathlib.Path("raw_data")
output_dir.mkdir(exist_ok=True)
output_file = output_dir / f"{symbol}_{date}_twelvedata_raw.json"
output_file.write_text(json.dumps(data, indent=2))
print(f"Saved raw data to {output_file}")

# Continuar con el código normal...
```

---

### Fase 2: Normalización (offline, 0 API calls)

Una vez tengas los archivos raw guardados:

```powershell
# Normalizar Twelve Data (UTC + RTH filtering opcional)
python preprocess_vendor_data.py `
  --in raw_data/WOLF_2025-05-13_twelvedata_raw.json `
  --vendor twelvedata `
  --assume-tz UTC `
  --rth-only `
  --out normalized/WOLF_2025-05-13_twelvedata_minute.parquet

# Normalizar Alpha Vantage
python preprocess_vendor_data.py `
  --in raw_data/WOLF_2025-05-13_alphavantage_raw.json `
  --vendor alphavantage `
  --assume-tz US/Eastern `
  --rth-only `
  --out normalized/WOLF_2025-05-13_alphavantage_minute.parquet
```

**Resultado:**
```
normalized/
├── WOLF_2025-05-13_twelvedata_minute.parquet
├── WOLF_2025-05-13_alphavantage_minute.parquet
└── WOLF_2025-05-13_polygon_minute.parquet
```

---

### Fase 3: Validaciones Ilimitadas (0 API calls)

Ahora puedes ejecutar validaciones infinitas veces:

```powershell
# Validación 1
python verify_against_references.py `
  --symbols WOLF `
  --dates 2025-05-13 `
  --vendors twelvedata,alphavantage,polygon `
  --normalized-root normalized `
  --our-dib-root "D:/04_TRADING_SMALLCAPS/processed/bars" `
  --price-tol 0.002 `
  --vol-tol 0.05 `
  --out reports/validation_1.json
# ✅ 0 API calls (usa archivos normalizados)

# Validación 2 (mismos símbolos/fechas)
python verify_against_references.py `
  --symbols WOLF `
  --dates 2025-05-13 `
  --vendors twelvedata,alphavantage,polygon `
  --normalized-root normalized `
  --our-dib-root "D:/04_TRADING_SMALLCAPS/processed/bars" `
  --price-tol 0.01 `
  --vol-tol 0.10 `
  --out reports/validation_2_relaxed_tols.json
# ✅ 0 API calls (reutiliza archivos)

# Validación 3 (nuevo ticker → fallback a API solo para nuevo ticker)
python verify_against_references.py `
  --symbols WOLF,NVDA `
  --dates 2025-05-13 `
  --vendors twelvedata,alphavantage `
  --normalized-root normalized `
  --our-dib-root "D:/04_TRADING_SMALLCAPS/processed/bars" `
  --out reports/validation_3_with_nvda.json
# ✅ WOLF: 0 API calls (usa normalized)
# ⚠️ NVDA: 2 API calls (twelvedata + alphavantage, no existe normalized)
```

---

## 5. Plan de Acción Inmediato

### Paso 1: Modificar Clientes para Guardar Raw Data

**Objetivo:** Interceptar respuestas API antes de procesarlas

**Archivo:** `vendors/twelvedata_client.py`

```python
# Después de línea 34 (r.json())
def fetch_twelvedata_1min(symbol: str, date: str, api_key: str = None):
    # ... código existente hasta r.json() ...

    data = r.json()

    # NUEVO: Guardar raw data
    import json
    raw_dir = pathlib.Path(__file__).parent.parent / "raw_data"
    raw_dir.mkdir(exist_ok=True)
    raw_file = raw_dir / f"{symbol}_{date}_twelvedata_raw.json"
    raw_file.write_text(json.dumps(data, indent=2, ensure_ascii=False))
    print(f"[SAVED RAW] {raw_file}")

    # Continuar con el procesamiento normal...
    if "status" in data and data["status"] == "error":
        raise RuntimeError(f"Twelve Data API error: {data.get('message', 'Unknown error')}")
    # ...
```

**Aplicar lo mismo en:**
- `vendors/alphavantage_client.py`
- `vendors/polygon_client.py`

---

### Paso 2: Re-ejecutar Descarga Controlada

```powershell
# SOLO 1 ticker × 1 fecha × 2 vendors = 2 API calls
python verify_against_references.py `
  --symbols WOLF `
  --dates 2025-05-13 `
  --vendors twelvedata,alphavantage `
  --our-dib-root "D:/04_TRADING_SMALLCAPS/processed/bars" `
  --out reports/download_controlled.json

# Resultado:
# - 2 API calls gastados (twelvedata=3/8, alphavantage=?/5)
# - 2 archivos raw guardados en raw_data/
```

---

### Paso 3: Normalizar Archivos Raw

```powershell
# Normalizar Twelve Data
python preprocess_vendor_data.py `
  --in raw_data/WOLF_2025-05-13_twelvedata_raw.json `
  --vendor twelvedata `
  --assume-tz UTC `
  --rth-only `
  --out normalized/WOLF_2025-05-13_twelvedata_minute.parquet

# Normalizar Alpha Vantage
python preprocess_vendor_data.py `
  --in raw_data/WOLF_2025-05-13_alphavantage_raw.json `
  --vendor alphavantage `
  --assume-tz US/Eastern `
  --rth-only `
  --out normalized/WOLF_2025-05-13_alphavantage_minute.parquet
```

---

### Paso 4: Validar con Archivos Normalizados (0 API calls)

```powershell
# Prueba 1: Solo vendors con normalized files
python verify_against_references.py `
  --symbols WOLF `
  --dates 2025-05-13 `
  --vendors twelvedata,alphavantage `
  --normalized-root normalized `
  --our-dib-root "D:/04_TRADING_SMALLCAPS/processed/bars" `
  --out reports/test_normalized_only.json
# ✅ 0 API calls

# Prueba 2: Mix normalized + API call
python verify_against_references.py `
  --symbols WOLF `
  --dates 2025-05-13 `
  --vendors twelvedata,alphavantage,polygon `
  --normalized-root normalized `
  --our-dib-root "D:/04_TRADING_SMALLCAPS/processed/bars" `
  --out reports/test_mixed.json
# ✅ twelvedata: 0 API calls (usa normalized)
# ✅ alphavantage: 0 API calls (usa normalized)
# ⚠️ polygon: 1 API call (no normalized, pero Polygon es unlimited)
```

---

## 6. Ventajas del Sistema

### Conservación de Rate Limits

**Antes:**
```
10 validaciones × 3 vendors × 1 ticker = 30 API calls
Con límite de 8/min en Twelve Data → FALLA en minuto 2
```

**Después:**
```
1 descarga inicial = 3 API calls
9 validaciones posteriores = 0 API calls
Total = 3 API calls ✅
```

---

### Reproducibilidad Científica

**Ventaja:** Los archivos normalizados son **inmutables** y **timestamped**.

```
Validación en Enero 2025:
normalized/WOLF_2025-05-13_twelvedata_minute.parquet (descargado 2025-01-15)

Validación en Junio 2025:
Mismo archivo → Mismos resultados → Reproducibilidad garantizada
```

**Nota:** Las APIs pueden revisar datos históricos. Con archivos normalizados, tienes snapshot inmutable.

---

### Testing y Debugging

**Ventaja:** Puedes modificar scripts de validación sin gastar API calls.

```powershell
# Cambiar tolerancias
python verify_against_references.py ... --price-tol 0.001  # 0 calls
python verify_against_references.py ... --price-tol 0.005  # 0 calls

# Cambiar comparador
# Modificar compare_minute.py
python verify_against_references.py ...  # 0 calls para re-testear

# Añadir nuevo vendor
# Si ya tienes normalized files de otros vendors:
python verify_against_references.py ... --vendors all  # Solo 1 call nuevo vendor
```

---

## 7. Expansión a Gran Escala

### Estrategia para 11,054 Ticker-Days

**Objetivo:** Validar toda la base de datos contra vendors externos.

**Problema:** Con rate limits, tardaría semanas.

**Solución con normalized files:**

1. **Fase 1 - Muestra Estadística (1 día):**
   ```
   100 tickers × 1 día × 3 vendors = 300 API calls
   Con Twelve Data (8/min) = 37.5 minutos
   Con Alpha Vantage (5/min) = 60 minutos
   Total: ~1 hora
   ```

2. **Fase 2 - Normalización (offline):**
   ```
   300 archivos raw → 300 archivos normalized
   Tiempo: ~10 minutos (local processing)
   ```

3. **Fase 3 - Validaciones Ilimitadas:**
   ```
   Validación 1: tolerance 0.2% → 0 API calls
   Validación 2: tolerance 0.5% → 0 API calls
   Validación 3: RTH only vs full day → 0 API calls
   Total: INFINITAS validaciones, 0 API calls adicionales
   ```

---

### Batch Download Script (futuro)

```python
# batch_download_normalized.py
import time

tickers = ["WOLF", "NVDA", "AAPL", "TSLA", ...]  # 100 tickers
dates = ["2025-05-13"]
vendors = ["twelvedata", "alphavantage"]

batch_size = 5  # Respetar rate limit de 5/min

for vendor in vendors:
    for i in range(0, len(tickers), batch_size):
        batch = tickers[i:i+batch_size]

        # Download batch
        run_verification(batch, dates, [vendor])

        # Normalize immediately (offline)
        for ticker in batch:
            for date in dates:
                normalize_raw_to_parquet(ticker, date, vendor)

        # Wait for rate limit reset
        if i + batch_size < len(tickers):
            print(f"Waiting 60s for rate limit reset...")
            time.sleep(60)

print("All data downloaded and normalized!")
```

**Tiempo estimado (100 tickers, 2 vendors):**
- Twelve Data: 100/8 = 12.5 min × 2 vendors = 25 min
- Alpha Vantage: 100/5 = 20 min × 2 vendors = 40 min
- **Total: ~1 hora para tener 200 archivos normalizados**

---

## 8. Próximos Pasos

### Acción Inmediata

1. ✅ **Aplicado:** Patch en `verify_against_references.py` para soportar `--normalized-root`
2. ✅ **Creado:** Script `preprocess_vendor_data.py` para normalización
3. ⚠️ **Pendiente:** Modificar clientes para guardar raw data
4. ⚠️ **Pendiente:** Ejecutar descarga controlada (2-3 API calls)
5. ⚠️ **Pendiente:** Normalizar archivos y validar (0 API calls)

---

### Acción a Mediano Plazo

1. Crear `batch_download_normalized.py` para descargas masivas controladas
2. Descargar muestra estadística (100 tickers × 10 días = 1,000 ticker-days)
3. Generar reporte de validación científica multi-vendor
4. Documentar en paper/reporte final

---

## 9. Conclusión

### Estado Actual

- ✅ **Sistema implementado**: `--normalized-root` funcional
- ✅ **Preprocesor creado**: `preprocess_vendor_data.py`
- ⚠️ **Pendiente:** Modificar clientes para guardar raw data
- 📊 **Rate limits preservados:** Twelve Data 6/8 restantes

### Impacto

**Antes:**
- Cada validación = N API calls
- Rate limits bloquean testing iterativo
- Imposible validar 11,054 ticker-days

**Después:**
- Primera descarga = N API calls
- Siguientes validaciones = 0 API calls
- Testing ilimitado sin restricciones
- Validación masiva factible (~1 hora para 100 tickers)

**Recomendación:** Aplicar modificación a clientes YA, descargar datos de WOLF (2 calls), normalizar, y luego escalar estratégicamente.
