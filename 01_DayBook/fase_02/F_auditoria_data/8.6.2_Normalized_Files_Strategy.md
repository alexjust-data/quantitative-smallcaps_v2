# 8.9 Estrategia de Archivos Normalizados - Conservaci√≥n de API Calls

**Fecha**: 2025-10-23
**Objetivo**: Evitar gastar llamadas API innecesarias usando archivos normalizados pre-descargados

---

## 1. Problema Identificado

### Rate Limits Actuales

| Vendor | L√≠mite | Usado | Restante | Criticidad |
|--------|--------|-------|----------|-----------|
| **Twelve Data** | 8 calls/min | 2 | 6 | ‚ö†Ô∏è Medio |
| **Alpha Vantage** | 5 calls/min | ? | ? | ‚úÖ Bajo |
| **Polygon** | Unlimited (paid) | N/A | N/A | ‚úÖ Bajo |
| **FMP** | N/A (403) | N/A | N/A | ‚ùå No funcional |

### Problema

Cada vez que ejecutamos `verify_against_references.py`, el script hace **1 API call por (ticker √ó date √ó vendor)**:

```
Ejemplo actual:
--symbols WOLF,NVDA,AAPL (3 tickers)
--dates 2025-05-13,2025-05-14 (2 dates)
--vendors twelvedata,alphavantage (2 vendors)

Total API calls = 3 √ó 2 √ó 2 = 12 calls

Si Twelve Data tiene l√≠mite de 8/min ‚Üí FALLA
Si Alpha Vantage tiene l√≠mite de 5/min ‚Üí FALLA
```

---

## 2. Soluci√≥n: Sistema de Archivos Normalizados

### Concepto

**Descargar una vez, usar infinitas veces:**

1. **Descarga inicial** (1 API call) ‚Üí Guardar respuesta raw
2. **Normalizaci√≥n** (offline, 0 API calls) ‚Üí Convertir a formato est√°ndar UTC
3. **Validaciones** (0 API calls) ‚Üí Leer archivos normalizados en lugar de llamar API

### Arquitectura

```
Flujo anterior (cada validaci√≥n = API calls):
verify_against_references.py ‚Üí twelvedata_client.py ‚Üí API call ‚Üí 1 call gastado

Flujo nuevo (primera vez):
1. twelvedata_client.py ‚Üí API call ‚Üí Guardar raw data ‚Üí 1 call gastado
2. preprocess_vendor_data.py ‚Üí Leer raw ‚Üí Normalizar ‚Üí Guardar normalized ‚Üí 0 calls
3. verify_against_references.py ‚Üí Leer normalized ‚Üí 0 calls

Flujo nuevo (siguientes veces):
verify_against_references.py ‚Üí Leer normalized ‚Üí 0 calls ‚úÖ
```

---

## 3. Implementaci√≥n Aplicada

### Cambios en `verify_against_references.py`

**Nuevo par√°metro:**
```bash
--normalized-root <path>
```

**L√≥gica:**
```python
def load_vendor_frame(vendor, symbol, date, ..., normalized_root=None):
    # 1) Buscar archivo normalizado primero
    if normalized_root:
        path = f"{normalized_root}/{symbol}_{date}_{vendor}_minute.parquet"
        if path exists:
            return load_normalized(path)  # 0 API calls ‚úÖ

    # 2) Si no existe, fallback a API client
    return fetch_from_api(vendor, symbol, date)  # 1 API call
```

**Patron de nombres:**
```
<SYMBOL>_<DATE>_<VENDOR>_minute.(parquet|csv|json)

Ejemplos:
WOLF_2025-05-13_twelvedata_minute.parquet
NVDA_2025-05-14_alphavantage_minute.csv
AAPL_2025-05-13_polygon_minute.json
```

---

## 4. Workflow Recomendado

### Fase 1: Descarga Estrat√©gica (1 vez)

**Objetivo:** Descargar datos de los vendors con rate limits m√°s estrictos

```powershell
cd "D:\04_TRADING_SMALLCAPS\01_DayBook\fase_02\F_auditoria_data\independent_audit_multi_wrds\independent_audit_multi"

# Crear carpeta para archivos normalizados
mkdir normalized

# Descargar 1 ticker con Twelve Data (gastar 1 call controlado)
python verify_against_references.py `
  --symbols WOLF `
  --dates 2025-05-13 `
  --vendors twelvedata `
  --our-dib-root "D:/04_TRADING_SMALLCAPS/processed/bars" `
  --out reports/download_twelvedata_wolf.json

# El script ya hizo el API call y tiene la data en memoria
# Pero necesitamos interceptar la respuesta para guardarla...
```

**‚ö†Ô∏è Problema:** El script actual no guarda las respuestas raw.

**Soluci√≥n:** Modificar temporalmente los clientes para guardar respuestas:

```python
# En twelvedata_client.py, l√≠nea 34-35 (despu√©s de r.json()):
data = r.json()

# GUARDAR RAW DATA
import json, pathlib
output_dir = pathlib.Path("raw_data")
output_dir.mkdir(exist_ok=True)
output_file = output_dir / f"{symbol}_{date}_twelvedata_raw.json"
output_file.write_text(json.dumps(data, indent=2))
print(f"Saved raw data to {output_file}")

# Continuar con el c√≥digo normal...
```

---

### Fase 2: Normalizaci√≥n (offline, 0 API calls)

Una vez tengas los archivos raw guardados:

```powershell
# Normalizar Twelve Data (UTC + RTH filtering opcional)
python preprocess_vendor_data.py `
  --in raw_data/WOLF_2025-05-13_twelvedata_raw.json `
  --vendor twelvedata `
  --assume-tz UTC `
  --rth-only `
  --out normalized/WOLF_2025-05-13_twelvedata_minute.parquet

# Normalizar Alpha Vantage
python preprocess_vendor_data.py `
  --in raw_data/WOLF_2025-05-13_alphavantage_raw.json `
  --vendor alphavantage `
  --assume-tz US/Eastern `
  --rth-only `
  --out normalized/WOLF_2025-05-13_alphavantage_minute.parquet
```

**Resultado:**
```
normalized/
‚îú‚îÄ‚îÄ WOLF_2025-05-13_twelvedata_minute.parquet
‚îú‚îÄ‚îÄ WOLF_2025-05-13_alphavantage_minute.parquet
‚îî‚îÄ‚îÄ WOLF_2025-05-13_polygon_minute.parquet
```

---

### Fase 3: Validaciones Ilimitadas (0 API calls)

Ahora puedes ejecutar validaciones infinitas veces:

```powershell
# Validaci√≥n 1
python verify_against_references.py `
  --symbols WOLF `
  --dates 2025-05-13 `
  --vendors twelvedata,alphavantage,polygon `
  --normalized-root normalized `
  --our-dib-root "D:/04_TRADING_SMALLCAPS/processed/bars" `
  --price-tol 0.002 `
  --vol-tol 0.05 `
  --out reports/validation_1.json
# ‚úÖ 0 API calls (usa archivos normalizados)

# Validaci√≥n 2 (mismos s√≠mbolos/fechas)
python verify_against_references.py `
  --symbols WOLF `
  --dates 2025-05-13 `
  --vendors twelvedata,alphavantage,polygon `
  --normalized-root normalized `
  --our-dib-root "D:/04_TRADING_SMALLCAPS/processed/bars" `
  --price-tol 0.01 `
  --vol-tol 0.10 `
  --out reports/validation_2_relaxed_tols.json
# ‚úÖ 0 API calls (reutiliza archivos)

# Validaci√≥n 3 (nuevo ticker ‚Üí fallback a API solo para nuevo ticker)
python verify_against_references.py `
  --symbols WOLF,NVDA `
  --dates 2025-05-13 `
  --vendors twelvedata,alphavantage `
  --normalized-root normalized `
  --our-dib-root "D:/04_TRADING_SMALLCAPS/processed/bars" `
  --out reports/validation_3_with_nvda.json
# ‚úÖ WOLF: 0 API calls (usa normalized)
# ‚ö†Ô∏è NVDA: 2 API calls (twelvedata + alphavantage, no existe normalized)
```

---

## 5. Plan de Acci√≥n Inmediato

### Paso 1: Modificar Clientes para Guardar Raw Data

**Objetivo:** Interceptar respuestas API antes de procesarlas

**Archivo:** `vendors/twelvedata_client.py`

```python
# Despu√©s de l√≠nea 34 (r.json())
def fetch_twelvedata_1min(symbol: str, date: str, api_key: str = None):
    # ... c√≥digo existente hasta r.json() ...

    data = r.json()

    # NUEVO: Guardar raw data
    import json
    raw_dir = pathlib.Path(__file__).parent.parent / "raw_data"
    raw_dir.mkdir(exist_ok=True)
    raw_file = raw_dir / f"{symbol}_{date}_twelvedata_raw.json"
    raw_file.write_text(json.dumps(data, indent=2, ensure_ascii=False))
    print(f"[SAVED RAW] {raw_file}")

    # Continuar con el procesamiento normal...
    if "status" in data and data["status"] == "error":
        raise RuntimeError(f"Twelve Data API error: {data.get('message', 'Unknown error')}")
    # ...
```

**Aplicar lo mismo en:**
- `vendors/alphavantage_client.py`
- `vendors/polygon_client.py`

---

### Paso 2: Re-ejecutar Descarga Controlada

```powershell
# SOLO 1 ticker √ó 1 fecha √ó 2 vendors = 2 API calls
python verify_against_references.py `
  --symbols WOLF `
  --dates 2025-05-13 `
  --vendors twelvedata,alphavantage `
  --our-dib-root "D:/04_TRADING_SMALLCAPS/processed/bars" `
  --out reports/download_controlled.json

# Resultado:
# - 2 API calls gastados (twelvedata=3/8, alphavantage=?/5)
# - 2 archivos raw guardados en raw_data/
```

---

### Paso 3: Normalizar Archivos Raw

```powershell
# Normalizar Twelve Data
python preprocess_vendor_data.py `
  --in raw_data/WOLF_2025-05-13_twelvedata_raw.json `
  --vendor twelvedata `
  --assume-tz UTC `
  --rth-only `
  --out normalized/WOLF_2025-05-13_twelvedata_minute.parquet

# Normalizar Alpha Vantage
python preprocess_vendor_data.py `
  --in raw_data/WOLF_2025-05-13_alphavantage_raw.json `
  --vendor alphavantage `
  --assume-tz US/Eastern `
  --rth-only `
  --out normalized/WOLF_2025-05-13_alphavantage_minute.parquet
```

---

### Paso 4: Validar con Archivos Normalizados (0 API calls)

```powershell
# Prueba 1: Solo vendors con normalized files
python verify_against_references.py `
  --symbols WOLF `
  --dates 2025-05-13 `
  --vendors twelvedata,alphavantage `
  --normalized-root normalized `
  --our-dib-root "D:/04_TRADING_SMALLCAPS/processed/bars" `
  --out reports/test_normalized_only.json
# ‚úÖ 0 API calls

# Prueba 2: Mix normalized + API call
python verify_against_references.py `
  --symbols WOLF `
  --dates 2025-05-13 `
  --vendors twelvedata,alphavantage,polygon `
  --normalized-root normalized `
  --our-dib-root "D:/04_TRADING_SMALLCAPS/processed/bars" `
  --out reports/test_mixed.json
# ‚úÖ twelvedata: 0 API calls (usa normalized)
# ‚úÖ alphavantage: 0 API calls (usa normalized)
# ‚ö†Ô∏è polygon: 1 API call (no normalized, pero Polygon es unlimited)
```

---

## 6. Ventajas del Sistema

### Conservaci√≥n de Rate Limits

**Antes:**
```
10 validaciones √ó 3 vendors √ó 1 ticker = 30 API calls
Con l√≠mite de 8/min en Twelve Data ‚Üí FALLA en minuto 2
```

**Despu√©s:**
```
1 descarga inicial = 3 API calls
9 validaciones posteriores = 0 API calls
Total = 3 API calls ‚úÖ
```

---

### Reproducibilidad Cient√≠fica

**Ventaja:** Los archivos normalizados son **inmutables** y **timestamped**.

```
Validaci√≥n en Enero 2025:
normalized/WOLF_2025-05-13_twelvedata_minute.parquet (descargado 2025-01-15)

Validaci√≥n en Junio 2025:
Mismo archivo ‚Üí Mismos resultados ‚Üí Reproducibilidad garantizada
```

**Nota:** Las APIs pueden revisar datos hist√≥ricos. Con archivos normalizados, tienes snapshot inmutable.

---

### Testing y Debugging

**Ventaja:** Puedes modificar scripts de validaci√≥n sin gastar API calls.

```powershell
# Cambiar tolerancias
python verify_against_references.py ... --price-tol 0.001  # 0 calls
python verify_against_references.py ... --price-tol 0.005  # 0 calls

# Cambiar comparador
# Modificar compare_minute.py
python verify_against_references.py ...  # 0 calls para re-testear

# A√±adir nuevo vendor
# Si ya tienes normalized files de otros vendors:
python verify_against_references.py ... --vendors all  # Solo 1 call nuevo vendor
```

---

## 7. Expansi√≥n a Gran Escala

### Estrategia para 11,054 Ticker-Days

**Objetivo:** Validar toda la base de datos contra vendors externos.

**Problema:** Con rate limits, tardar√≠a semanas.

**Soluci√≥n con normalized files:**

1. **Fase 1 - Muestra Estad√≠stica (1 d√≠a):**
   ```
   100 tickers √ó 1 d√≠a √ó 3 vendors = 300 API calls
   Con Twelve Data (8/min) = 37.5 minutos
   Con Alpha Vantage (5/min) = 60 minutos
   Total: ~1 hora
   ```

2. **Fase 2 - Normalizaci√≥n (offline):**
   ```
   300 archivos raw ‚Üí 300 archivos normalized
   Tiempo: ~10 minutos (local processing)
   ```

3. **Fase 3 - Validaciones Ilimitadas:**
   ```
   Validaci√≥n 1: tolerance 0.2% ‚Üí 0 API calls
   Validaci√≥n 2: tolerance 0.5% ‚Üí 0 API calls
   Validaci√≥n 3: RTH only vs full day ‚Üí 0 API calls
   Total: INFINITAS validaciones, 0 API calls adicionales
   ```

---

### Batch Download Script (futuro)

```python
# batch_download_normalized.py
import time

tickers = ["WOLF", "NVDA", "AAPL", "TSLA", ...]  # 100 tickers
dates = ["2025-05-13"]
vendors = ["twelvedata", "alphavantage"]

batch_size = 5  # Respetar rate limit de 5/min

for vendor in vendors:
    for i in range(0, len(tickers), batch_size):
        batch = tickers[i:i+batch_size]

        # Download batch
        run_verification(batch, dates, [vendor])

        # Normalize immediately (offline)
        for ticker in batch:
            for date in dates:
                normalize_raw_to_parquet(ticker, date, vendor)

        # Wait for rate limit reset
        if i + batch_size < len(tickers):
            print(f"Waiting 60s for rate limit reset...")
            time.sleep(60)

print("All data downloaded and normalized!")
```

**Tiempo estimado (100 tickers, 2 vendors):**
- Twelve Data: 100/8 = 12.5 min √ó 2 vendors = 25 min
- Alpha Vantage: 100/5 = 20 min √ó 2 vendors = 40 min
- **Total: ~1 hora para tener 200 archivos normalizados**

---

## 8. Pr√≥ximos Pasos

### Acci√≥n Inmediata

1. ‚úÖ **Aplicado:** Patch en `verify_against_references.py` para soportar `--normalized-root`
2. ‚úÖ **Creado:** Script `preprocess_vendor_data.py` para normalizaci√≥n
3. ‚ö†Ô∏è **Pendiente:** Modificar clientes para guardar raw data
4. ‚ö†Ô∏è **Pendiente:** Ejecutar descarga controlada (2-3 API calls)
5. ‚ö†Ô∏è **Pendiente:** Normalizar archivos y validar (0 API calls)

---

### Acci√≥n a Mediano Plazo

1. Crear `batch_download_normalized.py` para descargas masivas controladas
2. Descargar muestra estad√≠stica (100 tickers √ó 10 d√≠as = 1,000 ticker-days)
3. Generar reporte de validaci√≥n cient√≠fica multi-vendor
4. Documentar en paper/reporte final

---

## 9. Conclusi√≥n

### Estado Actual

- ‚úÖ **Sistema implementado**: `--normalized-root` funcional
- ‚úÖ **Preprocesor creado**: `preprocess_vendor_data.py`
- ‚ö†Ô∏è **Pendiente:** Modificar clientes para guardar raw data
- üìä **Rate limits preservados:** Twelve Data 6/8 restantes

### Impacto

**Antes:**
- Cada validaci√≥n = N API calls
- Rate limits bloquean testing iterativo
- Imposible validar 11,054 ticker-days

**Despu√©s:**
- Primera descarga = N API calls
- Siguientes validaciones = 0 API calls
- Testing ilimitado sin restricciones
- Validaci√≥n masiva factible (~1 hora para 100 tickers)

**Recomendaci√≥n:** Aplicar modificaci√≥n a clientes YA, descargar datos de WOLF (2 calls), normalizar, y luego escalar estrat√©gicamente.
