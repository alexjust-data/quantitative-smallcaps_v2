Hay alternativas (gratuitas y de pago) para cubrir “shorts” sin depender solo de FINRA. Te dejo el mapa rápido y qué papel juega cada fuente:

## Opciones gratuitas / oficiales

1. **Ficheros diarios de “Short Sale Volume” por EXCHANGE**

* **Nasdaq Trader** publica ficheros diarios agregados de volumen en corto ejecutado en el mercado Nasdaq (no-TRF). Útil para complementar FINRA (que cubre TRF/ADF/ORF, es decir, off-exchange). ([data.nasdaq.com][1])
* **NYSE TAQ – Group Short Sales & Short Volume** (NY  ➜ NYSE, NYSE American, NYSE Arca, NYSE National, NYSE Texas). Producto histórico con ficheros diarios por símbolo (generalmente **de pago**, pero es la vía “oficial” para los libros de NYSE). ([Bolsa de Nueva York][2])
  **Cómo encaja:** FINRA (off-exchange) + Nasdaq/NYSE (on-exchange) = cobertura más completa del “short sale volume” diario.

2. **Short Interest (quincenal, consolidado por exchange)**

* **Nasdaq** publica el **short interest** dos veces al mes (mid-month y end-month). Es el dato regulatorio clásico (no volumen diario). ([nasdaqtrader.com][3])
* **NYSE** publica el **Consolidated Short Interest** (excel). Misma cadencia regulatoria. ([Bolsa de Nueva York][4])
  **Cómo encaja:** Métrica lenta (quincenal) para “crowding / days to cover”, no sustituye al short-volume diario.

3. **SEC Fails-to-Deliver (FTD)**

* Ficheros diarios (con retraso) de **fails-to-deliver** por CUSIP/ticker. Es un **proxy** de presión en la mecánica de liquidación (no es igual a short interest ni a short volume). ([SEC][5])
  **Cómo encaja:** Señal alternativa para eventos extremos (squeezes, estrés), no reemplaza short volume/interest.

## Proveedores comerciales (securities lending / estimaciones en tiempo casi real)

* **S3 Partners** (API/licencia; también vía AWS/Bloomberg/partners). Métricas de **estimated short interest**, borrow rates, days-to-cover, etc. ([s3partners.com][6])
* **S&P Global Market Intelligence (ex IHS Markit)** – **Securities Finance**: flujos del **securities lending** (supply/demand), short interest “revisado/estimado”, histórico amplio. ([S&P Global Marketplace][7])
  **Cómo encaja:** Si quieres señales intradía/diarias más ricas que lo público, combinan inventarios de préstamo, fee rates, etc. (pero con coste).

---

## Recomendación práctica para tu pipeline (Small Caps)

* **Capa pública mínima viable (gratis):**

  * FINRA **Short Sale Volume (TRF/ADF/ORF)** (cuando te funcione el endpoint o por TXT). ([FINRA][8])
  * **Nasdaq daily Short Sale Volume** (on-exchange). ([data.nasdaq.com][1])
  * **NYSE TAQ daily Short Volume** si puedes presupuestarlo (para completar “on-exchange”). ([Bolsa de Nueva York][2])
  * **Short Interest quincenal** (Nasdaq/NYSE) para “stock-level crowding”. ([nasdaqtrader.com][3])
  * **SEC FTD** como señal de estrés/“fails” (lag). ([SEC][5])

* **Si FINRA sigue bloqueándote temporalmente:**

  * **Plan B inmediato:** tira de **ficheros diarios públicos** (descarga masiva por fecha) desde FINRA (no API) y **Nasdaq Trader** para Nasdaq; si NYSE es crítico, valora **NYSE TAQ**. ([FINRA][9])
  * **Plan C (mejor señal, con coste):** conéctate a **S3** o **S&P Global (IHS Markit Securities Finance)** para métricas de préstamo/short interest estimado en tiempo casi real. ([s3partners.com][6])

**TL;DR:** Sí, hay alternativas. Para gratis/regulatorio combina **Nasdaq/NYSE (on-exchange)** + **FINRA (off-exchange)** + **Short Interest quincenal** + **SEC FTD**. Si necesitas “alpha” de microestructura de cortos en **tiempo cercano al real**, evalúa **S3** o **S&P Global (IHS Markit)**.

[1]: https://data.nasdaq.com/databases/NSS?utm_source=chatgpt.com "Nasdaq Short Sale Volume and Transaction Reports"
[2]: https://www.nyse.com/market-data/historical/taq-nyse-group-short-sales?utm_source=chatgpt.com "TAQ Group Short Sales & Short Volume"
[3]: https://www.nasdaqtrader.com/asp/short_interest.asp?utm_source=chatgpt.com "Short Interest"
[4]: https://www.nyse.com/publicdocs/nyse/data/NYSE_Group_Consolidated_Short_Interest.xls?utm_source=chatgpt.com "Consolidated Short Interest Report"
[5]: https://www.sec.gov/data-research/sec-markets-data/fails-deliver-data?utm_source=chatgpt.com "Fails-to-Deliver Data"
[6]: https://www.s3partners.com/trial?utm_source=chatgpt.com "Trial"
[7]: https://www.marketplace.spglobal.com/en/datasets/securities-finance-short-interest-data-%281704372663%29?utm_source=chatgpt.com "Securities Finance: Short Interest Data Dataset"
[8]: https://www.finra.org/finra-data/browse-catalog/short-sale-volume-data?utm_source=chatgpt.com "Short Sale Volume Data"
[9]: https://www.finra.org/finra-data/browse-catalog/short-sale-volume-data/daily-short-sale-volume-files?utm_source=chatgpt.com "Daily Short Sale Volume Files"


---

## Implementacion Ejecutada (2025-10-23)

### Scripts Creados

Implementamos 3 scripts de descarga para cubrir multiples fuentes de short data:

#### 1. download_finra_short_volume_txt.py - FINRA Off-Exchange (TRF/ADF/ORF)
**Ubicacion:** `scripts/fase_G_atributos_faltantes/download_finra_short_volume_txt.py`

**Caracteristicas:**
- Descarga ficheros TXT diarios consolidados CNMS (Consolidated NMS)
- URL: `https://cdn.finra.org/equity/regsho/daily/CNMSshvol{yyyymmdd}.txt`
- Autodeteccion de separador (comma, tab, pipe)
- Normalizacion de columnas: tradeDate, symbol, shortVolume, shortExemptVolume, totalVolume, market
- Resume capability con _SUCCESS markers
- Particionado por date=YYYY-MM-DD
- Compresion ZSTD nivel 3
- Encoding robusto (utf-8, latin-1, cp1252)

**Output:** `raw/finra/regsho_daily_txt/date=YYYY-MM-DD/regsho.parquet`

**Test ejecutado:** Verificado con 2024-10-18 - 10,042 filas descargadas correctamente

#### 2. download_nasdaq_short_volume.py - Nasdaq On-Exchange
**Ubicacion:** `scripts/fase_G_atributos_faltantes/download_nasdaq_short_volume.py`

**Caracteristicas:**
- Descarga Nasdaq daily short sale volume (on-exchange)
- URL template: `https://ftp.nasdaqtrader.com/files/shorts/NASDAQshvol{yyyymmdd}.txt`
- Misma arquitectura robusta que FINRA (autodeteccion, normalizacion, resume)
- Columnas: symbol, shortVolume, shortExemptVolume, totalVolume, market
- Particionado por date=YYYY-MM-DD

**Output:** `raw/nasdaq/short_volume/date=YYYY-MM-DD/shorts.parquet`

#### 3. download_sec_ftd.py - SEC Fails-to-Deliver (Mensual)
**Ubicacion:** `scripts/fase_G_atributos_faltantes/download_sec_ftd.py`

**Caracteristicas:**
- Descarga ZIPs mensuales de Fails-to-Deliver
- URL: `https://www.sec.gov/files/data/fails-deliver-data/cnsfails{yyyymm}.zip`
- Extrae automaticamente el primer CSV/TXT del ZIP
- Normalizacion: settlement_date, cusip, symbol, quantity_fails, price
- Particionado por year=YYYY/month=MM
- Manejo de fechas YYYYMMDD - YYYY-MM-DD
- Dependencia: python-dateutil

**Output:** `raw/sec/ftd/year=YYYY/month=MM/ftd.parquet`

### Estado de Descargas

**Fecha de ejecucion:** 2025-10-23 19:59 UTC

**Descargas lanzadas en paralelo:**

1. FINRA TXT (PID: 68d567) - Running
   - Rango: 2020-01-01 a 2025-10-21 (~2,121 dias)
   - Output: `raw/finra/regsho_daily_txt/`

2. Nasdaq Short Volume (PID: 00f8b7) - Running
   - Rango: 2020-01-01 a 2025-10-21 (~2,121 dias)
   - Output: `raw/nasdaq/short_volume/`

3. SEC FTD (PID: 9f154a) - Running
   - Rango: 2020-01 a 2025-10 (~70 meses)
   - Output: `raw/sec/ftd/`

### Correcciones Aplicadas

**Bug fix en download_finra_short_volume_txt.py (lineas 136-140):**

```python
# ANTES (incorrecto):
(part/"regsho.parquet").write_bytes(
    df.write_parquet(compression="zstd", compression_level=3)
)

# DESPUES (correcto):
df.write_parquet(
    part/"regsho.parquet",
    compression="zstd",
    compression_level=3
)
```

**Razon:** write_parquet() requiere el file path como primer argumento, no devuelve bytes.

### Cobertura Lograda

Con estos 3 scripts implementados, cubrimos:

- FINRA (off-exchange): TRF/ADF/ORF consolidated volume - ~60-70% del volumen retail
- Nasdaq (on-exchange): Short volume ejecutado en Nasdaq exchange
- SEC FTD: Proxy de estres/squeezes (fails-to-deliver mensual)

**Fuentes pendientes (opcionales):**
- NYSE TAQ daily short volume (de pago)
- NYSE/Nasdaq short interest quincenal (baja prioridad - metrica lenta)

### Proximos Pasos

1. Monitorear completitud de las 3 descargas (~2-3 horas estimadas)
2. Auditar datos descargados (cobertura, gaps, calidad)
3. Integrar con pipeline ML (feature engineering de short ratios)
4. Documentar en 9.4_Events_&_ShortsVolume.md

---

## Auditoria Post-Lanzamiento (2025-10-23 20:20 UTC)

### Problemas Detectados

#### 1. Nasdaq Short Volume - ABORTADO
**PID:** 00f8b7 (killed)
**Problema:** Network timeout - URL no accesible (21+ segundos timeout)
**URL intentada:** `https://ftp.nasdaqtrader.com/files/shorts/NASDAQshvol{yyyymmdd}.txt`

**Causa raiz:** Segun documentacion oficial de Nasdaq:
- Archivos de short sale **desde agosto 1, 2010 son DE PAGO ("fee liable")**
- Solo archivos pre-2010 estan disponibles gratuitamente
- Se requiere **acuerdo firmado con Nasdaq** + "Web-Based Products Order Form"

**Decision:** SKIP - No es critico. FINRA TXT cubre off-exchange (~60-70% volumen retail small caps).

**Output:** 0 archivos descargados

#### 2. SEC FTD - FALLIDO
**PID:** 9f154a (completed con exit_code=0, pero 0 descargas exitosas)
**Problema:** 70/70 meses retornaron 404 Not Found
**URL intentada:** `https://www.sec.gov/files/data/fails-deliver-data/cnsfails{yyyymm}.zip`

**Causa raiz:**
- SEC publica FTD **dos veces por mes** con sufijos "a" y "b"
- Patron correcto: `cnsfails{yyyymm}a.zip` y `cnsfails{yyyymm}b.zip`
- Ademas: Akamai CDN devuelve 403 Forbidden (proteccion anti-scraping)

**Decision:** SKIP - Baja prioridad (proxy de squeeze events). Requiere reescritura del script + bypass Akamai.

**Output:** 0 archivos descargados

### Descargas Activas (OK)

#### 1. FINRA TXT (PID: 68d567) - RUNNING OK
**Status:** Progresando correctamente
**Verificacion:** Test manual exitoso (2024-10-18 → 10,042 filas)
**URL:** `https://cdn.finra.org/equity/regsho/daily/CNMSshvol{yyyymmdd}.txt`
**Output:** `raw/finra/regsho_daily_txt/date=YYYY-MM-DD/regsho.parquet`

#### 2. SEC EDGAR Offerings (PID: dacaf6) - RUNNING OK
**Status:** Progresando correctamente (~13-15 mins para completar)
**Output:** `processed/edgar/offerings/` (particionado por form_type + year)

### Cobertura Final Lograda

**Con FINRA TXT solamente:**
- Off-exchange (TRF/ADF/ORF): ~60-70% del volumen retail small caps
- Cobertura diaria desde 2020-01-01 a 2025-10-21 (~2,121 dias)
- Suficiente para feature engineering de short ratios en pipeline ML

**Fuentes NO implementadas (no criticas):**
- Nasdaq on-exchange (DE PAGO desde 2010)
- SEC FTD (requiere bypass Akamai + patron complejo a/b)
- NYSE TAQ (DE PAGO)
- Short Interest quincenal (baja prioridad - metrica lenta)

### Conclusion

**Objetivo logrado:** Short volume data cubierto via FINRA TXT (fuente gratuita mas robusta).

**Tiempo invertido:** ~30 mins (diagnostico, testing, correcciones)

**Proximos pasos:**
1. Monitorear completitud FINRA TXT (~2-3 horas estimadas para 2,121 dias)
2. Monitorear completitud SEC EDGAR Offerings (~15 mins restantes)
3. Auditar calidad de datos descargados
4. Integrar con pipeline ML








# D:/04_TRADING_SMALLCAPS/

```sh
================================================================================
ESTRUCTURA ACTUAL (v1: 2020-2025)
================================================================================

processed/
  daily_cache/                              # Métricas pre-calculadas (RVOL, %chg, $vol)
    MANIFEST.json
    _SUCCESS
    ticker=AAL/
      date=2020-01-02/metrics.parquet
      ...
    ticker=AAPL/
      ...

  universe/
    cs_xnas_xnys_under2b_2025-10-21.csv     # Universo completo: 3,107 tickers
    info_rich/
      info_rich_tickers_20200101_20251021.csv   # Subset info-rich: 1,906 tickers
      daily/
        date=2020-01-02/watchlist.parquet   # Watchlists diarias (tickers info-rich ese día)
        date=2020-01-03/watchlist.parquet
        ...
        date=2025-10-21/watchlist.parquet   # 1,562 días con watchlists
      stats/
        topN_12m_rolling.parquet            # TopN runners últimos 12 meses (rolling)

  bars/                                      # Dollar Imbalance Bars (DIB)
    ticker=AAME/
      date=2021-02-05/bars.parquet
      date=2021-02-08/bars.parquet
      ...
    ticker=AAOI/
      date=2020-07-21/bars.parquet
      ...
    [1,906 tickers con días info-rich]

  labels/                                    # Triple Barrier Labels
    ticker=AAME/
      date=2021-02-05/labels.parquet
      ...

  weights/                                   # Sample Weights (uniqueness + time decay)
    ticker=AAME/
      date=2021-02-05/weights.parquet
      ...

  datasets/
    global/
      dataset.parquet                       # Dataset ML consolidado: 1,622,333 eventos
      meta.json                             # Metadatos (fecha generación, stats)

  edgar/
    offerings/                              # Descargas en progreso (Fase G)
      ticker=AAME/
        filings.parquet
        ...

raw/
  polygon/
    ohlcv_daily/                            # OHLCV diario (2004-2025, 3,107 tickers)
      ticker=AAL/
        date=2004-01-02/ohlcv.parquet
        ...
      ticker=AAPL/
        ...

    ohlcv_intraday_1m/                      # OHLCV 1-min (2004-2025, 3,110 tickers)
      ticker=AAL/
        date=2004-01-02/minute.parquet
        ...

    trades/                                 # Ticks (SOLO días info-rich 2020-2025)
      ticker=AAME/
        date=2021-02-05/trades.parquet      # 11,054 ticker-days
        date=2021-02-08/trades.parquet
        ...
      ticker=AAOI/
        date=2020-07-21/trades.parquet
        ...

    reference/
      dividends/                            # Dividendos (2000-2025, todos los tickers)
      splits/                               # Splits (histórico, todos los tickers)

    financials/                             # Fundamentals (en descarga, Polygon API)
    ticker_events/                          # Eventos corporativos (en descarga)
    ticker_changes/                         # Cambios de ticker/CIK

  finra/
    regsho_daily_txt/                       # FINRA Short Volume TXT (2020-2025)
      date=2020-01-02/regsho.parquet        # Descarga optimizada con 4 workers (en progreso)
      ...

  sec/
    ftd/                                    # Fails-to-Deliver (en descarga, 2020-2025)


================================================================================
PROPUESTA ESTRUCTURADAPARA v2 (2004-2019) - BACKFILL HISTÓRICO
================================================================================

processed/
  daily_cache/
    v1_2020_2025/                           # Período reciente (YA EXISTE)
      MANIFEST.json
      _SUCCESS
      ticker=AAL/...

    v2_2004_2019/                           # Período histórico (NUEVO)
      MANIFEST.json
      _SUCCESS
      ticker=AAL/
        date=2004-01-02/metrics.parquet
        ...
        date=2019-12-31/metrics.parquet

  universe/
    info_rich/
      v1_2020_2025/                         # Subset actual (YA EXISTE)
        info_rich_tickers.csv               # 1,906 tickers
        daily/date=.../watchlist.parquet
        stats/topN_12m_rolling.parquet

      v2_2004_2019/                         # Subset histórico (NUEVO)
        info_rich_tickers.csv               # ~2,500-3,000 tickers estimados
        daily/
          date=2004-01-02/watchlist.parquet
          ...
          date=2019-12-31/watchlist.parquet # ~4,000 días
        stats/topN_12m_rolling.parquet

raw/
  polygon/
    trades/
      v1_2020_2025/                         # Ticks actuales (YA EXISTE)
        ticker=AAME/
          date=2021-02-05/trades.parquet    # 11,054 ticker-days

      v2_2004_2019/                         # Ticks históricos (NUEVO)
        ticker=AAME/
          date=2008-10-09/trades.parquet    # ~30,000-50,000 ticker-days estimados
          ...

processed/
  bars/
    v1_2020_2025/...                        # DIB actuales
    v2_2004_2019/...                        # DIB históricos (NUEVO)

  labels/
    v1_2020_2025/...                        # Labels actuales
    v2_2004_2019/...                        # Labels históricos (NUEVO)

  weights/
    v1_2020_2025/...                        # Weights actuales
    v2_2004_2019/...                        # Weights históricos (NUEVO)

  datasets/
    global/
      dataset_v1_2020_2025.parquet          # Dataset reciente: 1.6M eventos (YA EXISTE)
      dataset_v2_2004_2019.parquet          # Dataset histórico: ~4-6M eventos (NUEVO)
      dataset_all_2004_2025.parquet         # Fusión completa (OPCIONAL, después)
      meta_v1.json
      meta_v2.json


================================================================================
VENTAJAS DE ESTA ESTRUCTURA VERSIONADA
================================================================================

1. NO ROMPE NADA: v1 queda intacto, auditado, funcionando
2. COMPARACIÓN FÁCIL: Puedes entrenar ML con v1, v2, o ambos
3. TRAZABILIDAD: Cada período tiene su metadata independiente
4. RÉGIMEN LABELING: Puedes añadir columna 'regime' = 'crisis_2008' | 'qe_era' | 'meme_era'
5. STORAGE CONTROLADO: Generas v2 por bloques (2004-2009, 2010-2014, 2015-2019)
6. DUCKDB READY: Queries SQL cross-version sin mover ficheros


================================================================================
COMANDOS PARA GENERAR v2 (PASO A PASO)
================================================================================

# PASO 1: Daily cache para 2004-2019 (30-60 min, usa tus 1-min ya descargados)
python scripts/fase_C_ingesta_tiks/build_daily_cache.py \
  --intraday-root raw/polygon/ohlcv_intraday_1m \
  --outdir processed/daily_cache/v2_2004_2019 \
  --from 2004-01-01 --to 2019-12-31 \
  --parallel 8

# PASO 2: Universo dinámico info-rich 2004-2019 (10-20 min)
python scripts/fase_C_ingesta_tiks/build_dynamic_universe_optimized.py \
  --daily-cache processed/daily_cache/v2_2004_2019 \
  --outdir processed/universe/info_rich/v2_2004_2019 \
  --from 2004-01-01 --to 2019-12-31 \
  --config configs/universe_config.yaml

# PASO 3: Descargar ticks SOLO días info-rich 2004-2019 (8-15 horas, por bloques)
# Bloque 1: 2004-2009 (incluye crisis 2008)
python scripts/fase_C_ingesta_tiks/download_trades_optimized.py \
  --tickers-csv processed/universe/cs_xnas_xnys_under2b_2025-10-21.csv \
  --watchlist-root processed/universe/info_rich/v2_2004_2019/daily \
  --outdir raw/polygon/trades/v2_2004_2019 \
  --from 2004-01-01 --to 2009-12-31 \
  --mode watchlists --workers 8 --resume

# Bloque 2: 2010-2014 (QE era)
python scripts/fase_C_ingesta_tiks/download_trades_optimized.py \
  --watchlist-root processed/universe/info_rich/v2_2004_2019/daily \
  --outdir raw/polygon/trades/v2_2004_2019 \
  --from 2010-01-01 --to 2014-12-31 \
  --mode watchlists --workers 8 --resume

# Bloque 3: 2015-2019 (pre-COVID)
python scripts/fase_C_ingesta_tiks/download_trades_optimized.py \
  --watchlist-root processed/universe/info_rich/v2_2004_2019/daily \
  --outdir raw/polygon/trades/v2_2004_2019 \
  --from 2015-01-01 --to 2019-12-31 \
  --mode watchlists --workers 8 --resume


# PASO 4: Construir barras/labels/weights v2 (similar a v1)
python scripts/fase_D_barras/build_dib_bars.py \
  --trades-root raw/polygon/trades/v2_2004_2019 \
  --outdir processed/bars/v2_2004_2019 \
  --config configs/dib_config.yaml

python scripts/fase_E_labels/build_triple_barrier_labels.py \
  --bars-root processed/bars/v2_2004_2019 \
  --outdir processed/labels/v2_2004_2019 \
  --config configs/labels_config.yaml

python scripts/fase_F_weights/build_sample_weights.py \
  --labels-root processed/labels/v2_2004_2019 \
  --outdir processed/weights/v2_2004_2019 \
  --config configs/weights_config.yaml

python scripts/build_ml_dataset.py \
  --bars processed/bars/v2_2004_2019 \
  --labels processed/labels/v2_2004_2019 \
  --weights processed/weights/v2_2004_2019 \
  --outdir processed/datasets/global \
  --output-name dataset_v2_2004_2019.parquet


================================================================================
ESTIMACIÓN DE RECURSOS v2 (2004-2019)
================================================================================

Daily cache generation:     30-60 min (procesa 1-min local, no API calls)
InfoRich universe build:    10-20 min (filtra daily cache)
Ticks download:             8-15 horas (~30,000-50,000 ticker-days estimados)
Bars/Labels/Weights:        3-5 horas (procesa ticks → DIB → labels)
ML Dataset consolidation:   30-60 min

TOTAL TIEMPO:               ~12-20 horas (puedes hacerlo por bloques en 3 días)
TOTAL STORAGE:              ~30-50 GB adicionales (ticks + bars + labels)


================================================================================
DECISION TREE (QUÉ HACER HOY)
================================================================================

OPCIÓN A: GENERAR SOLO PASO 1 Y 2 (1 hora, sin descargar ticks)
  → Genera daily_cache v2 + info_rich universe v2
  → Analiza distribución temporal (¿cuántos eventos info-rich hay por año?)
  → DECIDE si vale la pena descargar ticks históricos

OPCIÓN B: BACKFILL COMPLETO 2004-2019 (12-20 horas, por bloques)
  → Ejecuta pasos 1-4 completos
  → Entrena modelo con v1, v2, y fusión para comparar regímenes

OPCIÓN C: BACKFILL SELECTIVO (6-10 horas)
  → Solo descarga 2008-2009 (crisis financiera) + 2015-2019 (pre-COVID)
  → Ignora 2004-2007 y 2010-2014 si análisis muestra pocos eventos

```


La estructura v1/v2 está **bien planteada** . Siguiente paso: 

* generar [**daily_cache v2 (2004–2019)**](../H_Rich_universe_v2/9.5.1_daily_cache_v2.md) y 
* [**universo info-rich v2**](../H_Rich_universe_v2/9.5.2_rich_universe_v2.md) sin tocar nada de v1. 

Así validamos rápidamente cuántos “días con señal” hay por año y decidimos si merece la pena bajar ticks históricos por bloques.

Aquí tienes el plan de ejecución súper claro y seguro:

---

# ✅ Qué vamos a hacer ahora (Opción A)

1. **Daily cache v2 (2004–2019)**
   Calcula métricas diarias desde tus **1-min locales** (no llama a la API):

   * `close_d`, `vol_d`, `dollar_vol_d`
   * `%chg_d`, `rvol30`
   * (opcional) `vwap_d`, `return_d`

2. **Universo info-rich v2 (2004–2019)**
   Aplica el **mismo filtro** que en v1:
   `RVOL ≥ 2.0`, `|%chg| ≥ 15%`, `$-vol ≥ $5M`, y rango de precio (0.5–20$ si lo tienes activo en el config).
   Esto genera:

   * **watchlists diarias** (por fecha)
   * **TopN_12m rolling** (runners recurrentes por ventana móvil)

3. **No descargamos ticks todavía**
   Primero miramos **densidad temporal** (cuántos días info-rich por año/mes). Con eso decides si hacer backfill completo, parcial (2008–2009, 2015–2019), o nada.

---

# 🧪 Comandos listos (no pisan v1)

> Usa rutas **v2_2004_2019/** como pusiste, para no tocar v1.

### 1) Daily cache (2004–2019)

```bash
python scripts/fase_C_ingesta_tiks/build_daily_cache.py \
  --intraday-root raw/polygon/ohlcv_intraday_1m \
  --outdir processed/daily_cache/v2_2004_2019 \
  --from 2004-01-01 --to 2019-12-31 \
  --parallel 8
```

### 2) Universo info-rich (2004–2019)

Asegúrate que tu `configs/universe_config.yaml` tenga los umbrales oficiales:

```yaml
thresholds:
  rvol: 2.0
  pctchg: 0.15
  dvol: 5000000
  min_price: 0.5
  max_price: 20.0
  cap_max: 2000000000
processing:
  parallel: 8
  compression: zstd
  compression_level: 2
```

Ejecuta:

```bash
python scripts/fase_C_ingesta_tiks/build_dynamic_universe_optimized.py \
  --daily-cache processed/daily_cache/v2_2004_2019 \
  --outdir processed/universe/info_rich/v2_2004_2019 \
  --from 2004-01-01 --to 2019-12-31 \
  --config configs/universe_config.yaml
```

> Esto te creará:
>
> * `processed/universe/info_rich/v2_2004_2019/daily/date=YYYY-MM-DD/watchlist.parquet`
> * `processed/universe/info_rich/v2_2004_2019/stats/topN_12m_rolling.parquet`
> * `processed/universe/info_rich/v2_2004_2019/info_rich_tickers.csv`

---

# 🔎 Validación rápida que te recomiendo (post-Opción A)

1. **Densidad temporal**

   * nº de días info-rich por año (2004…2019)
   * media/mediana de info-rich tickers por día
   * top años con más señal (probable 2008–2009; 2015–2019)

2. **Top runners**

   * top 50 por `days_info_rich_252` en cada año (o por 12m rolling)
   * comprueba que salen los “sospechosos habituales” de cada régimen

3. **Sanity checks**

   * `%chg_d` tiene simetría razonable (colas gordas en 2008–2009)
   * `rvol30` no está sesgado por huecos (si ves outliers, revisa trading halts)
   * `$-vol` crece estacionalmente con el mercado (2016–2019 > 2004–2006)

Si estas distribuciones te convencen, pasamos a **descargar ticks solo días info-rich** por bloques (p. ej., 2004–2009, 2010–2014, 2015–2019) usando el mismo `download_trades_optimized.py` en **modo watchlists** apuntando a `processed/universe/info_rich/v2_2004_2019/daily`.

---

# 🧠 Notas y gotchas (para que no se nos escape nada)

* **Market cap**: ya vimos que el `tickers_dim.parquet` no tenía `market_cap`. Si quieres aplicar `cap_max` dentro del universo dinámico, añade **enriquecimiento** (merge con financials Polygon o tu CSV de cap) *antes* de etiquetar info-rich. Si no, deja el filtro por cap en “post-proceso” para informes/selección final (no bloquea Opción A).

* **Precio 0.5–20$**: mantenlo en el config (como ya lo tienes) para alinear con tu Playbook (evita penny chatarra y mega caps).

* **Versionado v1/v2**: tu layout propuesto es perfecto para **comparar regímenes** (pre-meme vs meme era). No mezcles outputs: guarda `dataset_v2_2004_2019.parquet` aparte, y si luego quieres, crea `dataset_all_2004_2025.parquet` (con columna `period`/`regime`).

* **DuckDB**: cuando termines Opción A, te recomiendo montar un `.duckdb` ligthweight para consultas cruzadas v1/v2 sin mover Parquets. (No hay que cambiar nada de ficheros.)

---

# ✅ Conclusión

* Tu estructura **v1/v2** está bien diseñada.
* **Opción A** es el paso correcto: **rápida, barata**, y te informa si merece la pena el backfill de ticks.
* No borres nada de v1. Añade v2 en paralelo y decide **después** si bajas ticks por bloques.

¿Lanzo los dos comandos de Opción A que he puesto arriba (daily cache + universo info-rich v2) y te dejo una plantilla de verificación (queries) para revisar densidad y top runners por año?
