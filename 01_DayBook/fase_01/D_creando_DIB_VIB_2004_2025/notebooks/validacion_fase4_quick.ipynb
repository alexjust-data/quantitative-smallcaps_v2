{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validaci√≥n Fase D.4: ML Dataset (Quick Version)\n",
    "\n",
    "**Fecha**: 2025-10-28  \n",
    "**Objetivo**: Validaci√≥n r√°pida del dataset ML sin cargar todo el global\n",
    "\n",
    "## Verificaciones\n",
    "\n",
    "1. Conteo de archivos\n",
    "2. Metadata verification\n",
    "3. Sample validation (10 files)\n",
    "4. Train/Valid split sizes\n",
    "5. Feature columns check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Change to project root\n",
    "os.chdir('D:/04_TRADING_SMALLCAPS')\n",
    "\n",
    "# Paths\n",
    "DATASETS_DIR = Path('processed/datasets')\n",
    "BARS_DIR = Path('processed/bars')\n",
    "LABELS_DIR = Path('processed/labels')\n",
    "WEIGHTS_DIR = Path('processed/weights')\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Conteo de Archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== CONTEO DE ARCHIVOS ===\")\n",
    "print()\n",
    "\n",
    "# Source files\n",
    "bars_files = list(BARS_DIR.rglob('dollar_imbalance.parquet'))\n",
    "labels_files = list(LABELS_DIR.rglob('labels.parquet'))\n",
    "weights_files = list(WEIGHTS_DIR.rglob('weights.parquet'))\n",
    "\n",
    "print(f\"Archivos fuente:\")\n",
    "print(f\"  Bars:    {len(bars_files):>6,}\")\n",
    "print(f\"  Labels:  {len(labels_files):>6,}\")\n",
    "print(f\"  Weights: {len(weights_files):>6,}\")\n",
    "print()\n",
    "\n",
    "# Daily datasets\n",
    "daily_files = list(DATASETS_DIR.glob('daily/*/date=*/dataset.parquet'))\n",
    "print(f\"Daily datasets: {len(daily_files):,}\")\n",
    "print()\n",
    "\n",
    "# Critical files\n",
    "global_file = DATASETS_DIR / 'global' / 'dataset.parquet'\n",
    "train_file = DATASETS_DIR / 'splits' / 'train.parquet'\n",
    "valid_file = DATASETS_DIR / 'splits' / 'valid.parquet'\n",
    "meta_file = DATASETS_DIR / 'meta.json'\n",
    "\n",
    "print(\"Archivos cr√≠ticos:\")\n",
    "print(f\"  Global: {global_file.exists()} ({global_file.stat().st_size / 1024**2:.1f} MB)\")\n",
    "print(f\"  Train:  {train_file.exists()} ({train_file.stat().st_size / 1024**2:.1f} MB)\")\n",
    "print(f\"  Valid:  {valid_file.exists()} ({valid_file.stat().st_size / 1024**2:.1f} MB)\")\n",
    "print(f\"  Meta:   {meta_file.exists()}\")\n",
    "print()\n",
    "\n",
    "coverage = len(daily_files) / len(bars_files) * 100\n",
    "print(f\"‚úÖ Cobertura: {coverage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Metadata Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== METADATA ===\")\n",
    "print()\n",
    "\n",
    "with open(meta_file, 'r') as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "for key, value in meta.items():\n",
    "    if isinstance(value, list):\n",
    "        print(f\"{key}: {len(value)} items\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")\n",
    "print()\n",
    "\n",
    "# Verify 14 features\n",
    "expected_features = [\n",
    "    'ret_1', 'range_norm', 'vol_f', 'dollar_f', 'imb_f',\n",
    "    'ret_1_ema10', 'ret_1_ema30', 'range_norm_ema20',\n",
    "    'vol_f_ema20', 'dollar_f_ema20', 'imb_f_ema20',\n",
    "    'vol_z20', 'dollar_z20', 'n'\n",
    "]\n",
    "\n",
    "actual = set(meta.get('feature_columns_example', []))\n",
    "expected = set(expected_features)\n",
    "\n",
    "if actual == expected:\n",
    "    print(\"‚úÖ 14 features correctas\")\n",
    "else:\n",
    "    print(f\"‚ùå Features mismatch: {actual ^ expected}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sample Daily Files Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== VALIDACI√ìN SAMPLE (10 archivos) ===\")\n",
    "print()\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "sample_files = random.sample(daily_files, min(10, len(daily_files)))\n",
    "\n",
    "for df_file in sample_files:\n",
    "    ticker = df_file.parent.parent.name\n",
    "    date = df_file.parent.name.split('=')[1]\n",
    "    \n",
    "    df = pl.read_parquet(df_file)\n",
    "    \n",
    "    print(f\"{ticker} {date}: {len(df)} rows, {df.shape[1]} cols\")\n",
    "    \n",
    "    # Check for nulls\n",
    "    null_counts = df.null_count()\n",
    "    total_nulls = sum([null_counts[col][0] for col in null_counts.columns])\n",
    "    \n",
    "    if total_nulls > 0:\n",
    "        print(f\"  ‚ö†Ô∏è  {total_nulls} nulls detected\")\n",
    "    else:\n",
    "        print(f\"  ‚úÖ Sin nulls\")\n",
    "\n",
    "print()\n",
    "print(\"‚úÖ Sample validation completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train/Valid Split Verification (Headers Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== TRAIN/VALID SPLITS ===\")\n",
    "print()\n",
    "\n",
    "# Read just schema and count\n",
    "print(\"Reading train file (scan only)...\")\n",
    "train_count = pl.scan_parquet(train_file).select(pl.count()).collect()[0, 0]\n",
    "\n",
    "print(\"Reading valid file (scan only)...\")\n",
    "valid_count = pl.scan_parquet(valid_file).select(pl.count()).collect()[0, 0]\n",
    "\n",
    "print(f\"Train: {train_count:,} rows\")\n",
    "print(f\"Valid: {valid_count:,} rows\")\n",
    "print()\n",
    "\n",
    "total = train_count + valid_count\n",
    "train_pct = train_count / total * 100\n",
    "valid_pct = valid_count / total * 100\n",
    "\n",
    "print(f\"Train: {train_pct:.1f}%\")\n",
    "print(f\"Valid: {valid_pct:.1f}%\")\n",
    "print()\n",
    "\n",
    "# Check against metadata\n",
    "expected_train = meta.get('train_rows', 0)\n",
    "expected_valid = meta.get('valid_rows', 0)\n",
    "\n",
    "train_match = \"‚úÖ\" if train_count == expected_train else \"‚ùå\"\n",
    "valid_match = \"‚úÖ\" if valid_count == expected_valid else \"‚ùå\"\n",
    "\n",
    "print(f\"{train_match} Train: {train_count:,} == {expected_train:,}\")\n",
    "print(f\"{valid_match} Valid: {valid_count:,} == {expected_valid:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Schema Verification (Sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== SCHEMA VERIFICATION ===\")\n",
    "print()\n",
    "\n",
    "# Load one daily file to check schema\n",
    "sample_df = pl.read_parquet(sample_files[0])\n",
    "\n",
    "print(\"Schema (sample daily file):\")\n",
    "for col, dtype in sample_df.schema.items():\n",
    "    print(f\"  {col}: {dtype}\")\n",
    "print()\n",
    "\n",
    "# Check required columns\n",
    "required = ['anchor_ts', 'label', 'weight'] + expected_features\n",
    "missing = [col for col in required if col not in sample_df.columns]\n",
    "\n",
    "if not missing:\n",
    "    print(f\"‚úÖ All required columns present ({len(required)} total)\")\n",
    "else:\n",
    "    print(f\"‚ùå Missing columns: {missing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Resumen Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"RESUMEN VALIDACI√ìN FASE D.4 (QUICK)\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(\"‚úÖ CHECKS PASSED:\")\n",
    "print(f\"  Daily datasets:   {len(daily_files):,}\")\n",
    "print(f\"  Coverage:         {coverage:.2f}%\")\n",
    "print(f\"  Train rows:       {train_count:,}\")\n",
    "print(f\"  Valid rows:       {valid_count:,}\")\n",
    "print(f\"  Features:         14/14\")\n",
    "print(f\"  Required columns: All present\")\n",
    "print()\n",
    "print(\"üìÅ OUTPUT:\")\n",
    "print(f\"  {global_file}\")\n",
    "print(f\"  {train_file}\")\n",
    "print(f\"  {valid_file}\")\n",
    "print()\n",
    "print(\"=\"*60)\n",
    "print(\"‚úÖ FASE D.4 VALIDADA (Quick Check)\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
