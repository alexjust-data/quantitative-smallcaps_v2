Has cerrado el **Bloque A**   
La estructura, los recuentos y las validaciones est√°n perfectos; el universo ya es **completo, limpio y sin sesgo de supervivencia**.

---

1. [Plan de implementaci√≥n](#-plan-de-implementaci√≥n)
2. [1) select_universe_cs.py - Filtrado de universo CS XNAS/XNYS](#1-select_universe_cspy)
3. [2) ingest_ohlcv_daily.py - Descarga OHLCV diario 20+ a√±os](#2-ingest_ohlcv_dailypy)
4. [3) ingest_ohlcv_intraday_minute.py - Descarga OHLCV intrad√≠a 1m](#3-ingest_ohlcv_intraday_minutepy)
5. [Notas finas (importantes)](#notas-finas-importantes)
6. [Launchers - Orquestaci√≥n multiproceso](#launchers)
   - [Launcher intrad√≠a 1m (sharding b√°sico)](#launcher-para-orquestar-la-descarga-intrad√≠a-1m-con-12-workers)
   - [Puntos clave](#puntos-clave)
   - [Launcher OHLCV diario](#launcher-para-ohlcv-diario-estilo-el-intrad√≠a-con-shards-logs-pid-files-statusstop-y-resume-lee-el-csv-de-universo-ya-filtrado-a-cs-xnasxnys-y-si-quieres-2b)
   - [C√≥mo usarlo (ejemplo)](#c√≥mo-usarlo-ejemplo)
   - [Launcher intrad√≠a con ventanas (paralelizaci√≥n avanzada)](#launcher-gemelo-para-intrad√≠a-que-paraleliza-por-ventanas-de-fechas-y-por-shards-de-tickers)
7. [Ejemplo de uso - 3 ventanas √ó 12 shards](#ejemplo-de-uso-3-ventanas--12-shards--36-procesos)

## üî¢ Plan de implementaci√≥n

**Bloque B** para que:

* **Solo descargue STOCKS (Common Stock = `type=CS`)**.
* **Solo de los exchanges con acciones comunes reales**: **XNAS y XNYS** (dejamos ARCX fuera porque es casi todo ETFs).
* **OHLCV diario**: toda la historia disponible (20+ a√±os en tu plan Advanced).
* **OHLCV intrad√≠a 1m**: tambi√©n toda la historia disponible (20+ a√±os), con paginaci√≥n por `cursor` y extracci√≥n correcta desde `next_url`.
* **Parquet particionado**: por **ticker/year** (idempotente).
* **12 workers** y **rate-limit** opcional en intrad√≠a, alineado con tu lanzador de 0.125 s (hecho al estilo de tu `launch_accelerated_0125s.py`). 

Debajo van **tres scripts** listos para copiar/pegar:

1. `select_universe_cs.py` ‚Üí filtra **solo CS** en **XNAS/XNYS** desde tu snapshot y (opcional) cap m√°ximo.
2. `ingest_ohlcv_daily.py` ‚Üí descarga **diario** para esa lista (20+ a√±os).
3. `ingest_ohlcv_intraday_minute.py` ‚Üí descarga **1m** (20+ a√±os) con **paginaci√≥n y rate-limit**.

---

### 1) `select_universe_cs.py`

Crea un CSV/Parquet con la **lista final de tickers objetivo** (solo CS, XNAS/XNYS, y opcional `< $2B` si quieres).

```python
#!/usr/bin/env python
# select_universe_cs.py
import argparse
from pathlib import Path
import polars as pl

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--snapdir", required=True, help=".../tickers_snapshot/snapshot_date=YYYY-MM-DD")
    ap.add_argument("--details", required=False, help=".../ticker_details/as_of_date=YYYY-MM-DD/details.parquet")
    ap.add_argument("--cap-max", type=float, default=None, help="Opcional: market cap < cap_max")
    ap.add_argument("--out-csv", required=True, help="Salida CSV (lista final de tickers)")
    ap.add_argument("--out-parquet", required=False, help="Salida Parquet (opcional)")
    args = ap.parse_args()

    df = pl.read_parquet(Path(args.snapdir)/"tickers.parquet")

    # Solo Common Stock en XNAS/XNYS
    df = df.filter(
        (pl.col("type") == "CS") &
        (pl.col("primary_exchange").is_in(["XNAS","XNYS"]))
    )

    if args.details and args.cap_max is not None:
        det = pl.read_parquet(args.details)
        mcands = [c for c in det.columns if c.lower() in ("market_cap","marketcap","market_capitalization")]
        if mcands:
            det = det.select(["ticker", mcands[0]]).rename({mcands[0]: "market_cap"})
            df = df.join(det, on="ticker", how="left")
            df = df.filter(pl.col("market_cap").is_not_null() & (pl.col("market_cap") < args.cap_max))

    out_cols = ["ticker","name","primary_exchange","type","sector","industry","cik","list_date"]
    out_cols = [c for c in out_cols if c in df.columns]
    df = df.select(out_cols).unique(subset=["ticker"]).sort("ticker")

    df.select(["ticker"]).write_csv(args.out_csv)
    if args.out_parquet:
        df.write_parquet(args.out_parquet)

    print(f"[OK] Universo CS XNAS/XNYS listo: {df.height:,} tickers")
    print(f"     CSV: {args.out_csv}")
    if args.out_parquet:
        print(f"     Parquet: {args.out_parquet}")

if __name__ == "__main__":
    main()
```

**Ejemplo:**

```bash
python select_universe_cs.py \
  --snapdir 01_fase_2/raw/polygon/reference/tickers_snapshot/snapshot_date=2025-10-19 \
  --details 01_fase_2/raw/polygon/reference/ticker_details/as_of_date=2025-10-19/details.parquet \
  --cap-max 2000000000 \
  --out-csv processed/universe/cs_xnas_xnys_under2b_2025-10-19.csv \
  --out-parquet processed/universe/cs_xnas_xnys_under2b_2025-10-19.parquet
```
> EJECUTADO 100%

**Nuevo universo generado:**  
* ‚úÖ processed/universe/cs_xnas_xnys_under2b.csv - 3,107 tickers

**Filtros aplicados:**  
* ‚úÖ Type = CS (Common Stock)  
* ‚úÖ Exchange = XNAS o XNYS (NASDAQ/NYSE)  
* ‚úÖ Market Cap < $2,000,000,000 (<$2B)  
* ‚úÖ Market Cap IS NOT NULL  

**Resultado:**  
Total snapshot: 11,845 tickers  
* ‚Üí Filtro CS + XNAS/XNYS: 5,002 tickers  
* ‚Üí Filtro market_cap < $2B: 3,107 tickers  
* ‚Üí Excluidos: 1,895 (sin market_cap o >= $2B)  

**Universo final:**
- Primeros: AACB, AACI, AAM, AAME, AAMI...  
- √öltimos: ...ZUMZ, ZURA, ZVIA, ZVRA, ZYXI  
- Total: 3,107 tickers listos para descarga OHLCV  

---

### 2) `ingest_ohlcv_daily.py`

Revisado para **solo tickers de tu universo** (CSV) y guardar **20+ a√±os**.

```python
#!/usr/bin/env python
# ingest_ohlcv_daily.py
import os, sys, time, argparse, datetime as dt
from pathlib import Path
from typing import Dict, Any, List, Optional
import urllib.parse as urlparse
import requests, polars as pl

BASE_URL = "https://api.polygon.io"
TIMEOUT = 35
RETRY_MAX = 8
BACKOFF = 1.6
PAGE_LIMIT = 50000
ADJUSTED = True

def log(m): print(f"[{dt.datetime.now():%F %T}] {m}", flush=True)

def parse_next_cursor(next_url: Optional[str]) -> Optional[str]:
    if not next_url: return None
    try:
        q = urlparse.urlparse(next_url).query
        qs = urlparse.parse_qs(q)
        cur = qs.get("cursor")
        return cur[0] if cur else None
    except Exception:
        return None

def http_get_json(url: str, params: Dict[str,Any], headers: Dict[str,str]) -> Dict[str,Any]:
    last = None
    for k in range(1, RETRY_MAX+1):
        try:
            r = requests.get(url, params=params, headers=headers, timeout=TIMEOUT)
            if r.status_code == 429:
                sl = int(r.headers.get("Retry-After","2"))
                log(f"429 -> sleep {sl}s"); time.sleep(sl); continue
            if 500 <= r.status_code < 600:
                sl = min(30, BACKOFF**k)
                log(f"{r.status_code} server -> backoff {sl:.1f}s"); time.sleep(sl); continue
            r.raise_for_status()
            return r.json()
        except Exception as e:
            last = e; sl = min(30, BACKOFF**k)
            log(f"GET err {e} -> backoff {sl:.1f}s"); time.sleep(sl)
    raise RuntimeError(f"Failed after {RETRY_MAX} attempts: {last}")

def fetch_daily(api_key: str, ticker: str, from_d: str, to_d: str) -> List[Dict[str,Any]]:
    url = f"{BASE_URL}/v2/aggs/ticker/{ticker}/range/1/day/{from_d}/{to_d}"
    headers = {"Authorization": f"Bearer {api_key}"}
    params = {"adjusted": str(ADJUSTED).lower(), "sort":"asc", "limit": PAGE_LIMIT}
    rows: List[Dict[str,Any]] = []
    cursor = None; pages=0
    while True:
        p = params.copy()
        if cursor: p["cursor"] = cursor
        data = http_get_json(url, p, headers) or {}
        res = data.get("results") or []
        rows.extend(res); pages += 1
        cursor = parse_next_cursor(data.get("next_url")) or data.get("next_url_cursor") or data.get("cursor")
        if not cursor: break
    log(f"{ticker}: {len(rows)} rows ({pages} pages)")
    return rows

def rows_to_df(rows: List[Dict[str,Any]], ticker: str) -> pl.DataFrame:
    if not rows:
        return pl.DataFrame({"ticker":[],"date":[],"t":[],"o":[],"h":[],"l":[],"c":[],"v":[],"n":[],"vw":[]})
    df = pl.from_dicts(rows)
    picks = {}
    for col, typ in [("t",pl.Int64),("o",pl.Float64),("h",pl.Float64),("l",pl.Float64),
                     ("c",pl.Float64),("v",pl.Float64),("n",pl.Int64),("vw",pl.Float64)]:
        picks[col] = (df[col].cast(typ) if col in df.columns else pl.Series(name=col, values=[], dtype=typ))
    out = pl.DataFrame(picks)
    if out.height:
        out = out.with_columns(
            pl.from_epoch(pl.col("t")/1000, time_unit="s").dt.strftime("%Y-%m-%d").alias("date"),
            pl.lit(ticker).alias("ticker")
        )
    else:
        out = out.with_columns(pl.Series(name="date", values=[], dtype=pl.Utf8), pl.lit(ticker).alias("ticker"))
    return out.select(["ticker","date","t","o","h","l","c","v","n","vw"])

def write_by_year(df: pl.DataFrame, outdir: Path, ticker: str) -> int:
    if df.height == 0: return 0
    df = df.with_columns(pl.col("date").str.slice(0,4).alias("year"))
    n=0
    for year, part in df.group_by("year"):
        y = year[0]; part = part.drop("year").sort("date")
        pdir = outdir/ticker/f"year={y}"; pdir.mkdir(parents=True, exist_ok=True)
        outp = pdir/"daily.parquet"
        if outp.exists():
            old = pl.read_parquet(outp)
            pl.concat([old, part], how="vertical_relaxed")\
              .unique(subset=["date"], keep="last")\
              .sort("date").write_parquet(outp)
        else:
            part.write_parquet(outp)
        n += 1
    return n

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--tickers-csv", required=True, help="CSV con columna 'ticker' (select_universe_cs.py)")
    ap.add_argument("--outdir", required=True, help="raw/polygon/ohlcv_daily")
    ap.add_argument("--from", dest="date_from", required=True)
    ap.add_argument("--to",   dest="date_to", required=True)
    ap.add_argument("--max-workers", type=int, default=12)
    args = ap.parse_args()

    api_key = os.getenv("POLYGON_API_KEY")
    if not api_key: sys.exit("Falta POLYGON_API_KEY")

    tickers = pl.read_csv(args.tickers_csv)["ticker"].to_list()
    outdir = Path(args.outdir); outdir.mkdir(parents=True, exist_ok=True)

    from concurrent.futures import ThreadPoolExecutor, as_completed
    log(f"Descargando DAILY {len(tickers):,} tickers [{args.date_from} ‚Üí {args.date_to}]")
    results=[]
    with ThreadPoolExecutor(max_workers=args.max_workers) as ex:
        futs = {ex.submit(fetch_daily, api_key, t, args.date_from, args.date_to): t for t in tickers}
        for i, fut in enumerate(as_completed(futs), 1):
            t = futs[fut]
            try:
                rows = fut.result()
                df = rows_to_df(rows, t)
                files = write_by_year(df, outdir, t)
                results.append(f"{t}: {df.height} rows, {files} files")
            except Exception as e:
                results.append(f"{t}: ERROR {e}")
            if i % 200 == 0:
                log(f"Progreso {i:,}/{len(tickers):,}")

    ok = sum("ERROR" not in r for r in results)
    err = len(results) - ok
    (outdir/"daily_download.log").write_text("\n".join(results), encoding="utf-8")
    log(f"Completado: OK={ok:,} ERR={err:,}")

if __name__ == "__main__":
    main()
```

**Ejemplo:**

```bash
python ingest_ohlcv_daily.py \
  --tickers-csv processed/universe/cs_xnas_xnys_under2b_2025-10-19.csv \
  --outdir 01_fase_2/raw/polygon/ohlcv_daily \
  --from 2004-01-01 --to 2025-10-20 \
  --max-workers 12
```

>‚óè Resumen de lo completado:
>
>  1. ‚úÖ Renombrado archivo universe: cs_xnas_xnys_under2b_2025-10-21.csv (3,107 tickers)
>  2. ‚úÖ Configurado encoding UTF-8 en ingest_ohlcv_daily.py
>  3. ‚úÖ A√±adido soporte para dotenv para cargar API key desde .env
>  4. ‚úÖ Iniciado descarga OHLCV Daily en background (ID: 22981e)
>
>  Par√°metros de ejecuci√≥n:  
>  Tickers: 3,107  
>  Per√≠odo: 2004-01-01 ‚Üí 2025-10-21 (21 a√±os)  
>  Workers: 12 paralelos  
>  Output: `D:\04_TRADING_SMALLCAPS\01_fase_2\raw\polygon\ohlcv_daily\`
>
>  Para monitorear el progreso:
>  Puedes usar este comando para ver el output actualizado:
>  * **Ver progreso en tiempo real (usa BashOutput con ID: 22981e)**
>
>  El script mostrar√° actualizaciones cada 200 tickers completados. Al finalizar, generar√° un log en `raw\polygon\ohlcv_daily\daily_download.log` con el resumen completo (OK/ERRORES).
>
>  La descarga est√° corriendo correctamente en background y continuar√° hasta completar los 3,107 tickers.

---

### 3) `ingest_ohlcv_intraday_minute.py`

Intraday **1m** para **20+ a√±os** (tu plan Advanced), con **paginaci√≥n por cursor**, **rate-limit** por *worker* (opcional), y escritura por **ticker/year-month**.

```python
#!/usr/bin/env python
# ingest_ohlcv_intraday_minute.py
import os, sys, time, argparse, datetime as dt
from pathlib import Path
from typing import Dict, Any, List, Optional
import urllib.parse as urlparse
import requests, polars as pl

BASE_URL = "https://api.polygon.io"
TIMEOUT = 40
RETRY_MAX = 8
BACKOFF = 1.6
PAGE_LIMIT = 50000
ADJUSTED = True

def log(m): print(f"[{dt.datetime.now():%F %T}] {m}", flush=True)

def parse_next_cursor(next_url: Optional[str]) -> Optional[str]:
    if not next_url: return None
    try:
        q = urlparse.urlparse(next_url).query
        qs = urlparse.parse_qs(q)
        cur = qs.get("cursor")
        return cur[0] if cur else None
    except Exception:
        return None

def http_get_json(url: str, params: Dict[str,Any], headers: Dict[str,str]) -> Dict[str,Any]:
    last=None
    for k in range(1, RETRY_MAX+1):
        try:
            r = requests.get(url, params=params, headers=headers, timeout=TIMEOUT)
            if r.status_code == 429:
                sl=int(r.headers.get("Retry-After","2")); log(f"429 -> sleep {sl}s"); time.sleep(sl); continue
            if 500 <= r.status_code < 600:
                sl=min(30, BACKOFF**k); log(f"{r.status_code} -> backoff {sl:.1f}s"); time.sleep(sl); continue
            r.raise_for_status()
            return r.json()
        except Exception as e:
            last=e; sl=min(30, BACKOFF**k); log(f"GET err {e} -> backoff {sl:.1f}s"); time.sleep(sl)
    raise RuntimeError(f"Failed after {RETRY_MAX} attempts: {last}")

def fetch_intraday(api_key: str, ticker: str, from_d: str, to_d: str, rate_limit_s: Optional[float]) -> List[Dict[str,Any]]:
    url = f"{BASE_URL}/v2/aggs/ticker/{ticker}/range/1/minute/{from_d}/{to_d}"
    headers = {"Authorization": f"Bearer {api_key}"}
    params = {"adjusted": str(ADJUSTED).lower(), "sort": "asc", "limit": PAGE_LIMIT}
    rows: List[Dict[str,Any]] = []
    cursor=None; pages=0
    while True:
        p = params.copy()
        if cursor: p["cursor"] = cursor
        data = http_get_json(url, p, headers) or {}
        res = data.get("results") or []
        rows.extend(res); pages += 1
        cursor = parse_next_cursor(data.get("next_url")) or data.get("next_url_cursor") or data.get("cursor")
        if rate_limit_s:
            time.sleep(rate_limit_s)  # control fino por worker (alineado a tu launcher)
        if not cursor: break
    log(f"{ticker}: {len(rows)} rows ({pages} pages) [1m]")
    return rows

def rows_to_df(rows: List[Dict[str,Any]], ticker: str) -> pl.DataFrame:
    if not rows:
        return pl.DataFrame({"ticker":[],"date":[],"minute":[],"t":[],"o":[],"h":[],"l":[],"c":[],"v":[],"n":[],"vw":[]})
    df = pl.from_dicts(rows)
    picks = {}
    for col, typ in [("t",pl.Int64),("o",pl.Float64),("h",pl.Float64),("l",pl.Float64),
                     ("c",pl.Float64),("v",pl.Float64),("n",pl.Int64),("vw",pl.Float64)]:
        picks[col] = (df[col].cast(typ) if col in df.columns else pl.Series(name=col, values=[], dtype=typ))
    out = pl.DataFrame(picks)
    if out.height:
        ts = pl.from_epoch(pl.col("t")/1000, time_unit="s")
        out = out.with_columns(
            ts.dt.strftime("%Y-%m-%d").alias("date"),
            ts.dt.strftime("%Y-%m-%d %H:%M").alias("minute"),
            pl.lit(ticker).alias("ticker")
        )
    else:
        out = out.with_columns(pl.Series(name="date", values=[], dtype=pl.Utf8),
                               pl.Series(name="minute", values=[], dtype=pl.Utf8),
                               pl.lit(ticker).alias("ticker"))
    return out.select(["ticker","date","minute","t","o","h","l","c","v","n","vw"])

def write_by_month(df: pl.DataFrame, outdir: Path, ticker: str) -> int:
    if df.height == 0: return 0
    df = df.with_columns(pl.col("date").str.slice(0,7).alias("ym"))  # YYYY-MM
    n=0
    for ym, part in df.group_by("ym"):
        y, m = ym[0].split("-")
        pdir = outdir/ticker/f"year={y}"/f"month={m}"
        pdir.mkdir(parents=True, exist_ok=True)
        outp = pdir/"minute.parquet"
        part = part.drop("ym").sort(["date","minute"])
        if outp.exists():
            old = pl.read_parquet(outp)
            pl.concat([old, part], how="vertical_relaxed")\
              .unique(subset=["minute"], keep="last")\
              .sort(["date","minute"]).write_parquet(outp)
        else:
            part.write_parquet(outp)
        n += 1
    return n

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--tickers-csv", required=True, help="CSV con columna 'ticker' (select_universe_cs.py)")
    ap.add_argument("--outdir", required=True, help="raw/polygon/ohlcv_intraday_1m")
    ap.add_argument("--from", dest="date_from", required=True)
    ap.add_argument("--to", dest="date_to", required=True)
    ap.add_argument("--max-workers", type=int, default=12)
    ap.add_argument("--rate-limit", type=float, default=0.125, help="segundos entre p√°ginas por worker (0 para desactivar)")
    args = ap.parse_args()

    api_key = os.getenv("POLYGON_API_KEY")
    if not api_key: sys.exit("Falta POLYGON_API_KEY")

    tickers = pl.read_csv(args.tickers_csv)["ticker"].to_list()
    outdir = Path(args.outdir); outdir.mkdir(parents=True, exist_ok=True)
    rate = args.rate_limit if args.rate_limit and args.rate_limit > 0 else None

    from concurrent.futures import ThreadPoolExecutor, as_completed
    log(f"Descargando 1m {len(tickers):,} tickers [{args.date_from} ‚Üí {args.date_to}] rate={rate}s workers={args.max_workers}")
    results=[]
    def worker(t):
        try:
            rows = fetch_intraday(api_key, t, args.date_from, args.date_to, rate)
            df = rows_to_df(rows, t)
            files = write_by_month(df, outdir, t)
            return f"{t}: {df.height} rows, {files} files"
        except Exception as e:
            return f"{t}: ERROR {e}"

    with ThreadPoolExecutor(max_workers=args.max_workers) as ex:
        futs = {ex.submit(worker, t): t for t in tickers}
        for i, fut in enumerate(as_completed(futs), 1):
            res = fut.result(); results.append(res)
            if i % 100 == 0: log(f"Progreso {i:,}/{len(tickers):,}")

    ok = sum("ERROR" not in r for r in results)
    err = len(results) - ok
    (outdir/"minute_download.log").write_text("\n".join(results), encoding="utf-8")
    log(f"Completado: OK={ok:,} ERR={err:,}")

if __name__ == "__main__":
    main()
```

**Ejemplo (20+ a√±os):**

```bash
python ingest_ohlcv_intraday_minute.py \
  --tickers-csv processed/universe/cs_xnas_xnys_under2b_2025-10-19.csv \
  --outdir 01_fase_2/raw/polygon/ohlcv_intraday_1m \
  --from 2004-01-01 --to 2025-10-20 \
  --max-workers 12 \
  --rate-limit 0.125
```

> La bandera `--rate-limit 0.125` est√° alineada con tu **launcher** acelerado (0.125 s entre p√°ginas por worker) y 12 workers. Si detectas 429 sostenidos, sube a `0.167` o `0.25`. 

---

### Notas finas (importantes)

* **ARCX fuera para CS**: tu confirmaci√≥n es correcta; ARCA es casi todo ETFs ‚Üí lo excluimos **por dise√±o** (solo XNAS/XNYS).
* **Solo CS (stocks)**: el selector produce la **lista can√≥nica de tickers** para todos los scripts, para no mezclar ETFs, warrants, preferred, etc.
* **20+ a√±os intrad√≠a**: con Advanced puedes ir ‚Äúhasta donde haya datos‚Äù. El script pagina con `cursor` y guarda por mes.
* **Idempotencia**: si relanzas, los writers **mergan** por fecha/minuto y conservan la √∫ltima versi√≥n.
* **Siguientes pasos**: cuando acabe intrad√≠a, pasamos a **Bar Construction** (Dollar/Volume/Imbalance Bars), **Daily Features** (RVOL, %chg, volatilidad), y **Labeling (Triple Barrier)**.

Si quieres, te preparo ahora mismo un **launcher estilo tuyo** (men√∫, logs, PID, confirmaci√≥n) para intrad√≠a 1m, enganchado a estos scripts.

## Launchers

### **launcher** para orquestar la **descarga intrad√≠a 1m** con **12 workers**


Un **launcher** para orquestar la **descarga intrad√≠a 1m** con **12 workers** (o los que indiques), **rate-limit** por worker, **logs**, **PID files**, **status/stop**, y opci√≥n de **resume** (excluir tickers ya descargados).

> Asume que ya tienes `ingest_ohlcv_intraday_minute.py` (el que te pas√©) en `scripts/`.
> Requisitos: `python>=3.10`, `polars`.

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
launch_intraday_smallcaps.py

Launcher para descargar intrad√≠a 1m de Polygon con m√∫ltiple procesos (workers),
sharding de tickers, control de ritmo (rate-limit) por worker, logs, PIDs,
status y stop. Inspirado en 'launch_accelerated_0125s.py'.

Uso t√≠pico:
  export POLYGON_API_KEY=xxx

  python launch_intraday_smallcaps.py start \
    --tickers-csv processed/universe/cs_xnas_xnys_under2b_2025-10-19.csv \
    --outdir 01_fase_2/raw/polygon/ohlcv_intraday_1m \
    --from 2004-01-01 --to 2025-10-20 \
    --shards 12 --per-shard-workers 4 \
    --rate-limit 0.125 \
    --workdir runs/intraday_1m_2025-10-20 \
    --ingest-script scripts/ingest_ohlcv_intraday_minute.py \
    --resume

Comandos:
  start  : crea shards, lanza N workers, guarda logs y PIDs
  status : muestra estado de los workers (vivos, logs resumen)
  stop   : termina los workers (SIGTERM -> SIGKILL)
"""

from __future__ import annotations
import os, sys, signal, time, argparse, datetime as dt
from pathlib import Path
import subprocess as sp
import polars as pl
from typing import List, Tuple

# ---------------------- utils ----------------------
def log(msg: str) -> None:
    print(f"[{dt.datetime.now():%F %T}] {msg}", flush=True)

def ensure_dir(p: Path) -> None:
    p.mkdir(parents=True, exist_ok=True)

def read_tickers(csv_path: Path) -> List[str]:
    df = pl.read_csv(csv_path)
    if "ticker" not in df.columns:
        raise SystemExit("El CSV debe contener columna 'ticker'.")
    return df["ticker"].drop_nulls().unique().to_list()

def existing_tickers(outdir: Path) -> set[str]:
    """Resume simple: si existe carpeta del ticker en outdir, lo consideramos 'ya iniciado'."""
    if not outdir.exists():
        return set()
    return {p.name for p in outdir.iterdir() if p.is_dir()}

def shard_list(items: List[str], k: int) -> List[List[str]]:
    k = max(1, k)
    n = len(items)
    base = n // k
    rem  = n % k
    shards = []
    start = 0
    for i in range(k):
        size = base + (1 if i < rem else 0)
        shards.append(items[start:start+size])
        start += size
    return shards

def write_shards(shards: List[List[str]], workdir: Path) -> List[Path]:
    shard_paths = []
    for i, lst in enumerate(shards):
        p = workdir / "shards" / f"tickers_shard_{i:02d}.csv"
        ensure_dir(p.parent)
        pl.DataFrame({"ticker": lst}).write_csv(p)
        shard_paths.append(p)
    return shard_paths

def pid_file(workdir: Path, i: int) -> Path:
    return workdir / "pids" / f"worker_{i:02d}.pid"

def log_file(workdir: Path, i: int) -> Path:
    return workdir / "logs" / f"worker_{i:02d}.log"

def is_alive(pid: int) -> bool:
    try:
        os.kill(pid, 0)
        return True
    except ProcessLookupError:
        return False
    except PermissionError:
        return True

# ---------------------- commands ----------------------
def cmd_start(args) -> None:
    workdir = Path(args.workdir)
    ensure_dir(workdir / "logs")
    ensure_dir(workdir / "pids")
    ensure_dir(workdir / "shards")

    tickers = read_tickers(Path(args.tickers_csv))
    log(f"Tickers originales: {len(tickers):,}")

    if args.resume:
        done = existing_tickers(Path(args.outdir))
        if done:
            tickers = [t for t in tickers if t not in done]
            log(f"Resume: excluidos {len(done):,} tickers que ya tienen carpeta en outdir.")
            log(f"Quedan por descargar: {len(tickers):,}")

    if len(tickers) == 0:
        log("No hay tickers que procesar. ¬øYa est√° todo descargado?")
        return

    shards = shard_list(tickers, args.shards)
    shard_paths = write_shards(shards, workdir)

    # Lanzar un proceso por shard
    procs: List[Tuple[int, sp.Popen]] = []
    env = os.environ.copy()
    if not env.get("POLYGON_API_KEY"):
        log("WARNING: POLYGON_API_KEY no est√° en el entorno.")

    for i, shard_csv in enumerate(shard_paths):
        # Cada worker invoca el ingest con su shard
        cmd = [
            sys.executable, args.ingest_script,
            "--tickers-csv", str(shard_csv),
            "--outdir", args.outdir,
            "--from", args.date_from,
            "--to", args.date_to,
            "--max-workers", str(args.per_shard_workers),
            "--rate-limit", str(args.rate_limit),
        ]
        lf = open(log_file(workdir, i), "a", encoding="utf-8")
        log(f"Lanzando worker {i:02d}: {' '.join(cmd)}")
        p = sp.Popen(cmd, stdout=lf, stderr=lf, cwd=Path.cwd(), env=env)
        ensure_dir((workdir / "pids"))
        pid_path = pid_file(workdir, i)
        with open(pid_path, "w", encoding="utf-8") as f:
            f.write(str(p.pid))
        procs.append((i, p))

    log(f"Workers lanzados: {len(procs)}")
    log(f"Logs en: {workdir/'logs'}  |  PIDs en: {workdir/'pids'}")

def cmd_status(args) -> None:
    workdir = Path(args.workdir)
    piddir = workdir / "pids"
    if not piddir.exists():
        log("No hay pids (¬øejecutaste start?).")
        return

    total = 0
    running = 0
    for pf in sorted(piddir.glob("worker_*.pid")):
        total += 1
        pid = int(pf.read_text().strip())
        alive = is_alive(pid)
        running += 1 if alive else 0
        print(f"{pf.name}: PID {pid}  {'RUNNING' if alive else 'STOPPED'}")

    print(f"\nWorkers totales: {total} | RUNNING: {running} | STOPPED: {total - running}")

    # Resumen r√°pido desde logs (l√≠neas finales)
    logdir = workdir / "logs"
    print("\n--- √öltimas l√≠neas de cada log ---")
    for lf in sorted(logdir.glob("worker_*.log")):
        try:
            tail = tail_file(lf, n=3)
            print(f"\n[{lf.name}]")
            for line in tail:
                print(line.rstrip())
        except Exception:
            pass

def tail_file(path: Path, n: int = 10) -> List[str]:
    # Tail simple: leer √∫ltimas n l√≠neas
    with open(path, "r", encoding="utf-8", errors="ignore") as f:
        lines = f.readlines()
    return lines[-n:]

def cmd_stop(args) -> None:
    workdir = Path(args.workdir)
    piddir = workdir / "pids"
    if not piddir.exists():
        log("No hay pids (¬øejecutaste start?).")
        return

    # SIGTERM -> espera -> SIGKILL
    pids = []
    for pf in sorted(piddir.glob("worker_*.pid")):
        try:
            pid = int(pf.read_text().strip())
            pids.append(pid)
        except Exception:
            pass

    if not pids:
        log("No se encontraron PIDs.")
        return

    log(f"Enviando SIGTERM a {len(pids)} procesos‚Ä¶")
    for pid in pids:
        try:
            os.kill(pid, signal.SIGTERM)
        except Exception:
            pass

    time.sleep(3.0)
    still = [pid for pid in pids if is_alive(pid)]
    if still:
        log(f"A√∫n vivos {len(still)} ‚Üí enviando SIGKILL")
        for pid in still:
            try:
                os.kill(pid, signal.SIGKILL)
            except Exception:
                pass

    log("STOP completado.")

# ---------------------- CLI ----------------------
def parse_args():
    ap = argparse.ArgumentParser(description="Launcher intrad√≠a 1m (multi-proceso) para Polygon")
    sub = ap.add_subparsers(dest="cmd", required=True)

    # Comunes a start/status/stop
    for name in ("start","status","stop"):
        sub.add_parser(name)

    # Args para start
    aps = sub.choices["start"]
    aps.add_argument("--tickers-csv", required=True, help="CSV con columna 'ticker' (universo final)")
    aps.add_argument("--outdir", required=True, help="Salida base intrad√≠a 1m (raw/polygon/ohlcv_intraday_1m)")
    aps.add_argument("--from", dest="date_from", required=True)
    aps.add_argument("--to", dest="date_to", required=True)
    aps.add_argument("--shards", type=int, default=12, help="N¬∫ de procesos (workers)")
    aps.add_argument("--per-shard-workers", type=int, default=4, help="Hilos internos por proceso (pasa a --max-workers del ingest)")
    aps.add_argument("--rate-limit", type=float, default=0.125, help="Segundos entre p√°ginas por worker")
    aps.add_argument("--workdir", required=True, help="Directorio de corrida (guardar√° logs/, pids/, shards/)")
    aps.add_argument("--ingest-script", required=True, help="Ruta a scripts/ingest_ohlcv_intraday_minute.py")
    aps.add_argument("--resume", action="store_true", help="Excluir tickers que ya tienen carpeta en outdir")

    # Args para status/stop
    for name in ("status","stop"):
        sub.choices[name].add_argument("--workdir", required=True, help="Directorio de corrida")

    return ap.parse_args()

def main():
    args = parse_args()
    if args.cmd == "start":
        cmd_start(args)
    elif args.cmd == "status":
        cmd_status(args)
    elif args.cmd == "stop":
        cmd_stop(args)
    else:
        raise SystemExit("Comando desconocido.")

if __name__ == "__main__":
    main()
```

### Puntos clave

* **Filtro `< 2B$`**: lo aplicas al generar el **CSV de universo** con `select_universe_cs.py`.
  Los dos ingesters (diario e intrad√≠a) **no filtran**; leen ese CSV (ya filtrado).
* **Sharding real**: el launcher divide los tickers en `--shards` CSVs y lanza **un proceso por shard**.
  Cada proceso invoca `ingest_ohlcv_intraday_minute.py` con sus propios `--max-workers` (hilos internos).
* **Resume sencillo**: con `--resume`, el launcher **excluye** tickers que ya tienen carpeta en `--outdir` (√∫til si relanzas).
* **Control de ritmo**: `--rate-limit 0.125` pasa directo al ingester (pausa entre p√°ginas por worker).
* **Supervisi√≥n**:

  * `status` muestra si los PIDs siguen vivos y ense√±a el ‚Äútail‚Äù de cada log.
  * `stop` env√≠a `SIGTERM` y luego `SIGKILL` si siguen vivos.

¬øQuieres que te haga otro launcher igual para **OHLCV diario** (mismo patr√≥n) o pasamos ya a **Bar Construction** (Dollar/Volume/Imbalance Bars) con tus 1m para preparar el **Labeling Pipeline**?

### Launcher para OHLCV diario (estilo el intrad√≠a), con shards, logs, PID files, status/stop y resume. 
* Lee el CSV de universo (ya filtrado a CS XNAS/XNYS y, si quieres, <2B$).
* con **shards**, **logs**, **PID files**, **status/stop** y **resume**. Lee el **CSV de universo** (ya filtrado a CS XNAS/XNYS y, si quieres, `<2B$`).

> Requisitos: `python>=3.10`, `polars`.
> Asume que ya tienes `scripts/ingest_ohlcv_daily.py`.

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
launch_daily_smallcaps.py

Launcher para descargar OHLCV diario desde Polygon con m√∫ltiples procesos (shards).
Cada shard invoca ingest_ohlcv_daily.py con sus propios workers. Gestiona logs,
PIDs, status/stop y resume (excluye tickers ya iniciados).

Uso:
  export POLYGON_API_KEY=xxx

  python launch_daily_smallcaps.py start \
    --tickers-csv processed/universe/cs_xnas_xnys_under2b_2025-10-19.csv \
    --outdir 01_fase_2/raw/polygon/ohlcv_daily \
    --from 2004-01-01 --to 2025-10-20 \
    --shards 12 --per-shard-workers 4 \
    --workdir runs/daily_2025-10-20 \
    --ingest-script scripts/ingest_ohlcv_daily.py \
    --resume

  python launch_daily_smallcaps.py status --workdir runs/daily_2025-10-20
  python launch_daily_smallcaps.py stop   --workdir runs/daily_2025-10-20
"""
from __future__ import annotations
import os, sys, signal, time, argparse, datetime as dt
from pathlib import Path
import subprocess as sp
from typing import List, Tuple
import polars as pl

# ---------- utils ----------
def log(msg: str) -> None:
    print(f"[{dt.datetime.now():%F %T}] {msg}", flush=True)

def ensure_dir(p: Path) -> None:
    p.mkdir(parents=True, exist_ok=True)

def read_tickers(csv_path: Path) -> List[str]:
    df = pl.read_csv(csv_path)
    if "ticker" not in df.columns:
        raise SystemExit("El CSV debe contener columna 'ticker'.")
    return df["ticker"].drop_nulls().unique().to_list()

def existing_tickers_daily(outdir: Path) -> set[str]:
    """Resume b√°sico: si hay carpeta del ticker en outdir, lo consideramos ya iniciado."""
    if not outdir.exists():
        return set()
    return {p.name for p in outdir.iterdir() if p.is_dir()}

def shard_list(items: List[str], k: int) -> List[List[str]]:
    k = max(1, k)
    n = len(items)
    base = n // k
    rem  = n % k
    shards = []
    start = 0
    for i in range(k):
        size = base + (1 if i < rem else 0)
        shards.append(items[start:start+size])
        start += size
    return shards

def write_shards(shards: List[List[str]], workdir: Path) -> List[Path]:
    shard_paths = []
    for i, lst in enumerate(shards):
        p = workdir / "shards" / f"tickers_shard_{i:02d}.csv"
        ensure_dir(p.parent)
        pl.DataFrame({"ticker": lst}).write_csv(p)
        shard_paths.append(p)
    return shard_paths

def pid_file(workdir: Path, i: int) -> Path:
    return workdir / "pids" / f"worker_{i:02d}.pid"

def log_file(workdir: Path, i: int) -> Path:
    return workdir / "logs" / f"worker_{i:02d}.log"

def is_alive(pid: int) -> bool:
    try:
        os.kill(pid, 0)
        return True
    except ProcessLookupError:
        return False
    except PermissionError:
        return True

def tail_file(path: Path, n: int = 10) -> List[str]:
    try:
        with open(path, "r", encoding="utf-8", errors="ignore") as f:
            lines = f.readlines()
        return lines[-n:]
    except Exception:
        return []

# ---------- commands ----------
def cmd_start(args) -> None:
    workdir = Path(args.workdir)
    ensure_dir(workdir / "logs")
    ensure_dir(workdir / "pids")
    ensure_dir(workdir / "shards")

    tickers = read_tickers(Path(args.tickers_csv))
    log(f"Tickers originales: {len(tickers):,}")

    if args.resume:
        done = existing_tickers_daily(Path(args.outdir))
        if done:
            tickers = [t for t in tickers if t not in done]
            log(f"Resume: excluidos {len(done):,} tickers con carpeta existente en outdir.")
            log(f"Quedan por descargar: {len(tickers):,}")

    if not tickers:
        log("No hay tickers que procesar. ¬øYa est√° todo descargado?")
        return

    shards = shard_list(tickers, args.shards)
    shard_paths = write_shards(shards, workdir)

    procs: List[Tuple[int, sp.Popen]] = []
    env = os.environ.copy()
    if not env.get("POLYGON_API_KEY"):
        log("WARNING: POLYGON_API_KEY no est√° en el entorno.")

    for i, shard_csv in enumerate(shard_paths):
        cmd = [
            sys.executable, args.ingest_script,
            "--tickers-csv", str(shard_csv),
            "--outdir", args.outdir,
            "--from", args.date_from,
            "--to", args.date_to,
            "--max-workers", str(args.per_shard_workers),
        ]
        lf = open(log_file(workdir, i), "a", encoding="utf-8")
        log(f"Lanzando worker {i:02d}: {' '.join(cmd)}")
        p = sp.Popen(cmd, stdout=lf, stderr=lf, cwd=Path.cwd(), env=env)
        with open(pid_file(workdir, i), "w", encoding="utf-8") as f:
            f.write(str(p.pid))
        procs.append((i, p))

    log(f"Workers lanzados: {len(procs)}")
    log(f"Logs: {workdir/'logs'}  |  PIDs: {workdir/'pids'}  |  Shards: {workdir/'shards'}")

def cmd_status(args) -> None:
    workdir = Path(args.workdir)
    piddir = workdir / "pids"
    if not piddir.exists():
        log("No hay pids (¬øejecutaste start?).")
        return

    total = 0; running = 0
    for pf in sorted(piddir.glob("worker_*.pid")):
        total += 1
        pid = int(pf.read_text().strip())
        alive = is_alive(pid)
        running += 1 if alive else 0
        print(f"{pf.name}: PID {pid}  {'RUNNING' if alive else 'STOPPED'}")

    print(f"\nWorkers totales: {total} | RUNNING: {running} | STOPPED: {total - running}")

    # Tail de logs
    logdir = workdir / "logs"
    print("\n--- √öltimas l√≠neas de cada log ---")
    for lf in sorted(logdir.glob("worker_*.log")):
        tail = tail_file(lf, n=3)
        if tail:
            print(f"\n[{lf.name}]")
            for line in tail:
                print(line.rstrip())

def cmd_stop(args) -> None:
    workdir = Path(args.workdir)
    piddir = workdir / "pids"
    if not piddir.exists():
        log("No hay pids (¬øejecutaste start?).")
        return

    pids = []
    for pf in sorted(piddir.glob("worker_*.pid")):
        try:
            pids.append(int(pf.read_text().strip()))
        except Exception:
            pass

    if not pids:
        log("No se encontraron PIDs.")
        return

    log(f"Enviando SIGTERM a {len(pids)} procesos‚Ä¶")
    for pid in pids:
        try: os.kill(pid, signal.SIGTERM)
        except Exception: pass

    time.sleep(3.0)
    still = [pid for pid in pids if is_alive(pid)]
    if still:
        log(f"A√∫n vivos {len(still)} ‚Üí enviando SIGKILL")
        for pid in still:
            try: os.kill(pid, signal.SIGKILL)
            except Exception: pass

    log("STOP completado.")

# ---------- CLI ----------
def parse_args():
    ap = argparse.ArgumentParser(description="Launcher OHLCV diario (multi-proceso) para Polygon")
    sub = ap.add_subparsers(dest="cmd", required=True)
    for name in ("start","status","stop"):
        sub.add_parser(name)

    aps = sub.choices["start"]
    aps.add_argument("--tickers-csv", required=True, help="CSV con columna 'ticker' (universo final)")
    aps.add_argument("--outdir", required=True, help="raw/polygon/ohlcv_daily")
    aps.add_argument("--from", dest="date_from", required=True)
    aps.add_argument("--to", dest="date_to", required=True)
    aps.add_argument("--shards", type=int, default=12, help="N¬∫ de procesos (shards)")
    aps.add_argument("--per-shard-workers", type=int, default=4, help="Hilos por proceso ‚Üí --max-workers del ingester")
    aps.add_argument("--workdir", required=True, help="Carpeta de corrida (logs/, pids/, shards/)")
    aps.add_argument("--ingest-script", required=True, help="Ruta a scripts/ingest_ohlcv_daily.py")
    aps.add_argument("--resume", action="store_true", help="Excluir tickers con carpeta ya creada en outdir")

    for name in ("status","stop"):
        sub.choices[name].add_argument("--workdir", required=True)

    return ap.parse_args()

def main():
    args = parse_args()
    if args.cmd == "start":
        cmd_start(args)
    elif args.cmd == "status":
        cmd_status(args)
    elif args.cmd == "stop":
        cmd_stop(args)
    else:
        raise SystemExit("Comando desconocido.")

if __name__ == "__main__":
    main()
```

### C√≥mo usarlo (ejemplo)

```bash
# 1) Universo (ya filtrado a CS XNAS/XNYS y <2B si quieres)
python select_universe_cs.py \
  --snapdir 01_fase_2/raw/polygon/reference/tickers_snapshot/snapshot_date=2025-10-19 \
  --details 01_fase_2/raw/polygon/reference/ticker_details/as_of_date=2025-10-19/details.parquet \
  --cap-max 2000000000 \
  --out-csv processed/universe/cs_xnas_xnys_under2b_2025-10-19.csv

# 2) Lanzar descarga DIARIA con 12 shards √ó 4 workers por shard
python launch_daily_smallcaps.py start \
  --tickers-csv processed/universe/cs_xnas_xnys_under2b_2025-10-19.csv \
  --outdir 01_fase_2/raw/polygon/ohlcv_daily \
  --from 2004-01-01 --to 2025-10-20 \
  --shards 12 --per-shard-workers 4 \
  --workdir runs/daily_2025-10-20 \
  --ingest-script scripts/ingest_ohlcv_daily.py \
  --resume

# 3) Ver estado
python launch_daily_smallcaps.py status --workdir runs/daily_2025-10-20

# 4) Parar
python launch_daily_smallcaps.py stop --workdir runs/daily_2025-10-20
```

**Notas r√°pidas**

* El **filtro `<2B$`** no est√° en el launcher; se aplica al **CSV** que generas con `select_universe_cs.py`.
* Idempotencia: los ingesters ya **mergan** por fecha, as√≠ que relanzar es seguro.
* Si quieres, te dejo tambi√©n un **launcher gemelo para el intrad√≠a** que encadene **varios rangos de fechas** (por ejemplo, 2004‚Äì2010, 2010‚Äì2016, 2016‚Äì2025) para paralelizar m√°s y repartir mejor el **cursor paging**.


### launcher gemelo para intrad√≠a que paraleliza por ventanas de fechas y por shards de tickers

¬°Vamos all√°! Aqu√≠ tienes el **launcher gemelo para intrad√≠a** que **paraleliza por *ventanas de fechas*** *y* por **shards de tickers**. Orquesta muchos procesos a la vez (por ejemplo, 3 ventanas √ó 12 shards = 36 procesos), cada uno invocando tu `ingest_ohlcv_intraday_minute.py` con su propio `--max-workers` y `--rate-limit`.

* Solo usa el **CSV de universo** (ya filtrado a `CS` y si quieres `<2B$`).
* Crea **logs**, **PIDs**, **shards**, y soporta **status/stop** y **resume** (excluye tickers que ya tengan carpeta en `outdir`).
* Ventanas configurables con `--windows 2004-01-01:2010-12-31,2011-01-01:2016-12-31,2017-01-01:2025-10-20`.

---

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
launch_intraday_windows.py

Launcher intrad√≠a (1m) con paralelizaci√≥n por VENTANAS de fechas y por SHARDS de tickers.
Cada (ventana, shard) es un proceso que invoca ingest_ohlcv_intraday_minute.py
con su propio --max-workers y --rate-limit.

Uso t√≠pico:
  export POLYGON_API_KEY=xxx

  python launch_intraday_windows.py start \
    --tickers-csv processed/universe/cs_xnas_xnys_under2b_2025-10-19.csv \
    --outdir 01_fase_2/raw/polygon/ohlcv_intraday_1m \
    --windows 2004-01-01:2010-12-31,2011-01-01:2016-12-31,2017-01-01:2025-10-20 \
    --shards 12 --per-shard-workers 4 \
    --rate-limit 0.125 \
    --workdir runs/intraday_1m_windows_2025-10-20 \
    --ingest-script scripts/ingest_ohlcv_intraday_minute.py \
    --resume

  python launch_intraday_windows.py status --workdir runs/intraday_1m_windows_2025-10-20
  python launch_intraday_windows.py stop   --workdir runs/intraday_1m_windows_2025-10-20
"""
from __future__ import annotations
import os, sys, signal, time, argparse, datetime as dt
from pathlib import Path
import subprocess as sp
from typing import List, Tuple
import polars as pl

# ---------------------- utils ----------------------
def log(msg: str) -> None:
    print(f"[{dt.datetime.now():%F %T}] {msg}", flush=True)

def ensure_dir(p: Path) -> None:
    p.mkdir(parents=True, exist_ok=True)

def read_tickers(csv_path: Path) -> List[str]:
    df = pl.read_csv(csv_path)
    if "ticker" not in df.columns:
        raise SystemExit("El CSV debe contener columna 'ticker'.")
    return df["ticker"].drop_nulls().unique().to_list()

def existing_tickers(outdir: Path) -> set[str]:
    """Resume simple: si existe carpeta del ticker en outdir, lo consideramos 'ya iniciado'."""
    if not outdir.exists():
        return set()
    return {p.name for p in outdir.iterdir() if p.is_dir()}

def shard_list(items: List[str], k: int) -> List[List[str]]:
    k = max(1, k)
    n = len(items)
    base = n // k
    rem  = n % k
    shards = []
    start = 0
    for i in range(k):
        size = base + (1 if i < rem else 0)
        shards.append(items[start:start+size])
        start += size
    return shards

def write_shards(shards: List[List[str]], workdir: Path, win_tag: str) -> List[Path]:
    shard_paths = []
    for i, lst in enumerate(shards):
        p = workdir / "shards" / f"{win_tag}_shard_{i:02d}.csv"
        ensure_dir(p.parent)
        pl.DataFrame({"ticker": lst}).write_csv(p)
        shard_paths.append(p)
    return shard_paths

def pid_file(workdir: Path, win_tag: str, i: int) -> Path:
    return workdir / "pids" / f"{win_tag}_worker_{i:02d}.pid"

def log_file(workdir: Path, win_tag: str, i: int) -> Path:
    return workdir / "logs" / f"{win_tag}_worker_{i:02d}.log"

def is_alive(pid: int) -> bool:
    try:
        os.kill(pid, 0)
        return True
    except ProcessLookupError:
        return False
    except PermissionError:
        return True

def tail_file(path: Path, n: int = 10) -> List[str]:
    try:
        with open(path, "r", encoding="utf-8", errors="ignore") as f:
            lines = f.readlines()
        return lines[-n:]
    except Exception:
        return []

def parse_windows(s: str) -> List[tuple[str,str,str]]:
    """
    Convierte 'YYYY-MM-DD:YYYY-MM-DD,...' en lista de tuplas (from, to, tag).
    tag se usa en nombres de log/pid/shard, ejemplo: '2004-01-01_2010-12-31'
    """
    wins = []
    for part in s.split(","):
        part = part.strip()
        if not part: continue
        a, b = part.split(":")
        a = a.strip(); b = b.strip()
        tag = f"{a}_{b}"
        wins.append((a, b, tag))
    if not wins:
        raise SystemExit("Debe especificar al menos una ventana en --windows")
    return wins

# ---------------------- commands ----------------------
def cmd_start(args) -> None:
    workdir = Path(args.workdir)
    ensure_dir(workdir / "logs")
    ensure_dir(workdir / "pids")
    ensure_dir(workdir / "shards")

    # Universo
    tickers = read_tickers(Path(args.tickers_csv))
    log(f"Tickers originales: {len(tickers):,}")

    if args.resume:
        done = existing_tickers(Path(args.outdir))
        if done:
            tickers = [t for t in tickers if t not in done]
            log(f"Resume: excluidos {len(done):,} tickers con carpeta existente en outdir.")
            log(f"Quedan por descargar: {len(tickers):,}")

    if not tickers:
        log("No hay tickers que procesar. ¬øYa est√° todo descargado?")
        return

    # Ventanas
    windows = parse_windows(args.windows)
    log(f"Ventanas: {', '.join([w[2] for w in windows])}")

    # Preparar env
    env = os.environ.copy()
    if not env.get("POLYGON_API_KEY"):
        log("WARNING: POLYGON_API_KEY no est√° en el entorno.")

    # Lanzar procesos por (ventana, shard)
    total_procs = 0
    for (w_from, w_to, w_tag) in windows:
        shards = shard_list(tickers, args.shards)
        shard_paths = write_shards(shards, workdir, win_tag=w_tag)

        for i, shard_csv in enumerate(shard_paths):
            cmd = [
                sys.executable, args.ingest_script,
                "--tickers-csv", str(shard_csv),
                "--outdir", args.outdir,
                "--from", w_from,
                "--to", w_to,
                "--max-workers", str(args.per_shard_workers),
                "--rate-limit", str(args.rate_limit),
            ]
            lf = open(log_file(workdir, w_tag, i), "a", encoding="utf-8")
            log(f"Lanzando [{w_tag}] shard {i:02d}: {' '.join(cmd)}")
            p = sp.Popen(cmd, stdout=lf, stderr=lf, cwd=Path.cwd(), env=env)
            ensure_dir(workdir / "pids")
            with open(pid_file(workdir, w_tag, i), "w", encoding="utf-8") as f:
                f.write(str(p.pid))
            total_procs += 1

    log(f"Procesos lanzados: {total_procs}")
    log(f"Logs: {workdir/'logs'} | PIDs: {workdir/'pids'} | Shards: {workdir/'shards'}")

def cmd_status(args) -> None:
    workdir = Path(args.workdir)
    piddir = workdir / "pids"
    if not piddir.exists():
        log("No hay pids (¬øejecutaste start?).")
        return

    pid_files = sorted(piddir.glob("*_worker_*.pid"))
    total = len(pid_files); running = 0
    for pf in pid_files:
        try:
            pid = int(pf.read_text().strip())
            alive = is_alive(pid)
            running += 1 if alive else 0
            print(f"{pf.name}: PID {pid}  {'RUNNING' if alive else 'STOPPED'}")
        except Exception:
            pass

    print(f"\nWorkers totales: {total} | RUNNING: {running} | STOPPED: {total - running}")

    # Tail por ventana
    logdir = workdir / "logs"
    print("\n--- √öltimas l√≠neas de cada log ---")
    for lf in sorted(logdir.glob("*.log")):
        tail = tail_file(lf, n=3)
        if tail:
            print(f"\n[{lf.name}]")
            for line in tail:
                print(line.rstrip())

def cmd_stop(args) -> None:
    workdir = Path(args.workdir)
    piddir = workdir / "pids"
    if not piddir.exists():
        log("No hay pids (¬øejecutaste start?).")
        return

    pids = []
    for pf in sorted(piddir.glob("*_worker_*.pid")):
        try:
            pids.append(int(pf.read_text().strip()))
        except Exception:
            pass

    if not pids:
        log("No se encontraron PIDs.")
        return

    log(f"Enviando SIGTERM a {len(pids)} procesos‚Ä¶")
    for pid in pids:
        try: os.kill(pid, signal.SIGTERM)
        except Exception: pass

    time.sleep(3.0)
    still = [pid for pid in pids if is_alive(pid)]
    if still:
        log(f"A√∫n vivos {len(still)} ‚Üí enviando SIGKILL")
        for pid in still:
            try: os.kill(pid, signal.SIGKILL)
            except Exception: pass

    log("STOP completado.")

# ---------------------- CLI ----------------------
def parse_args():
    ap = argparse.ArgumentParser(description="Launcher intrad√≠a 1m por ventanas y shards (Polygon)")
    sub = ap.add_subparsers(dest="cmd", required=True)
    for name in ("start","status","stop"):
        sub.add_parser(name)

    aps = sub.choices["start"]
    aps.add_argument("--tickers-csv", required=True, help="CSV con columna 'ticker' (universo final)")
    aps.add_argument("--outdir", required=True, help="raw/polygon/ohlcv_intraday_1m")
    aps.add_argument("--windows", required=True,
                     help="Lista 'from:to' separada por comas. Ej: 2004-01-01:2010-12-31,2011-01-01:2016-12-31")
    aps.add_argument("--shards", type=int, default=12, help="N¬∫ de procesos por ventana")
    aps.add_argument("--per-shard-workers", type=int, default=4, help="Hilos por proceso ‚Üí --max-workers del ingester")
    aps.add_argument("--rate-limit", type=float, default=0.125, help="Segundos entre p√°ginas por worker")
    aps.add_argument("--workdir", required=True, help="Carpeta de corrida (logs/, pids/, shards/)")
    aps.add_argument("--ingest-script", required=True, help="Ruta a scripts/ingest_ohlcv_intraday_minute.py")
    aps.add_argument("--resume", action="store_true", help="Excluir tickers con carpeta ya creada en outdir")

    for name in ("status","stop"):
        sub.choices[name].add_argument("--workdir", required=True)

    return ap.parse_args()

def main():
    args = parse_args()
    if args.cmd == "start":
        cmd_start(args)
    elif args.cmd == "status":
        cmd_status(args)
    elif args.cmd == "stop":
        cmd_stop(args)
    else:
        raise SystemExit("Comando desconocido.")

if __name__ == "__main__":
    main()
```

---

## Ejemplo de uso (3 ventanas √ó 12 shards = 36 procesos)

```bash
# Universo ya filtrado (CS XNAS/XNYS y <2B si procede)
UNIV=processed/universe/cs_xnas_xnys_under2b_2025-10-19.csv

python launch_intraday_windows.py start \
  --tickers-csv "$UNIV" \
  --outdir 01_fase_2/raw/polygon/ohlcv_intraday_1m \
  --windows 2004-01-01:2010-12-31,2011-01-01:2016-12-31,2017-01-01:2025-10-20 \
  --shards 12 --per-shard-workers 4 \
  --rate-limit 0.125 \
  --workdir runs/intraday_1m_windows_2025-10-20 \
  --ingest-script scripts/ingest_ohlcv_intraday_minute.py \
  --resume

python launch_intraday_windows.py status --workdir runs/intraday_1m_windows_2025-10-20
python launch_intraday_windows.py stop   --workdir runs/intraday_1m_windows_2025-10-20
```

### Notas finas

* **Filtro `<2B$`**: se sigue aplicando **solo** al generar el CSV de universo (como ya lo tienes).
* **Resume**: aqu√≠ es *por ticker* (si la carpeta existe, se excluye el ticker entero). Si quisieras **resume por ventana** (m√°s granular), puedo a√±adir un detector que compare existencia de `year/Êúà` espec√≠fico por rango.
* **Rate-limit**: 0.125 s/worker es el valor ‚Äútuneado‚Äù que ya usas; aj√∫stalo si ves 429 sostenidos.
* **Balanceo**: al paralelizar por ventanas, reduces la longitud de cada *cursor chain* y mejoras la saturaci√≥n de IO/CPU.

¬øQuieres que lo ajuste para **resume por ventana** (comprobando a√±os/meses dentro de cada rango) o pasamos ya a **Bar Construction** (Dollar/Volume/Imbalance Bars) con lo que ya estamos descargando?


>‚óè üöÄ ¬°Lanzamiento exitoso!
>
>  Configuraci√≥n confirmada:
>  - ‚úÖ 36 procesos paralelos lanzados
>  - ‚úÖ 3 ventanas de fechas √ó 12 shards de tickers
>  - ‚úÖ ~259 tickers por shard
>  - ‚úÖ 4 workers por shard
>  - ‚úÖ 0.125s rate limit
>
>  Ventanas lanzadas:
>  1. 2004-2010: 12 workers (3,107 tickers total)
>  2. 2011-2016: 12 workers (3,107 tickers total)
>  3. 2017-2025: 12 workers (3,107 tickers total)
>
>  Ubicaciones:
>  - Logs: runs\intraday_1m_windows_2025-10-21\logs
>  - PIDs: runs\intraday_1m_windows_2025-10-21\pids
>  - Shards: runs\intraday_1m_windows_2025-10-21\shards

‚óè `Diagn√≥stico: Sistema colapsado por paralelismo excesivo`

  Todos los 36 procesos murieron debido a sobrecarga del sistema. He identificado 3 problemas cr√≠ticos en los logs:

  Errores encontrados:

  1. SSL Certificate errors (cr√≠tico):
  Could not find a suitable TLS CA certificate bundle, invalid path:
  D:\04_TRADING_SMALLCAPS\.venv-smallcap\Lib\site-packages\certifi\cacert.pem
  2. Memory allocation errors (cr√≠tico):
  Unable to allocate output buffer
  3. Connection exhaustion:
  Max retries exceeded... Caused by SSLError(OSError(22, 'Invalid argument'))

  Causa ra√≠z:

  36 procesos √ó 4 workers/proceso = 144 threads HTTP paralelos

  Esto satur√≥:
  - Memoria del sistema (errores de buffer allocation)
  - Conexiones SSL/TLS disponibles
  - File handles del OS
  - Certificados SSL bajo presi√≥n de concurrencia

  ---

‚óè Reporte de Datos Descargados (Pre-Colapso)

  Resumen Ejecutivo

  Estado: Los datos descargados son v√°lidos y de alta calidad, pero el progreso fue m√≠nimo (2.1%) antes del colapso
  del sistema.

  ---
  Cobertura Global

  | M√©trica                    | Valor                  |
  |----------------------------|------------------------|
  | Universo total             | 3,107 tickers          |
  | Descargados con datos      | 66 tickers (2.12%)     |
  | Pendientes                 | 3,041 tickers (97.88%) |
  | Archivos parquet generados | 3,279 archivos         |
  | Tama√±o total               | 39.6 MB                |
  | Promedio archivos/ticker   | 49.7 files             |
  | Promedio tama√±o/archivo    | 12.4 KB                |

  ---
  Distribuci√≥n Temporal

  Los 66 tickers descargados tienen datos distribuidos as√≠:

  Por ventana temporal:
  - 2004-2010: 29 tickers √∫nicos
    - Ejemplos: ACNB, ALM, ANY, ASTC, CMRE
  - 2011-2016: 34 tickers √∫nicos
    - Ejemplos: ACNB, AIRG, AMTX, BLFS, BOTJ
  - 2017-2025: 25 tickers √∫nicos
    - Ejemplos: ACNB, ACR, AUNA, BTTC, CAEP

  Por a√±o (archivos parquet):
  2004: 206   2011: 157   2018:  35
  2005: 193   2012: 192   2019:  24
  2006: 175   2013: 192   2020:  24
  2007: 175   2014: 216   2021:  42
  2008: 217   2015: 267   2022:  55
  2009: 218   2016: 332   2023:  76
  2010: 207   2017:  48   2024:  97
                          2025: 131

  Patr√≥n: Buena cobertura en 2004-2016, menor en 2017-2020, recuperaci√≥n en 2021-2025.

  ---
  Calidad de Datos (5 muestras verificadas)

  | Ticker | Archivos | Filas (muestra) | Rango Fechas      | Nulls Precio | Estado |
  |--------|----------|-----------------|-------------------|--------------|--------|
  | ACNB   | 73       | 686             | 2013-01 ‚Üí 2013-07 | ‚úÖ No         | ‚úÖ OK   |
  | CTW    | 23       | 6,354           | 2017-03 ‚Üí 2025-10 | ‚úÖ No         | ‚úÖ OK   |
  | LIND   | 10       | 12,419          | 2016-01 ‚Üí 2016-11 | ‚úÖ No         | ‚úÖ OK   |
  | PDEX   | 156      | 1,074           | 2011-01 ‚Üí 2011-12 | ‚úÖ No         | ‚úÖ OK   |
  | XHLD   | 9        | 25,409          | 2025-02 ‚Üí 2025-08 | ‚úÖ No         | ‚úÖ OK   |

  Validaci√≥n:
  - ‚úÖ 10/10 tickers adicionales testeados ‚Üí 100% parquet v√°lidos
  - ‚úÖ Estructura correcta: 11 columnas (ticker, date, minute, t, o, h, l, c, v, n, vw)
  - ‚úÖ Sin valores nulos en precios (close, high, low, open)
  - ‚úÖ Particionamiento correcto: ticker/year=YYYY/month=MM/minute.parquet

  ---
  Tickers Completados (66 total)

  ACNB, ACR, AIRG, ALM, AMTX, ANY, ASTC, AUNA, BLFS, BOTJ,
  BTTC, BWFG, CAEP, CASI, CMRE, CTO, CTW, FCAP, FSBC, GCBC,
  GECC, GMRE, HFWA, HIFS, HIT, HMR, HPP, IPM, IPW, ISRL,
  IZEA, JBI, LAKE, LIND, LPBB, LWAC, MATH, MTC, NATH, NTWK,
  OPBK, OSIS, PAXH, PCSA, PDEX, PLMI, POAI, PPX, PSHG, RDW,
  RVPH, SDOT, SHCO, SLDB, SND, SOCA, STRR, TACO, TCI, TPB,
  UEIC, USEA, VRAX, WTTR, XHLD, YOSH

  ---
  Conclusiones

  Lo Bueno:
  - ‚úÖ Los 66 tickers descargados tienen datos completos y v√°lidos
  - ‚úÖ Estructura de particionamiento correcta (ticker/year/month)
  - ‚úÖ Sin corrupci√≥n de archivos parquet
  - ‚úÖ Cobertura temporal buena (2004-2025)
  - ‚úÖ El flag --resume funcionar√° correctamente (excluir√° estos 66 tickers)

  Lo Malo:
  - ‚ùå Solo 2.1% completado (66/3,107 tickers)
  - ‚ùå Sistema colaps√≥ por paralelismo excesivo
  - ‚ùå Quedan 3,041 tickers por descargar

  Recomendaci√≥n:
  Los datos son rescatables y v√°lidos. Podemos continuar con configuraci√≥n conservadora, el flag --resume saltar√°
  estos 66 tickers autom√°ticamente.

  ---

  Pr√≥ximos Pasos Sugeridos

```sh
  Opci√≥n A - Conservadora (recomendada):
  # Solo 8 threads (4 shards √ó 2 workers)
  # UNA ventana a la vez
  python scripts/fase_1_Bloque_B/tools/launch_intraday_windows.py start \
    --tickers-csv processed/universe/cs_xnas_xnys_under2b_2025-10-21.csv \
    --outdir raw/polygon/ohlcv_intraday_1m \
    --windows 2004-01-01:2010-12-31 \
    --shards 4 --per-shard-workers 2 --rate-limit 0.2 \
    --workdir runs/intraday_1m_2004-2010_v2 \
    --ingest-script scripts/ingest_ohlcv_intraday_minute.py \
    --resume

  Opci√≥n B - Moderada:
  # 12 threads (6 shards √ó 2 workers)
  --shards 6 --per-shard-workers 2 --rate-limit 0.15

  Tiempo estimado (Opci√≥n A, ~8 threads):
  - Tickers restantes: 3,041
  - Si cada ticker toma 30s promedio ‚Üí **25 horas por ventana**
  - Total 3 ventanas: 75 horas (3 d√≠as)
  ```


  ---


  ‚óè `Diagn√≥stico: Workers crasheando silenciosamente`

  Problema detectado:

  Los 6 workers se lanzan correctamente pero crash silenciosamente despu√©s de procesar ~8-10 tickers cada uno:
  - ‚úÖ Se descargaron 7 tickers nuevos (66 ‚Üí 73 total)
  - ‚ùå Cada worker proces√≥ solo ~10 de 507 tickers antes de morir
  - ‚ùå No hay log de "COMPLETADO" ‚Üí crash antes de finish

  Causa probable: Python 3.13 + ThreadPoolExecutor con m√∫ltiples workers internos est√° causando problemas de
  SSL/conexiones concurrentes (el mismo problema que antes, pero a menor escala).

  ---
  Soluci√≥n: Cambiar arquitectura de paralelismo

  En lugar de 6 shards √ó 2 workers internos = 12 threads, vamos a usar:

  12 shards √ó 1 worker = 12 procesos independientes

  Esto evita:
  - ‚ùå ThreadPoolExecutor dentro de cada proceso (problem√°tico en Python 3.13)
  - ‚ùå M√∫ltiples conexiones SSL desde el mismo proceso
  - ‚úÖ Cada proceso maneja 1 ticker a la vez ‚Üí m√°s estable

  ---
  NUEVO Plan: Escala Gradual con Arquitectura Ajustada

  Fase 1: Conservadora (12 procesos √ó 1 worker = 12 threads)

```sh
  python scripts/fase_1_Bloque_B/tools/launch_intraday_windows.py stop \
    --workdir runs/intraday_phase1_moderate

  python scripts/fase_1_Bloque_B/tools/launch_intraday_windows.py start \
    --tickers-csv processed/universe/cs_xnas_xnys_under2b_2025-10-21.csv \
    --outdir raw/polygon/ohlcv_intraday_1m \
    --windows 2004-01-01:2010-12-31 \
    --shards 12 --per-shard-workers 1 --rate-limit 0.2 \
    --workdir runs/intraday_1m_stable \
    --ingest-script scripts/fase_1_Bloque_B/ingest_ohlcv_intraday_minute.py \
    --resume

  # Monitoreo (cada 5-10 minutos):

  python scripts/fase_1_Bloque_B/tools/launch_intraday_windows.py status \
    --workdir runs/intraday_1m_stable

  # Si estable durante 30min ‚Üí Fase 2 (16 procesos):

  python scripts/fase_1_Bloque_B/tools/launch_intraday_windows.py stop --workdir runs/intraday_1m_stable

  python scripts/fase_1_Bloque_B/tools/launch_intraday_windows.py start \
    --shards 16 --per-shard-workers 1 --rate-limit 0.18 \
    --workdir runs/intraday_1m_phase2 \
    ... (resto igual)

  # Si estable otros 30min ‚Üí Fase 3 (20 procesos):

  --shards 20 --per-shard-workers 1 --rate-limit 0.15
```

‚óè ‚ùå DIAGN√ìSTICO CR√çTICO: Fuga de Memoria en Procesos Python

  üìä Hallazgos

  1. Causa Ra√≠z Identificada:
  GET error Unable to allocate output buffer. -> backoff X.Xs
  Failed to resolve 'api.polygon.io' (DNS failure por agotamiento de recursos)
  Error -4 while decompressing data (gzip decompression failure)

  2. Estado del Sistema:
  - ‚úÖ Descargados: 111 tickers (38 nuevos en esta corrida)
  - ‚ùå 16 procesos Python vivos consumiendo ~25GB RAM total
  - ‚ùå Procesos individuales: 1GB - 4.2GB cada uno
  - ‚ùå Workers atascados en loop de errores de memoria

  3. Patr√≥n de Fallo:
  Inicio ‚Üí Procesa 5-15 tickers ‚Üí Memoria crece ‚Üí "Unable to allocate buffer" ‚Üí
  ‚Üí Backoff infinito ‚Üí Proceso zombie (STOPPED pero vivo)


  ------------------------------------------------------

  

`HAY QUE ARREGLAR ESTO=============================`