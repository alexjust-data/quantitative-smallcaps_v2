{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track A: Validaci√≥n Profesional Daily OHLCV Pipeline\n",
    "\n",
    "**Autor**: Data Science Team\n",
    "\n",
    "**Fecha**: 2025-10-28\n",
    "\n",
    "**Objetivo**: Validaci√≥n exhaustiva del pipeline de agregaci√≥n 1m ‚Üí Daily OHLCV para Track A (Event Detectors E1/E4/E7/E8)\n",
    "\n",
    "---\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "Este notebook valida de forma rigurosa:\n",
    "\n",
    "1. **Cobertura y completitud del dataset**\n",
    "2. **Estructura y schema de datos**\n",
    "3. **Calidad de los datos (nulls, duplicados, outliers)**\n",
    "4. **Integridad OHLC (reglas de mercado)**\n",
    "5. **Correcci√≥n de agregaci√≥n** (comparaci√≥n vs fuente)\n",
    "6. **Distribuciones estad√≠sticas**\n",
    "7. **Time series properties**\n",
    "8. **Recomendaciones y siguientes pasos**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, date\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n\n",
    "sns.set_theme(style='whitegrid', palette='husl')\n",
    "pl.Config.set_tbl_rows(10)\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Rutas\n",
    "DAILY_OHLCV_ROOT = Path('D:/04_TRADING_SMALLCAPS/processed/daily_ohlcv')\n",
    "INTRADAY_1M_ROOT = Path('D:/04_TRADING_SMALLCAPS/raw/polygon/ohlcv_intraday_1m')\n",
    "\n",
    "print('‚úÖ Environment configurado')\n",
    "print(f'   Daily OHLCV: {DAILY_OHLCV_ROOT}')\n",
    "print(f'   Intraday 1m: {INTRADAY_1M_ROOT}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Discovery: Estructura y Cobertura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descubrir estructura\n",
    "ticker_dirs = [d for d in DAILY_OHLCV_ROOT.iterdir() if d.is_dir()]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"1.1 ESTRUCTURA DE DIRECTORIOS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(f\"Total tickers procesados: {len(ticker_dirs):,}\")\n",
    "print(f\"Tickers esperados: 8,620\")\n",
    "print(f\"Coverage: {len(ticker_dirs)/8620*100:.2f}%\")\n",
    "print()\n",
    "\n",
    "# Analizar estructura de archivos\n",
    "with_parquet = 0\n",
    "with_success = 0\n",
    "with_both = 0\n",
    "empty_dirs = 0\n",
    "\n",
    "for ticker_dir in ticker_dirs:\n",
    "    has_parquet = (ticker_dir / 'daily.parquet').exists()\n",
    "    has_success = (ticker_dir / '_SUCCESS').exists()\n",
    "    \n",
    "    if has_parquet and has_success:\n",
    "        with_both += 1\n",
    "    elif has_parquet:\n",
    "        with_parquet += 1\n",
    "    elif has_success:\n",
    "        with_success += 1\n",
    "    else:\n",
    "        empty_dirs += 1\n",
    "\n",
    "print(\"Distribuci√≥n de archivos:\")\n",
    "print(f\"  daily.parquet + _SUCCESS: {with_both:,} ({with_both/len(ticker_dirs)*100:.2f}%)\")\n",
    "print(f\"  Solo daily.parquet: {with_parquet:,}\")\n",
    "print(f\"  Solo _SUCCESS: {with_success:,}\")\n",
    "print(f\"  Directorios vac√≠os: {empty_dirs:,}\")\n",
    "print()\n",
    "print(f\"‚úÖ Tickers v√°lidos: {with_both:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"1.2 EJEMPLO DE ESTRUCTURA F√çSICA\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Seleccionar 3 tickers aleatorios\n",
    "import random\n",
    "random.seed(42)\n",
    "sample_dirs = random.sample([d for d in ticker_dirs if (d/'daily.parquet').exists()], 3)\n",
    "\n",
    "for ticker_dir in sample_dirs:\n",
    "    print(f\"Ticker: {ticker_dir.name}\")\n",
    "    print(f\"  Ubicaci√≥n: {ticker_dir.relative_to(DAILY_OHLCV_ROOT.parent)}\")\n",
    "    \n",
    "    # Listar archivos y tama√±os\n",
    "    for file in ticker_dir.iterdir():\n",
    "        size_kb = file.stat().st_size / 1024\n",
    "        print(f\"  ‚îú‚îÄ {file.name:20s} ({size_kb:>8.2f} KB)\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Schema y Tipado de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"2.1 SCHEMA ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Expected schema\n",
    "EXPECTED_SCHEMA = {\n",
    "    'ticker': pl.Utf8,\n",
    "    'date': pl.Date,\n",
    "    'o': pl.Float64,\n",
    "    'h': pl.Float64,\n",
    "    'l': pl.Float64,\n",
    "    'c': pl.Float64,\n",
    "    'v': pl.Float64,\n",
    "    'n': pl.Int64,\n",
    "    'dollar': pl.Float64\n",
    "}\n",
    "\n",
    "print(\"Expected Schema:\")\n",
    "print(f\"  Columns: {len(EXPECTED_SCHEMA)}\")\n",
    "for col, dtype in EXPECTED_SCHEMA.items():\n",
    "    print(f\"  ‚îú‚îÄ {col:10s}: {dtype}\")\n",
    "print()\n",
    "\n",
    "# Verificar contra 20 tickers aleatorios\n",
    "sample_tickers = random.sample([d for d in ticker_dirs if (d/'daily.parquet').exists()], min(20, len(ticker_dirs)))\n",
    "\n",
    "schema_matches = 0\n",
    "schema_mismatches = []\n",
    "\n",
    "for ticker_dir in sample_tickers:\n",
    "    df = pl.read_parquet(ticker_dir / 'daily.parquet')\n",
    "    \n",
    "    if df.schema == EXPECTED_SCHEMA:\n",
    "        schema_matches += 1\n",
    "    else:\n",
    "        schema_mismatches.append({\n",
    "            'ticker': ticker_dir.name,\n",
    "            'schema': df.schema\n",
    "        })\n",
    "\n",
    "print(f\"Schema validation (n={len(sample_tickers)}):\")\n",
    "print(f\"  ‚úÖ Matches: {schema_matches}/{len(sample_tickers)}\")\n",
    "print(f\"  ‚ùå Mismatches: {len(schema_mismatches)}/{len(sample_tickers)}\")\n",
    "print()\n",
    "\n",
    "if schema_mismatches:\n",
    "    print(\"‚ö†Ô∏è  Schema mismatches found:\")\n",
    "    for mismatch in schema_mismatches[:3]:\n",
    "        print(f\"  Ticker: {mismatch['ticker']}\")\n",
    "        print(f\"  Schema: {mismatch['schema']}\")\n",
    "else:\n",
    "    print(\"‚úÖ ALL schemas match expected structure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Contenido Interno: Deep Dive en los Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"3.1 DEEP DIVE: CONTENIDO REAL DE 3 TICKERS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "for ticker_dir in sample_dirs:\n",
    "    ticker = ticker_dir.name\n",
    "    df = pl.read_parquet(ticker_dir / 'daily.parquet')\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"TICKER: {ticker}\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "    \n",
    "    # Metadata b√°sica\n",
    "    print(f\"üìä METADATA:\")\n",
    "    print(f\"  Total d√≠as: {len(df):,}\")\n",
    "    print(f\"  Fecha inicio: {df['date'].min()}\")\n",
    "    print(f\"  Fecha fin: {df['date'].max()}\")\n",
    "    print(f\"  D√≠as totales: {(df['date'].max() - df['date'].min()).days}\")\n",
    "    print(f\"  Tama√±o en memoria: {df.estimated_size() / 1024:.2f} KB\")\n",
    "    print()\n",
    "    \n",
    "    # Primeras 5 filas\n",
    "    print(f\"üìÑ PRIMERAS 5 FILAS:\")\n",
    "    print(df.head(5))\n",
    "    print()\n",
    "    \n",
    "    # √öltimas 5 filas\n",
    "    print(f\"üìÑ √öLTIMAS 5 FILAS:\")\n",
    "    print(df.tail(5))\n",
    "    print()\n",
    "    \n",
    "    # Estad√≠sticas descriptivas\n",
    "    print(f\"üìà ESTAD√çSTICAS DESCRIPTIVAS:\")\n",
    "    print(df.select(['o', 'h', 'l', 'c', 'v', 'n', 'dollar']).describe())\n",
    "    print()\n",
    "    \n",
    "    # D√≠a con mayor volumen\n",
    "    max_vol_day = df.filter(pl.col('v') == pl.col('v').max())\n",
    "    print(f\"üî• D√çA CON MAYOR VOLUMEN:\")\n",
    "    print(max_vol_day)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Quality: NULLs, Duplicados, Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"4.1 DATA QUALITY ASSESSMENT (sample 50 tickers)\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "sample_quality = random.sample([d for d in ticker_dirs if (d/'daily.parquet').exists()], min(50, len(ticker_dirs)))\n",
    "\n",
    "# Contadores\n",
    "null_counts = {col: 0 for col in EXPECTED_SCHEMA.keys()}\n",
    "duplicate_dates = 0\n",
    "unordered_dates = 0\n",
    "negative_values = {'o': 0, 'h': 0, 'l': 0, 'c': 0, 'v': 0, 'n': 0, 'dollar': 0}\n",
    "zero_volume = 0\n",
    "total_rows = 0\n",
    "\n",
    "for ticker_dir in sample_quality:\n",
    "    df = pl.read_parquet(ticker_dir / 'daily.parquet')\n",
    "    total_rows += len(df)\n",
    "    \n",
    "    # NULLs\n",
    "    for col in EXPECTED_SCHEMA.keys():\n",
    "        null_counts[col] += df[col].is_null().sum()\n",
    "    \n",
    "    # Duplicados\n",
    "    if df['date'].n_unique() < len(df):\n",
    "        duplicate_dates += 1\n",
    "    \n",
    "    # Orden\n",
    "    dates = df['date'].to_list()\n",
    "    if dates != sorted(dates):\n",
    "        unordered_dates += 1\n",
    "    \n",
    "    # Valores negativos\n",
    "    for col in ['o', 'h', 'l', 'c', 'v', 'n', 'dollar']:\n",
    "        if df[col].min() < 0:\n",
    "            negative_values[col] += 1\n",
    "    \n",
    "    # Volumen cero\n",
    "    if df.filter(pl.col('v') == 0).height > 0:\n",
    "        zero_volume += 1\n",
    "\n",
    "print(f\"Sample size: {len(sample_quality)} tickers, {total_rows:,} rows\\n\")\n",
    "\n",
    "print(\"NULL VALUES:\")\n",
    "total_nulls = sum(null_counts.values())\n",
    "for col, count in null_counts.items():\n",
    "    pct = (count / total_rows * 100) if total_rows > 0 else 0\n",
    "    print(f\"  {col:10s}: {count:>6,} ({pct:>6.3f}%)\")\n",
    "print(f\"  TOTAL: {total_nulls:>6,}\\n\")\n",
    "\n",
    "print(\"DUPLICATE DATES:\")\n",
    "print(f\"  Tickers con duplicados: {duplicate_dates}/{len(sample_quality)}\\n\")\n",
    "\n",
    "print(\"TEMPORAL ORDER:\")\n",
    "print(f\"  Tickers desordenados: {unordered_dates}/{len(sample_quality)}\\n\")\n",
    "\n",
    "print(\"NEGATIVE VALUES:\")\n",
    "for col, count in negative_values.items():\n",
    "    print(f\"  {col:10s}: {count} tickers\")\n",
    "print()\n",
    "\n",
    "print(\"ZERO VOLUME DAYS:\")\n",
    "print(f\"  Tickers con volumen=0: {zero_volume}/{len(sample_quality)}\\n\")\n",
    "\n",
    "# Resumen\n",
    "issues = total_nulls + duplicate_dates + unordered_dates + sum(negative_values.values())\n",
    "if issues == 0:\n",
    "    print(\"‚úÖ DATA QUALITY: EXCELLENT (no issues found)\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  DATA QUALITY: {issues} issues found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Integridad OHLC: Market Rules Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"5.1 OHLC INTEGRITY RULES\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"Market rules:\")\n",
    "print(\"  1. high >= max(open, close)\")\n",
    "print(\"  2. low <= min(open, close)\")\n",
    "print(\"  3. high >= low\")\n",
    "print(\"  4. close > 0 (no negative prices)\")\n",
    "print()\n",
    "\n",
    "violations = {'rule1': 0, 'rule2': 0, 'rule3': 0, 'rule4': 0}\n",
    "total_rows_checked = 0\n",
    "\n",
    "for ticker_dir in sample_quality:\n",
    "    df = pl.read_parquet(ticker_dir / 'daily.parquet')\n",
    "    total_rows_checked += len(df)\n",
    "    \n",
    "    # Rule 1: high >= max(open, close)\n",
    "    v1 = df.filter(pl.col('h') < pl.max_horizontal('o', 'c'))\n",
    "    violations['rule1'] += len(v1)\n",
    "    \n",
    "    # Rule 2: low <= min(open, close)\n",
    "    v2 = df.filter(pl.col('l') > pl.min_horizontal('o', 'c'))\n",
    "    violations['rule2'] += len(v2)\n",
    "    \n",
    "    # Rule 3: high >= low\n",
    "    v3 = df.filter(pl.col('h') < pl.col('l'))\n",
    "    violations['rule3'] += len(v3)\n",
    "    \n",
    "    # Rule 4: close > 0\n",
    "    v4 = df.filter(pl.col('c') <= 0)\n",
    "    violations['rule4'] += len(v4)\n",
    "\n",
    "print(f\"Rows checked: {total_rows_checked:,}\\n\")\n",
    "\n",
    "print(\"VIOLATIONS:\")\n",
    "total_violations = sum(violations.values())\n",
    "for rule, count in violations.items():\n",
    "    pct = (count / total_rows_checked * 100) if total_rows_checked > 0 else 0\n",
    "    print(f\"  {rule}: {count:>6,} ({pct:>8.4f}%)\")\n",
    "print(f\"  TOTAL: {total_violations:>6,}\\n\")\n",
    "\n",
    "if total_violations == 0:\n",
    "    print(\"‚úÖ OHLC INTEGRITY: PERFECT (0 violations)\")\n",
    "else:\n",
    "    print(f\"‚ùå OHLC INTEGRITY: {total_violations:,} violations ({total_violations/total_rows_checked*100:.4f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Agregaci√≥n Correcta: Verificaci√≥n Matem√°tica vs Fuente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 80)\nprint(\"6.1 AGGREGATION VERIFICATION (1m ‚Üí Daily)\")\nprint(\"=\" * 80)\nprint()\nprint(\"Buscando 3 tickers con datos 1m disponibles...\\n\")\n\n# Buscar 3 tickers que tengan datos 1m\ntest_tickers = []\n\nfor ticker_dir in sample_quality:\n    if len(test_tickers) >= 3:\n        break\n    \n    ticker = ticker_dir.name\n    intraday_dir = INTRADAY_1M_ROOT / ticker\n    \n    if intraday_dir.exists():\n        minute_files = list(intraday_dir.rglob('minute.parquet'))\n        if len(minute_files) > 0:\n            test_tickers.append({\n                'ticker': ticker,\n                'ticker_dir': ticker_dir,\n                'intraday_dir': intraday_dir,\n                'minute_files': minute_files\n            })\n\nif len(test_tickers) == 0:\n    print(\"‚ö†Ô∏è  No se encontraron tickers con datos 1m en el sample\")\n    print(\"   Skipping aggregation verification\")\nelse:\n    print(f\"‚úÖ Tickers seleccionados: {len(test_tickers)}\\n\")\n    \n    # Procesar cada ticker\n    for test_info in test_tickers:\n        ticker = test_info['ticker']\n        ticker_dir = test_info['ticker_dir']\n        minute_files = test_info['minute_files']\n        \n        print(\"=\" * 80)\n        print(f\"TICKER: {ticker}\")\n        print(\"=\" * 80)\n        print()\n        \n        # Leer daily agregado\n        df_daily = pl.read_parquet(ticker_dir / 'daily.parquet')\n        print(f\"Daily OHLCV: {len(df_daily):,} d√≠as\")\n        print(f\"  Rango: {df_daily['date'].min()} ‚Üí {df_daily['date'].max()}\")\n        print()\n        \n        # Leer TODOS los archivos 1m y concatenar\n        print(f\"Archivos 1m encontrados: {len(minute_files)}\")\n        \n        dfs_1m = []\n        for mf in minute_files:\n            try:\n                df = pl.read_parquet(mf)\n                if len(df) > 0:\n                    dfs_1m.append(df)\n            except Exception as e:\n                print(f\"  ‚ö†Ô∏è  Error leyendo {mf.name}: {e}\")\n        \n        if len(dfs_1m) == 0:\n            print(\"  ‚ö†Ô∏è  No se pudieron leer archivos 1m\")\n            print()\n            continue\n        \n        # Concatenar todos los archivos\n        df_1m_all = pl.concat(dfs_1m)\n        print(f\"Datos 1m concatenados: {len(df_1m_all):,} filas\")\n        print()\n        \n        # Agregar manualmente\n        # La columna 'date' ya existe como String, solo hay que convertirla a Date\n        df_manual = (\n            df_1m_all\n            .with_columns([\n                pl.col('date').str.to_date().alias('date')\n            ])\n            .sort(['date', 't'])\n            .group_by('date')\n            .agg([\n                pl.col('ticker').first(),\n                pl.col('o').first().alias('o_manual'),\n                pl.col('h').max().alias('h_manual'),\n                pl.col('l').min().alias('l_manual'),\n                pl.col('c').last().alias('c_manual'),\n                pl.col('v').sum().alias('v_manual'),\n                pl.col('n').sum().alias('n_manual'),\n            ])\n            .sort('date')\n        )\n        \n        print(f\"Agregaci√≥n manual: {len(df_manual):,} d√≠as\")\n        print(f\"  Rango: {df_manual['date'].min()} ‚Üí {df_manual['date'].max()}\")\n        print()\n        \n        # Join y comparar\n        df_compare = df_daily.join(df_manual, on='date', how='inner')\n        \n        if len(df_compare) == 0:\n            print(\"‚ö†Ô∏è  No hay d√≠as comparables entre daily y 1m manual\")\n            print()\n        else:\n            print(f\"‚úÖ D√≠as comparables: {len(df_compare):,}\\n\")\n            \n            # Comparar valores\n            tolerance = 1e-6\n            \n            diff_o = (df_compare['o'] - df_compare['o_manual']).abs()\n            diff_h = (df_compare['h'] - df_compare['h_manual']).abs()\n            diff_l = (df_compare['l'] - df_compare['l_manual']).abs()\n            diff_c = (df_compare['c'] - df_compare['c_manual']).abs()\n            diff_v = (df_compare['v'] - df_compare['v_manual']).abs()\n            diff_n = (df_compare['n'] - df_compare['n_manual']).abs()\n            \n            print(\"DIFERENCIAS (Agregado vs Manual):\")\n            print(f\"  Open   - Max: {diff_o.max():.10f}, Mean: {diff_o.mean():.10f}\")\n            print(f\"  High   - Max: {diff_h.max():.10f}, Mean: {diff_h.mean():.10f}\")\n            print(f\"  Low    - Max: {diff_l.max():.10f}, Mean: {diff_l.mean():.10f}\")\n            print(f\"  Close  - Max: {diff_c.max():.10f}, Mean: {diff_c.mean():.10f}\")\n            print(f\"  Volume - Max: {diff_v.max():.2f}, Mean: {diff_v.mean():.2f}\")\n            print(f\"  Trades - Max: {diff_n.max():.0f}, Mean: {diff_n.mean():.2f}\")\n            print()\n            \n            # Verificaci√≥n\n            all_match = (\n                diff_o.max() < tolerance and\n                diff_h.max() < tolerance and\n                diff_l.max() < tolerance and\n                diff_c.max() < tolerance and\n                diff_v.max() < tolerance and\n                diff_n.max() == 0\n            )\n            \n            if all_match:\n                print(\"‚úÖ AGGREGATION: CORRECT (values match exactly)\")\n            else:\n                print(\"‚ö†Ô∏è  AGGREGATION: Some differences found (check tolerance)\")\n            \n            # Mostrar ejemplo de comparaci√≥n\n            print(\"\\nEJEMPLO DE COMPARACI√ìN (primeras 3 filas):\")\n            print(df_compare.select(['date', 'o', 'o_manual', 'h', 'h_manual', 'c', 'c_manual', 'v', 'v_manual']).head(3))\n            print()\n    \n    print(\"=\" * 80)\n    print(\"‚úÖ VERIFICACI√ìN COMPLETADA PARA 3 TICKERS\")\n    print(\"=\" * 80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Distribuciones Estad√≠sticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"7.1 STATISTICAL DISTRIBUTIONS (sample 100 tickers)\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "sample_stats = random.sample([d for d in ticker_dirs if (d/'daily.parquet').exists()], min(100, len(ticker_dirs)))\n",
    "\n",
    "# Recolectar m√©tricas\n",
    "days_per_ticker = []\n",
    "avg_close = []\n",
    "avg_volume = []\n",
    "avg_dollar_vol = []\n",
    "price_volatility = []\n",
    "\n",
    "for ticker_dir in sample_stats:\n",
    "    df = pl.read_parquet(ticker_dir / 'daily.parquet')\n",
    "    \n",
    "    days_per_ticker.append(len(df))\n",
    "    avg_close.append(df['c'].mean())\n",
    "    avg_volume.append(df['v'].mean())\n",
    "    avg_dollar_vol.append(df['dollar'].mean())\n",
    "    \n",
    "    # Volatilidad = std(returns)\n",
    "    returns = df['c'].pct_change().drop_nulls()\n",
    "    if len(returns) > 0:\n",
    "        price_volatility.append(returns.std())\n",
    "\n",
    "# Convertir a numpy\n",
    "days_per_ticker = np.array(days_per_ticker)\n",
    "avg_close = np.array(avg_close)\n",
    "avg_volume = np.array(avg_volume)\n",
    "avg_dollar_vol = np.array(avg_dollar_vol)\n",
    "price_volatility = np.array(price_volatility)\n",
    "\n",
    "print(f\"Sample size: {len(sample_stats)} tickers\\n\")\n",
    "\n",
    "print(\"D√çAS POR TICKER:\")\n",
    "print(f\"  Min: {days_per_ticker.min():,}\")\n",
    "print(f\"  Max: {days_per_ticker.max():,}\")\n",
    "print(f\"  Mean: {days_per_ticker.mean():,.1f}\")\n",
    "print(f\"  Median: {np.median(days_per_ticker):,.1f}\")\n",
    "print(f\"  Std: {days_per_ticker.std():,.1f}\\n\")\n",
    "\n",
    "print(\"PRECIO PROMEDIO:\")\n",
    "print(f\"  Min: ${avg_close.min():.2f}\")\n",
    "print(f\"  Max: ${avg_close.max():.2f}\")\n",
    "print(f\"  Mean: ${avg_close.mean():.2f}\")\n",
    "print(f\"  Median: ${np.median(avg_close):.2f}\\n\")\n",
    "\n",
    "print(\"VOLUMEN PROMEDIO:\")\n",
    "print(f\"  Min: {avg_volume.min():,.0f}\")\n",
    "print(f\"  Max: {avg_volume.max():,.0f}\")\n",
    "print(f\"  Mean: {avg_volume.mean():,.0f}\")\n",
    "print(f\"  Median: {np.median(avg_volume):,.0f}\\n\")\n",
    "\n",
    "print(\"DOLLAR VOLUME PROMEDIO:\")\n",
    "print(f\"  Min: ${avg_dollar_vol.min():,.0f}\")\n",
    "print(f\"  Max: ${avg_dollar_vol.max():,.0f}\")\n",
    "print(f\"  Mean: ${avg_dollar_vol.mean():,.0f}\")\n",
    "print(f\"  Median: ${np.median(avg_dollar_vol):,.0f}\\n\")\n",
    "\n",
    "print(\"VOLATILIDAD DIARIA (std returns):\")\n",
    "print(f\"  Min: {price_volatility.min():.4f}\")\n",
    "print(f\"  Max: {price_volatility.max():.4f}\")\n",
    "print(f\"  Mean: {price_volatility.mean():.4f}\")\n",
    "print(f\"  Median: {np.median(price_volatility):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaciones\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "# 1. D√≠as por ticker\n",
    "axes[0, 0].hist(days_per_ticker, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].axvline(days_per_ticker.mean(), color='red', linestyle='--', label=f'Mean: {days_per_ticker.mean():.0f}')\n",
    "axes[0, 0].axvline(np.median(days_per_ticker), color='green', linestyle='--', label=f'Median: {np.median(days_per_ticker):.0f}')\n",
    "axes[0, 0].set_xlabel('Trading Days')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Distribution: Days per Ticker')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# 2. Precio promedio\n",
    "axes[0, 1].hist(avg_close, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].axvline(avg_close.mean(), color='red', linestyle='--', label=f'Mean: ${avg_close.mean():.2f}')\n",
    "axes[0, 1].set_xlabel('Average Close Price ($)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Distribution: Average Close Price')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# 3. Volumen promedio (log scale)\n",
    "axes[0, 2].hist(np.log10(avg_volume + 1), bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 2].set_xlabel('log10(Average Volume)')\n",
    "axes[0, 2].set_ylabel('Frequency')\n",
    "axes[0, 2].set_title('Distribution: Average Volume (log scale)')\n",
    "axes[0, 2].grid(alpha=0.3)\n",
    "\n",
    "# 4. Dollar volume (log scale)\n",
    "axes[1, 0].hist(np.log10(avg_dollar_vol + 1), bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].set_xlabel('log10(Average Dollar Volume)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Distribution: Average Dollar Volume (log scale)')\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# 5. Volatilidad\n",
    "axes[1, 1].hist(price_volatility, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1, 1].axvline(price_volatility.mean(), color='red', linestyle='--', label=f'Mean: {price_volatility.mean():.4f}')\n",
    "axes[1, 1].set_xlabel('Daily Volatility (std returns)')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Distribution: Price Volatility')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "# 6. Boxplot comparativo\n",
    "axes[1, 2].boxplot([days_per_ticker / days_per_ticker.max(), \n",
    "                     avg_close / avg_close.max(),\n",
    "                     price_volatility / price_volatility.max()],\n",
    "                    labels=['Days\\n(norm)', 'Price\\n(norm)', 'Vol\\n(norm)'])\n",
    "axes[1, 2].set_ylabel('Normalized Value')\n",
    "axes[1, 2].set_title('Boxplot: Normalized Metrics')\n",
    "axes[1, 2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('D:/04_TRADING_SMALLCAPS/01_DayBook/fase_01/E_Event Detectors E1, E4, E7, E8/notebooks/distribuciones_daily_ohlcv.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Gr√°ficos generados y guardados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Time Series Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"8.1 TIME SERIES ANALYSIS (3 tickers ejemplo)\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "\n",
    "for idx, ticker_dir in enumerate(sample_dirs):\n",
    "    df = pl.read_parquet(ticker_dir / 'daily.parquet')\n",
    "    ticker = ticker_dir.name\n",
    "    \n",
    "    # Convertir a pandas para plotting\n",
    "    df_pd = df.to_pandas()\n",
    "    df_pd['date'] = pd.to_datetime(df_pd['date'])\n",
    "    df_pd = df_pd.set_index('date')\n",
    "    \n",
    "    # Plot precio y volumen\n",
    "    ax1 = axes[idx]\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    ax1.plot(df_pd.index, df_pd['c'], color='blue', linewidth=1, label='Close Price')\n",
    "    ax2.bar(df_pd.index, df_pd['v'], color='gray', alpha=0.3, label='Volume')\n",
    "    \n",
    "    ax1.set_ylabel('Price ($)', color='blue')\n",
    "    ax2.set_ylabel('Volume', color='gray')\n",
    "    ax1.set_title(f'{ticker} - Price and Volume Time Series')\n",
    "    ax1.grid(alpha=0.3)\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('D:/04_TRADING_SMALLCAPS/01_DayBook/fase_01/E_Event Detectors E1, E4, E7, E8/notebooks/time_series_daily_ohlcv.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Time series plots generados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Resumen Ejecutivo y Recomendaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EXECUTIVE SUMMARY: TRACK A DAILY OHLCV VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"üìä DATASET OVERVIEW:\")\n",
    "print(f\"  Total tickers processed: {len(ticker_dirs):,}\")\n",
    "print(f\"  Valid tickers (with data): {with_both:,}\")\n",
    "print(f\"  Coverage: {with_both/8620*100:.2f}%\")\n",
    "print(f\"  Sample validation: {len(sample_stats)} tickers\")\n",
    "print()\n",
    "\n",
    "print(\"‚úÖ QUALITY METRICS:\")\n",
    "print(f\"  Schema compliance: {schema_matches}/{len(sample_tickers)} ({schema_matches/len(sample_tickers)*100:.1f}%)\")\n",
    "print(f\"  NULL values: {total_nulls:,} ({total_nulls/(total_rows*9)*100:.4f}% of all cells)\")\n",
    "print(f\"  OHLC violations: {total_violations:,} ({total_violations/total_rows_checked*100:.4f}%)\")\n",
    "print(f\"  Duplicate dates: {duplicate_dates}/{len(sample_quality)} tickers\")\n",
    "print(f\"  Unordered dates: {unordered_dates}/{len(sample_quality)} tickers\")\n",
    "print()\n",
    "\n",
    "print(\"üìà STATISTICAL SUMMARY:\")\n",
    "print(f\"  Avg days per ticker: {days_per_ticker.mean():,.1f} ¬± {days_per_ticker.std():,.1f}\")\n",
    "print(f\"  Avg price: ${avg_close.mean():.2f} (median: ${np.median(avg_close):.2f})\")\n",
    "print(f\"  Avg volume: {avg_volume.mean():,.0f} (median: {np.median(avg_volume):,.0f})\")\n",
    "print(f\"  Avg volatility: {price_volatility.mean():.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"üéØ RECOMMENDATIONS:\")\n",
    "if total_violations == 0 and total_nulls == 0:\n",
    "    print(\"  ‚úÖ Dataset is PRODUCTION-READY\")\n",
    "    print(\"  ‚úÖ Proceed with Event Detectors E1, E4, E7, E8\")\n",
    "    print(\"  ‚úÖ No data quality issues found\")\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è  Minor issues found (see details above)\")\n",
    "    print(\"  ‚ö†Ô∏è  Review and fix before production use\")\n",
    "print()\n",
    "\n",
    "print(\"üìã NEXT STEPS:\")\n",
    "print(\"  1. Execute event detectors E1, E4, E7, E8\")\n",
    "print(\"  2. Create multi-event fuser\")\n",
    "print(\"  3. Generate event watchlists\")\n",
    "print(\"  4. Download ticks for detected events (¬±N days)\")\n",
    "print(\"  5. Build DIB/VIB bars for event-specific analysis\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"‚úÖ VALIDATION COMPLETED\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}