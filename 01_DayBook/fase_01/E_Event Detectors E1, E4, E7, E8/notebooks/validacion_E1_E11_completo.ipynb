{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validacion E1-E11: Event Detection Complete\n",
    "\n",
    "**Fecha**: 2025-10-29  \n",
    "**Objetivo**: Validar deteccion completa de eventos E1-E11 con 3,459,349 eventos totales\n",
    "\n",
    "---\n",
    "\n",
    "## Scope\n",
    "\n",
    "Validar:\n",
    "1. Conteo de eventos por tipo (E1-E11)\n",
    "2. Distribucion temporal de eventos\n",
    "3. Top tickers por evento\n",
    "4. Validacion flag `intraday_confirmed=False` en E3/E9\n",
    "5. Schema consistency across all event types\n",
    "6. Overlap analysis: cuantos dias tienen multiples eventos\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "print('Polars version:', pl.__version__)\n",
    "print('Matplotlib version:', plt.matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load All Events E1-E11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_dir = Path('processed/events')\n",
    "\n",
    "# Expected counts from detection\n",
    "expected_counts = {\n",
    "    'E1': 164_941,\n",
    "    'E2': 73_170,\n",
    "    'E3': 144_062,\n",
    "    'E4': 197_716,\n",
    "    'E5': 412_902,\n",
    "    'E6': 1_543_990,\n",
    "    'E7': 16_919,\n",
    "    'E8': 19_924,\n",
    "    'E9': 24_074,\n",
    "    'E10': 814_068,\n",
    "    'E11': 47_583\n",
    "}\n",
    "\n",
    "# Load all events\n",
    "dfs = {}\n",
    "for event_type in expected_counts.keys():\n",
    "    event_file = events_dir / f'events_{event_type.lower()}.parquet'\n",
    "    if event_file.exists():\n",
    "        df = pl.read_parquet(event_file)\n",
    "        dfs[event_type] = df\n",
    "        print(f'{event_type}: {len(df):,} events (expected: {expected_counts[event_type]:,})')\n",
    "    else:\n",
    "        print(f'{event_type}: FILE NOT FOUND')\n",
    "\n",
    "print()\n",
    "print(f'Total events loaded: {sum(len(df) for df in dfs.values()):,}')\n",
    "print(f'Expected total: {sum(expected_counts.values()):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Validate Schema: E3 and E9 Have `intraday_confirmed` Flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== SCHEMA VALIDATION ===')\n",
    "print()\n",
    "\n",
    "for event_type, df in dfs.items():\n",
    "    print(f'{event_type} Schema:')\n",
    "    for col, dtype in zip(df.columns, df.dtypes):\n",
    "        print(f'  {col}: {dtype}')\n",
    "    \n",
    "    # Check for intraday_confirmed flag in E3/E9\n",
    "    if event_type in ['E3', 'E9']:\n",
    "        if 'intraday_confirmed' in df.columns:\n",
    "            flag_value = df['intraday_confirmed'][0]\n",
    "            print(f'  ✅ intraday_confirmed flag present: {flag_value}')\n",
    "        else:\n",
    "            print(f'  ❌ intraday_confirmed flag MISSING')\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Event Count Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data = []\n",
    "\n",
    "event_descriptions = {\n",
    "    'E1': 'Volume Explosion (RVOL > 5x)',\n",
    "    'E2': 'Gap Up (+10%)',\n",
    "    'E3': 'Price Spike Intraday (+20%) [RADAR]',\n",
    "    'E4': 'Parabolic Move (+50% in ≤5d)',\n",
    "    'E5': 'Breakout ATH (52w high)',\n",
    "    'E6': 'Multiple Green Days (3+ consec)',\n",
    "    'E7': 'First Red Day (FRD)',\n",
    "    'E8': 'Gap Down Violent (-15%)',\n",
    "    'E9': 'Crash Intraday (-30%) [RADAR]',\n",
    "    'E10': 'First Green Bounce (after 3+ red)',\n",
    "    'E11': 'Volume Bounce (RVOL>3x + bounce)'\n",
    "}\n",
    "\n",
    "for event_type, df in dfs.items():\n",
    "    summary_data.append({\n",
    "        'Event': event_type,\n",
    "        'Description': event_descriptions[event_type],\n",
    "        'Count': len(df),\n",
    "        'Unique Tickers': df['ticker'].n_unique(),\n",
    "        'Date Range': f\"{df['date'].min()} to {df['date'].max()}\",\n",
    "        'Avg Events/Ticker': len(df) / df['ticker'].n_unique()\n",
    "    })\n",
    "\n",
    "df_summary = pl.DataFrame(summary_data)\n",
    "print(df_summary)\n",
    "\n",
    "# Save summary\n",
    "df_summary.write_csv('event_summary_E1_E11.csv')\n",
    "print()\n",
    "print('✅ Summary saved to event_summary_E1_E11.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Event Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart: Event counts\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "event_counts = {k: len(v) for k, v in dfs.items()}\n",
    "events = list(event_counts.keys())\n",
    "counts = list(event_counts.values())\n",
    "\n",
    "colors = ['red' if e in ['E3', 'E9'] else 'steelblue' for e in events]\n",
    "\n",
    "ax.bar(events, counts, color=colors, alpha=0.7)\n",
    "ax.set_xlabel('Event Type', fontsize=12)\n",
    "ax.set_ylabel('Event Count', fontsize=12)\n",
    "ax.set_title('Event Distribution E1-E11 (Total: 3,459,349)', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add count labels on bars\n",
    "for i, (event, count) in enumerate(zip(events, counts)):\n",
    "    ax.text(i, count + 20000, f'{count:,}', ha='center', fontsize=9)\n",
    "\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='steelblue', label='Daily OHLCV (confirmed)'),\n",
    "    Patch(facecolor='red', label='Daily OHLCV (RADAR - intraday_confirmed=False)')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('event_distribution_E1_E11.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('✅ Plot saved to event_distribution_E1_E11.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Temporal Distribution: Events Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all events with event_type preserved\n",
    "all_events = []\n",
    "for event_type, df in dfs.items():\n",
    "    df_temp = df.select(['ticker', 'date', 'event_type'])\n",
    "    all_events.append(df_temp)\n",
    "\n",
    "df_all = pl.concat(all_events)\n",
    "print(f'Total events concatenated: {len(df_all):,}')\n",
    "print()\n",
    "\n",
    "# Group by year and event type\n",
    "df_temporal = (\n",
    "    df_all\n",
    "    .with_columns([\n",
    "        pl.col('date').dt.year().alias('year')\n",
    "    ])\n",
    "    .group_by(['year', 'event_type'])\n",
    "    .agg(pl.count().alias('count'))\n",
    "    .sort(['year', 'event_type'])\n",
    ")\n",
    "\n",
    "print('Events by year and type (sample):')\n",
    "print(df_temporal.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Events over time (stacked area chart)\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "# Pivot for plotting\n",
    "df_pivot = df_temporal.pivot(index='year', columns='event_type', values='count').fill_null(0)\n",
    "df_pivot_pd = df_pivot.to_pandas().set_index('year')\n",
    "\n",
    "# Sort columns by event type\n",
    "event_order = ['E1_VolExplosion', 'E2_GapUp', 'E3_PriceSpikeIntraday', 'E4_ParabolicMove', \n",
    "               'E5_BreakoutATH', 'E6_MultipleGreenDays', 'E7_FirstRedDay', 'E8_GapDownViolent',\n",
    "               'E9_CrashIntraday', 'E10_FirstGreenBounce', 'E11_VolumeBounce']\n",
    "\n",
    "df_pivot_pd = df_pivot_pd[event_order]\n",
    "\n",
    "df_pivot_pd.plot(kind='area', stacked=True, ax=ax, alpha=0.7, colormap='tab20')\n",
    "ax.set_xlabel('Year', fontsize=12)\n",
    "ax.set_ylabel('Event Count', fontsize=12)\n",
    "ax.set_title('Event Distribution Over Time (2004-2025)', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper left', fontsize=8)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('event_temporal_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('✅ Plot saved to event_temporal_distribution.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Top 20 Tickers by Total Event Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count events per ticker across all event types\n",
    "df_ticker_counts = (\n",
    "    df_all\n",
    "    .group_by('ticker')\n",
    "    .agg(pl.count().alias('total_events'))\n",
    "    .sort('total_events', descending=True)\n",
    "    .head(20)\n",
    ")\n",
    "\n",
    "print('Top 20 tickers by total event count:')\n",
    "print(df_ticker_counts)\n",
    "print()\n",
    "\n",
    "# Save\n",
    "df_ticker_counts.write_csv('top20_tickers_events.csv')\n",
    "print('✅ Top 20 tickers saved to top20_tickers_events.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart: Top 20 tickers\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "df_top20_pd = df_ticker_counts.to_pandas()\n",
    "ax.barh(df_top20_pd['ticker'], df_top20_pd['total_events'], color='teal', alpha=0.7)\n",
    "ax.set_xlabel('Total Events', fontsize=12)\n",
    "ax.set_ylabel('Ticker', fontsize=12)\n",
    "ax.set_title('Top 20 Tickers by Total Event Count (E1-E11)', fontsize=14, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('top20_tickers_events.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('✅ Plot saved to top20_tickers_events.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Multi-Event Days Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique events per ticker-date\n",
    "df_multi_event = (\n",
    "    df_all\n",
    "    .group_by(['ticker', 'date'])\n",
    "    .agg([\n",
    "        pl.count().alias('event_count'),\n",
    "        pl.col('event_type').unique().alias('event_types')\n",
    "    ])\n",
    "    .sort('event_count', descending=True)\n",
    ")\n",
    "\n",
    "print('Multi-event days statistics:')\n",
    "print()\n",
    "\n",
    "# Distribution of event count per day\n",
    "event_count_dist = (\n",
    "    df_multi_event\n",
    "    .group_by('event_count')\n",
    "    .agg(pl.count().alias('days'))\n",
    "    .sort('event_count')\n",
    ")\n",
    "\n",
    "print(event_count_dist)\n",
    "print()\n",
    "\n",
    "# Top 10 days with most events\n",
    "print('Top 10 days with most events:')\n",
    "print(df_multi_event.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Distribution of events per day\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "df_dist_pd = event_count_dist.to_pandas()\n",
    "ax.bar(df_dist_pd['event_count'], df_dist_pd['days'], color='coral', alpha=0.7)\n",
    "ax.set_xlabel('Number of Events per Day', fontsize=12)\n",
    "ax.set_ylabel('Number of Days', fontsize=12)\n",
    "ax.set_title('Distribution: How Many Events Occur on Same Day?', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('multi_event_days_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('✅ Plot saved to multi_event_days_distribution.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sample Data: E3 and E9 with `intraday_confirmed=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample E3 events\n",
    "print('=== E3 (Price Spike Intraday) Sample ===')\n",
    "print()\n",
    "print(dfs['E3'].head(10))\n",
    "print()\n",
    "\n",
    "# Verify flag\n",
    "if 'intraday_confirmed' in dfs['E3'].columns:\n",
    "    flag_values = dfs['E3']['intraday_confirmed'].unique()\n",
    "    print(f'E3 intraday_confirmed unique values: {flag_values}')\n",
    "    print(f'All E3 events have intraday_confirmed=False: {flag_values == [False]}')\n",
    "else:\n",
    "    print('❌ E3 intraday_confirmed column MISSING')\n",
    "\n",
    "print()\n",
    "print('=== E9 (Crash Intraday) Sample ===')\n",
    "print()\n",
    "print(dfs['E9'].head(10))\n",
    "print()\n",
    "\n",
    "# Verify flag\n",
    "if 'intraday_confirmed' in dfs['E9'].columns:\n",
    "    flag_values = dfs['E9']['intraday_confirmed'].unique()\n",
    "    print(f'E9 intraday_confirmed unique values: {flag_values}')\n",
    "    print(f'All E9 events have intraday_confirmed=False: {flag_values == [False]}')\n",
    "else:\n",
    "    print('❌ E9 intraday_confirmed column MISSING')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Event Co-occurrence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary matrix: ticker-date vs event_type\n",
    "df_pivot_events = (\n",
    "    df_all\n",
    "    .with_columns([\n",
    "        pl.lit(1).alias('present')\n",
    "    ])\n",
    "    .pivot(index=['ticker', 'date'], columns='event_type', values='present')\n",
    "    .fill_null(0)\n",
    ")\n",
    "\n",
    "print('Binary event matrix (sample):')\n",
    "print(df_pivot_events.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate co-occurrence matrix\n",
    "event_cols = [col for col in df_pivot_events.columns if col not in ['ticker', 'date']]\n",
    "\n",
    "# Convert to pandas for correlation matrix\n",
    "df_events_pd = df_pivot_events.select(event_cols).to_pandas()\n",
    "corr_matrix = df_events_pd.corr()\n",
    "\n",
    "print('Event co-occurrence correlation matrix:')\n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap: Event co-occurrence\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8}, ax=ax)\n",
    "\n",
    "ax.set_title('Event Co-occurrence Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('event_cooccurrence_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('✅ Heatmap saved to event_cooccurrence_matrix.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 80)\n",
    "print('VALIDATION SUMMARY: EVENT DETECTION E1-E11')\n",
    "print('=' * 80)\n",
    "print()\n",
    "\n",
    "print(f'Total events detected: {sum(len(df) for df in dfs.values()):,}')\n",
    "print(f'Expected total: {sum(expected_counts.values()):,}')\n",
    "print()\n",
    "\n",
    "print('Event breakdown:')\n",
    "for event_type, df in dfs.items():\n",
    "    expected = expected_counts[event_type]\n",
    "    actual = len(df)\n",
    "    match = '✅' if actual == expected else '❌'\n",
    "    print(f'  {match} {event_type}: {actual:,} (expected: {expected:,})')\n",
    "\n",
    "print()\n",
    "print('Schema validation:')\n",
    "for event_type in ['E3', 'E9']:\n",
    "    if 'intraday_confirmed' in dfs[event_type].columns:\n",
    "        flag_value = dfs[event_type]['intraday_confirmed'][0]\n",
    "        print(f'  ✅ {event_type}: intraday_confirmed={flag_value}')\n",
    "    else:\n",
    "        print(f'  ❌ {event_type}: intraday_confirmed MISSING')\n",
    "\n",
    "print()\n",
    "print('Date range:')\n",
    "min_date = min(df['date'].min() for df in dfs.values())\n",
    "max_date = max(df['date'].max() for df in dfs.values())\n",
    "print(f'  {min_date} to {max_date}')\n",
    "\n",
    "print()\n",
    "print('Unique tickers with events:')\n",
    "all_tickers = set()\n",
    "for df in dfs.values():\n",
    "    all_tickers.update(df['ticker'].unique().to_list())\n",
    "print(f'  {len(all_tickers):,} tickers')\n",
    "\n",
    "print()\n",
    "print('Multi-event days:')\n",
    "multi_event_days = df_multi_event.filter(pl.col('event_count') > 1)\n",
    "print(f'  {len(multi_event_days):,} days have 2+ events')\n",
    "max_events = df_multi_event['event_count'].max()\n",
    "print(f'  Max events on single day: {max_events}')\n",
    "\n",
    "print()\n",
    "print('=' * 80)\n",
    "print('✅ VALIDATION COMPLETED')\n",
    "print('=' * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
