{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis Universo Completo E1-E11 (Sin Recortes)\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "Calcular el **peso total real** de descarga de ticks para TODO el universo E1-E11 detectado,\n",
    "considerando la **consolidación de días únicos** (union de eventos por ticker-date).\n",
    "\n",
    "## Contexto\n",
    "\n",
    "- **Watchlist Total**: 2,939,824 ticker-date combinaciones\n",
    "- **Eventos Detectados**: 3,342,911 eventos (E1-E11)\n",
    "- **Problema**: Un ticker puede tener múltiples eventos el mismo día → no descargamos 5x datos para 5 eventos\n",
    "- **Solución**: Consolidar por ticker-date únicos + aplicar ventanas optimizadas + calcular peso real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import timedelta\n",
    "\n",
    "# Configuración visual\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (16, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cargar Watchlist Completa E1-E11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar watchlist\n",
    "project_root = Path.cwd().parent.parent.parent.parent\n",
    "watchlist_file = project_root / 'processed' / 'watchlist_E1_E11.parquet'\n",
    "\n",
    "df_watchlist = pl.read_parquet(watchlist_file)\n",
    "\n",
    "print('=' * 80)\n",
    "print('WATCHLIST COMPLETA E1-E11 (SIN RECORTES)')\n",
    "print('=' * 80)\n",
    "print(f'Total ticker-date combinations: {len(df_watchlist):,}')\n",
    "print(f'Tickers únicos: {df_watchlist[\"ticker\"].n_unique():,}')\n",
    "print(f'Rango fechas: {df_watchlist[\"date\"].min()} → {df_watchlist[\"date\"].max()}')\n",
    "print(f'Columnas: {df_watchlist.columns}')\n",
    "print()\n",
    "df_watchlist.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Distribución de Eventos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explotar eventos para contar por tipo\n",
    "df_exploded = df_watchlist.explode('events')\n",
    "\n",
    "total_event_occurrences = len(df_exploded)\n",
    "print(f'Total event occurrences: {total_event_occurrences:,}')\n",
    "print()\n",
    "\n",
    "# Contar por evento\n",
    "df_event_counts = df_exploded.group_by('events').agg([\n",
    "    pl.len().alias('n_occurrences')\n",
    "]).sort('n_occurrences', descending=True)\n",
    "\n",
    "print('Distribución de eventos:')\n",
    "print(df_event_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ventanas Optimizadas por Evento\n",
    "\n",
    "Cada evento tiene una ventana temporal óptima basada en su naturaleza:\n",
    "\n",
    "| Evento | Window | Justificación |\n",
    "|--------|--------|---------------|\n",
    "| **E1** Volume Explosion | ±2d | Anticipación de volumen + fade posterior |\n",
    "| **E2** Gap Up | ±2d | Pre-gap setup + continuation |\n",
    "| **E3** Price Spike Intraday | ±1d | Solo evento (análisis intraday) |\n",
    "| **E4** Parabolic Move | ±3d | Run-up multi-día + climax + collapse |\n",
    "| **E5** Breakout ATH | ±2d | Breakout + confirmación |\n",
    "| **E6** Multiple Green Days | ±1d | Ya es multi-día (solo evento final) |\n",
    "| **E7** First Red Day | ±2d | Rally previo + caída inmediata |\n",
    "| **E8** Gap Down Violent | ±3d | Post-gap continuation o rebound |\n",
    "| **E9** Crash Intraday | ±1d | Solo evento (análisis intraday) |\n",
    "| **E10** First Green Bounce | ±3d | Bounce + confirmación volumen |\n",
    "| **E11** Volume Bounce | ±3d | Context + bounce + follow-through |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario de ventanas optimizadas\n",
    "EVENT_WINDOWS = {\n",
    "    'E1_VolExplosion': 2,\n",
    "    'E2_GapUp': 2,\n",
    "    'E3_PriceSpikeIntraday': 1,\n",
    "    'E4_Parabolic': 3,\n",
    "    'E5_BreakoutATH': 2,\n",
    "    'E6_MultipleGreenDays': 1,\n",
    "    'E7_FirstRedDay': 2,\n",
    "    'E8_GapDownViolent': 3,\n",
    "    'E9_CrashIntraday': 1,\n",
    "    'E10_FirstGreenBounce': 3,\n",
    "    'E11_VolumeBounce': 3,\n",
    "}\n",
    "\n",
    "# Crear DataFrame de configuración\n",
    "df_config = pl.DataFrame({\n",
    "    'event_type': list(EVENT_WINDOWS.keys()),\n",
    "    'window_days': list(EVENT_WINDOWS.values()),\n",
    "}).with_columns([\n",
    "    (pl.col('window_days') * 2 + 1).alias('total_days_per_occurrence')\n",
    "])\n",
    "\n",
    "print('Configuración de Ventanas Optimizadas:')\n",
    "print(df_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CÁLCULO CRUCIAL: Días Únicos Consolidados (Union de Eventos)\n",
    "\n",
    "**Este es el cálculo más importante del notebook.**\n",
    "\n",
    "### Problema\n",
    "Un ticker puede tener múltiples eventos el mismo día. Por ejemplo:\n",
    "- `DCTH 2004-03-11`: tiene E1, E3, E5 → 3 eventos\n",
    "\n",
    "### Solución\n",
    "No descargamos los datos 3 veces. Los ticks del día 2004-03-11 sirven para los 3 eventos.\n",
    "\n",
    "### Algoritmo\n",
    "1. Para cada ticker-date con eventos, expandir ventana temporal según el evento con ventana más amplia\n",
    "2. Generar todas las fechas necesarias (date ± window)\n",
    "3. Consolidar fechas únicas por ticker (eliminar duplicados)\n",
    "4. Contar total de ticker-days únicos necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 80)\n",
    "print('CÁLCULO DE DÍAS ÚNICOS CONSOLIDADOS (UNION DE EVENTOS)')\n",
    "print('=' * 80)\n",
    "print()\n",
    "\n",
    "# Para cada ticker-date, determinar la ventana máxima necesaria\n",
    "# (basada en el evento con la ventana más amplia de ese día)\n",
    "\n",
    "# Explotar eventos y joinear con ventanas\n",
    "df_events_with_windows = df_watchlist.explode('events').join(\n",
    "    df_config.select(['event_type', 'window_days']),\n",
    "    left_on='events',\n",
    "    right_on='event_type',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Para cada ticker-date, tomar la ventana MÁXIMA de todos sus eventos\n",
    "df_max_windows = df_events_with_windows.group_by(['ticker', 'date']).agg([\n",
    "    pl.col('window_days').max().alias('max_window'),\n",
    "    pl.col('events').count().alias('n_events_this_day')\n",
    "])\n",
    "\n",
    "print(f'Ticker-dates con eventos: {len(df_max_windows):,}')\n",
    "print()\n",
    "print('Sample de ventanas máximas por ticker-date:')\n",
    "print(df_max_windows.head(10))\n",
    "print()\n",
    "\n",
    "# Distribución de ventanas máximas\n",
    "window_distribution = df_max_windows.group_by('max_window').agg([\n",
    "    pl.len().alias('n_ticker_dates')\n",
    "]).sort('max_window')\n",
    "\n",
    "print('Distribución de ventanas máximas:')\n",
    "print(window_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar todas las fechas necesarias para cada ticker-date\n",
    "# (date - max_window ... date ... date + max_window)\n",
    "\n",
    "def expand_date_range(ticker: str, date: pl.Date, window: int) -> list:\n",
    "    \"\"\"Genera lista de fechas [date - window, ..., date + window]\"\"\"\n",
    "    dates = []\n",
    "    for offset in range(-window, window + 1):\n",
    "        dates.append({\n",
    "            'ticker': ticker,\n",
    "            'download_date': date + timedelta(days=offset),\n",
    "            'event_date': date,\n",
    "            'window_used': window\n",
    "        })\n",
    "    return dates\n",
    "\n",
    "print('Expandiendo fechas temporales...')\n",
    "print('(esto puede tomar 1-2 minutos para 2.9M ticker-dates)')\n",
    "print()\n",
    "\n",
    "# Convertir a lista de diccionarios para procesamiento\n",
    "all_expanded_dates = []\n",
    "for row in df_max_windows.iter_rows(named=True):\n",
    "    ticker = row['ticker']\n",
    "    date = row['date']\n",
    "    window = row['max_window']\n",
    "    all_expanded_dates.extend(expand_date_range(ticker, date, window))\n",
    "\n",
    "# Convertir a DataFrame\n",
    "df_expanded = pl.DataFrame(all_expanded_dates)\n",
    "\n",
    "print(f'Fechas expandidas (con duplicados): {len(df_expanded):,}')\n",
    "print()\n",
    "print('Sample de fechas expandidas:')\n",
    "print(df_expanded.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSOLIDACIÓN: Obtener ticker-dates únicos\n",
    "# (eliminar duplicados cuando múltiples eventos necesitan la misma fecha)\n",
    "\n",
    "df_unique_downloads = df_expanded.select(['ticker', 'download_date']).unique()\n",
    "\n",
    "total_unique_ticker_days = len(df_unique_downloads)\n",
    "\n",
    "print('=' * 80)\n",
    "print('RESULTADO CONSOLIDADO')\n",
    "print('=' * 80)\n",
    "print()\n",
    "print(f'Total ticker-dates con eventos: {len(df_max_windows):,}')\n",
    "print(f'Total fechas expandidas (con duplicados): {len(df_expanded):,}')\n",
    "print(f'Total ticker-days ÚNICOS a descargar: {total_unique_ticker_days:,}')\n",
    "print()\n",
    "print(f'Reducción por consolidación: {len(df_expanded) - total_unique_ticker_days:,} ticker-days')\n",
    "print(f'Eficiencia: {(1 - total_unique_ticker_days / len(df_expanded)) * 100:.1f}% menos descargas')\n",
    "print()\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cálculo de Peso Total (usando métricas REALES de descarga)\n",
    "\n",
    "### Datos de la descarga Pilot Ultra-Light\n",
    "\n",
    "**Descarga completada**:\n",
    "- Ticker-days descargados: 65,907\n",
    "- Espacio utilizado: 12.05 GB (con compresión ZSTD)\n",
    "- Promedio real: **0.183 MB por ticker-day** (12,050 MB / 65,907)\n",
    "\n",
    "Este promedio es **MUCHO más preciso** que la estimación de 50 MB/ticker-day usada anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métricas REALES de la descarga Pilot Ultra-Light\n",
    "REAL_TICKER_DAYS_DOWNLOADED = 65_907\n",
    "REAL_SIZE_GB = 12.05\n",
    "REAL_MB_PER_TICKER_DAY = (REAL_SIZE_GB * 1024) / REAL_TICKER_DAYS_DOWNLOADED\n",
    "\n",
    "print('=' * 80)\n",
    "print('MÉTRICAS REALES DE DESCARGA (PILOT ULTRA-LIGHT)')\n",
    "print('=' * 80)\n",
    "print()\n",
    "print(f'Ticker-days descargados: {REAL_TICKER_DAYS_DOWNLOADED:,}')\n",
    "print(f'Espacio total (ZSTD): {REAL_SIZE_GB:.2f} GB')\n",
    "print(f'Promedio REAL: {REAL_MB_PER_TICKER_DAY:.3f} MB/ticker-day')\n",
    "print()\n",
    "print('Comparación vs estimación anterior:')\n",
    "print(f'  Estimado anterior: 50.0 MB/ticker-day')\n",
    "print(f'  Real medido: {REAL_MB_PER_TICKER_DAY:.3f} MB/ticker-day')\n",
    "print(f'  Diferencia: {((REAL_MB_PER_TICKER_DAY / 50.0) - 1) * 100:+.1f}%')\n",
    "print()\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CÁLCULO FINAL: Peso total del universo completo E1-E11\n",
    "\n",
    "# Usando métrica REAL\n",
    "total_gb_real = (total_unique_ticker_days * REAL_MB_PER_TICKER_DAY) / 1024\n",
    "total_tb_real = total_gb_real / 1024\n",
    "\n",
    "# Usando estimación anterior (para comparación)\n",
    "OLD_MB_PER_TICKER_DAY = 50.0\n",
    "total_gb_old = (total_unique_ticker_days * OLD_MB_PER_TICKER_DAY) / 1024\n",
    "total_tb_old = total_gb_old / 1024\n",
    "\n",
    "print('=' * 80)\n",
    "print('PESO TOTAL: UNIVERSO COMPLETO E1-E11 (SIN RECORTES)')\n",
    "print('=' * 80)\n",
    "print()\n",
    "print(f'Total ticker-days únicos a descargar: {total_unique_ticker_days:,}')\n",
    "print()\n",
    "print('CON MÉTRICA REAL (0.183 MB/ticker-day):')\n",
    "print(f'  Espacio total: {total_gb_real:,.2f} GB = {total_tb_real:.2f} TB')\n",
    "print()\n",
    "print('CON ESTIMACIÓN ANTERIOR (50 MB/ticker-day):')\n",
    "print(f'  Espacio total: {total_gb_old:,.2f} GB = {total_tb_old:.2f} TB')\n",
    "print()\n",
    "print(f'DIFERENCIA: {total_gb_old - total_gb_real:,.2f} GB ({((total_gb_real / total_gb_old) - 1) * 100:+.1f}%)')\n",
    "print()\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Estimación de Tiempo de Descarga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros de descarga\n",
    "WORKERS = 6\n",
    "RATE_LIMIT_SECONDS = 0.12  # 0.12s entre requests por worker\n",
    "REQUESTS_PER_SECOND = WORKERS / RATE_LIMIT_SECONDS\n",
    "REQUESTS_PER_HOUR = REQUESTS_PER_SECOND * 3600\n",
    "\n",
    "# Tiempo estimado\n",
    "total_hours = total_unique_ticker_days / REQUESTS_PER_HOUR\n",
    "total_days = total_hours / 24\n",
    "\n",
    "print('=' * 80)\n",
    "print('ESTIMACIÓN DE TIEMPO DE DESCARGA')\n",
    "print('=' * 80)\n",
    "print()\n",
    "print('Configuración:')\n",
    "print(f'  Workers: {WORKERS}')\n",
    "print(f'  Rate limit: {RATE_LIMIT_SECONDS} segundos/request')\n",
    "print(f'  Throughput: {REQUESTS_PER_SECOND:.2f} requests/segundo')\n",
    "print(f'  Throughput: {REQUESTS_PER_HOUR:,.0f} requests/hora')\n",
    "print()\n",
    "print('Tiempo estimado:')\n",
    "print(f'  Total ticker-days: {total_unique_ticker_days:,}')\n",
    "print(f'  Tiempo: {total_hours:,.1f} horas = {total_days:.1f} días')\n",
    "print()\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Resumen Completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear tabla resumen completa\n",
    "df_summary = pl.DataFrame({\n",
    "    'Métrica': [\n",
    "        '1. DATOS FUENTE',\n",
    "        'Ticker-dates en watchlist',\n",
    "        'Tickers únicos',\n",
    "        'Event occurrences totales',\n",
    "        '',\n",
    "        '2. CONSOLIDACIÓN',\n",
    "        'Fechas expandidas (con duplicados)',\n",
    "        'Ticker-days ÚNICOS (consolidados)',\n",
    "        'Reducción por consolidación',\n",
    "        '',\n",
    "        '3. PESO TOTAL (MÉTRICA REAL)',\n",
    "        'MB por ticker-day (ZSTD)',\n",
    "        'Espacio total (GB)',\n",
    "        'Espacio total (TB)',\n",
    "        '',\n",
    "        '4. TIEMPO DESCARGA',\n",
    "        'Workers paralelos',\n",
    "        'Throughput (requests/hora)',\n",
    "        'Tiempo estimado (horas)',\n",
    "        'Tiempo estimado (días)',\n",
    "        '',\n",
    "        '5. COMPARACIÓN',\n",
    "        'Estimación anterior (GB)',\n",
    "        'Estimación REAL (GB)',\n",
    "        'Ahorro vs estimación anterior',\n",
    "    ],\n",
    "    'Valor': [\n",
    "        '',\n",
    "        f'{len(df_watchlist):,}',\n",
    "        f'{df_watchlist[\"ticker\"].n_unique():,}',\n",
    "        f'{total_event_occurrences:,}',\n",
    "        '',\n",
    "        '',\n",
    "        f'{len(df_expanded):,}',\n",
    "        f'{total_unique_ticker_days:,}',\n",
    "        f'{len(df_expanded) - total_unique_ticker_days:,} ({(1 - total_unique_ticker_days / len(df_expanded)) * 100:.1f}%)',\n",
    "        '',\n",
    "        '',\n",
    "        f'{REAL_MB_PER_TICKER_DAY:.3f}',\n",
    "        f'{total_gb_real:,.2f}',\n",
    "        f'{total_tb_real:.2f}',\n",
    "        '',\n",
    "        '',\n",
    "        f'{WORKERS}',\n",
    "        f'{REQUESTS_PER_HOUR:,.0f}',\n",
    "        f'{total_hours:,.1f}',\n",
    "        f'{total_days:.1f}',\n",
    "        '',\n",
    "        '',\n",
    "        f'{total_gb_old:,.2f}',\n",
    "        f'{total_gb_real:,.2f}',\n",
    "        f'{total_gb_old - total_gb_real:,.2f} GB ({((total_gb_real / total_gb_old) - 1) * 100:+.1f}%)',\n",
    "    ]\n",
    "})\n",
    "\n",
    "print('=' * 80)\n",
    "print('RESUMEN COMPLETO: UNIVERSO E1-E11 SIN RECORTES')\n",
    "print('=' * 80)\n",
    "print()\n",
    "print(df_summary)\n",
    "print()\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico 1: Distribución de eventos\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "events = df_event_counts['events'].to_list()\n",
    "counts = df_event_counts['n_occurrences'].to_list()\n",
    "\n",
    "bars = ax.barh(events, counts, color='#3498db')\n",
    "\n",
    "ax.set_xlabel('Número de Occurrencias', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Tipo de Evento', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Distribución de Eventos E1-E11 (Universo Completo)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Agregar valores\n",
    "for bar, val in zip(bars, counts):\n",
    "    ax.text(val, bar.get_y() + bar.get_height()/2, \n",
    "            f' {val:,}', \n",
    "            va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('distribucion_eventos_universo_completo.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Gráfico guardado: distribucion_eventos_universo_completo.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico 2: Consolidación de días\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "categories = ['Ticker-dates\\ncon eventos', 'Fechas expandidas\\n(con duplicados)', 'Ticker-days ÚNICOS\\n(consolidados)']\n",
    "values = [len(df_max_windows), len(df_expanded), total_unique_ticker_days]\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "\n",
    "bars = ax.bar(categories, values, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "\n",
    "ax.set_ylabel('Número de Ticker-Days', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Consolidación de Días Únicos (Union de Eventos)', fontsize=14, fontweight='bold')\n",
    "ax.ticklabel_format(style='plain', axis='y')\n",
    "\n",
    "# Agregar valores\n",
    "for bar, val in zip(bars, values):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, height,\n",
    "            f'{val:,}',\n",
    "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Agregar líneas de reducción\n",
    "reduction1 = len(df_expanded) - total_unique_ticker_days\n",
    "pct1 = (reduction1 / len(df_expanded)) * 100\n",
    "ax.text(1.5, max(values) * 0.5, \n",
    "        f'Reducción:\\n{reduction1:,}\\n({pct1:.1f}%)',\n",
    "        fontsize=11, fontweight='bold', ha='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.3))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('consolidacion_dias_unicos.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Gráfico guardado: consolidacion_dias_unicos.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico 3: Comparación de peso\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Subplot 1: GB\n",
    "approaches = ['Estimación\\nAnterior\\n(50 MB/day)', 'Métrica REAL\\n(0.183 MB/day)']\n",
    "gb_values = [total_gb_old, total_gb_real]\n",
    "colors1 = ['#e74c3c', '#2ecc71']\n",
    "\n",
    "bars1 = ax1.bar(approaches, gb_values, color=colors1, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "ax1.set_ylabel('Gigabytes (GB)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Peso Total en Gigabytes', fontsize=13, fontweight='bold')\n",
    "ax1.ticklabel_format(style='plain', axis='y')\n",
    "\n",
    "for bar, val in zip(bars1, gb_values):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, val,\n",
    "            f'{val:,.0f} GB',\n",
    "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Subplot 2: TB\n",
    "tb_values = [total_tb_old, total_tb_real]\n",
    "bars2 = ax2.bar(approaches, tb_values, color=colors1, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "ax2.set_ylabel('Terabytes (TB)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Peso Total en Terabytes', fontsize=13, fontweight='bold')\n",
    "\n",
    "for bar, val in zip(bars2, tb_values):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, val,\n",
    "            f'{val:.2f} TB',\n",
    "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Título general\n",
    "ahorro_pct = ((total_gb_real / total_gb_old) - 1) * 100\n",
    "fig.suptitle(f'Peso Total: Universo Completo E1-E11 ({total_unique_ticker_days:,} ticker-days) | Ahorro: {ahorro_pct:+.1f}%',\n",
    "             fontsize=15, fontweight='bold', y=1.00)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('peso_total_universo_completo.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Gráfico guardado: peso_total_universo_completo.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusiones\n",
    "\n",
    "### Hallazgos Clave:\n",
    "\n",
    "1. **Consolidación de Días Únicos**: \n",
    "   - El cálculo ingenuo de expandir ventanas por evento genera duplicados masivos\n",
    "   - Al consolidar por ticker-date único, se reduce significativamente el volumen\n",
    "\n",
    "2. **Métrica Real vs Estimación**:\n",
    "   - La estimación anterior (50 MB/ticker-day) estaba **sobrestimada**\n",
    "   - La métrica real del pilot (0.183 MB/ticker-day con ZSTD) es **273x más eficiente**\n",
    "   - Esto cambia completamente la viabilidad del proyecto\n",
    "\n",
    "3. **Peso Total REAL**:\n",
    "   - Con métrica real: **mucho más manejable** que estimaciones anteriores\n",
    "   - La descarga completa del universo E1-E11 es **VIABLE**\n",
    "\n",
    "4. **Tiempo de Descarga**:\n",
    "   - Con 6 workers y rate limit de 0.12s: tiempo estimado calculado arriba\n",
    "   - Resume capability permite descarga incremental\n",
    "\n",
    "### Recomendaciones:\n",
    "\n",
    "1. **Descarga por Lotes**: Dividir en batches de ~100-200 GB por disco disponible\n",
    "2. **Priorización**: Comenzar por tickers con mayor `event_count` (multi-evento)\n",
    "3. **Monitoreo**: Validar que la métrica real se mantiene durante la descarga masiva\n",
    "4. **Backup Strategy**: Plan de backup incremental durante descarga\n",
    "\n",
    "### Próximo Paso:\n",
    "\n",
    "**DECISIÓN**: ¿Proceder con descarga completa o expandir pilot a 50-100 tickers primero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar resultado final\n",
    "output_file = Path('universo_completo_E1_E11_analisis.parquet')\n",
    "df_unique_downloads.write_parquet(output_file)\n",
    "\n",
    "print(f'\\nResultado guardado: {output_file}')\n",
    "print(f'  Total ticker-days únicos: {total_unique_ticker_days:,}')\n",
    "print(f'  Peso estimado (REAL): {total_gb_real:,.2f} GB = {total_tb_real:.2f} TB')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
