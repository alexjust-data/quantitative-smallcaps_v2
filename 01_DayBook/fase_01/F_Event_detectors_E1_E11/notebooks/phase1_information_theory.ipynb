{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fase 1: Information Theory - Validación Híbrida Ventanas\n",
    "\n",
    "**Objetivo**: Calcular Mutual Information entre features diarias y retornos futuros para identificar días con información predictiva.\n",
    "\n",
    "**Método**: Information Theory (model-agnostic)\n",
    "- Mutual Information I(X_t; y) por día relativo\n",
    "- Filtrado rápido: descarta días sin señal\n",
    "- Solo usa columnas básicas de DIB bars\n",
    "\n",
    "**Output**: `phase1_results.pkl` con info_results por evento\n",
    "\n",
    "**Tiempo estimado**: 10-20 min (con sample_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mutual_info_score\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Config\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "# Paths\n",
    "BARS_ROOT = Path('../../../../processed/dib_bars/pilot50_validation')\n",
    "WATCHLIST = Path('../../../../processed/universe/pilot50_validation/daily')\n",
    "OUTPUT_DIR = Path('.')\n",
    "\n",
    "print(f\"DIB bars dir exists: {BARS_ROOT.exists()}\")\n",
    "print(f\"Watchlist exists: {WATCHLIST.exists()}\")\n",
    "print(f\"Output dir: {OUTPUT_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cargar Watchlist con Eventos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar todos los watchlists particionados por fecha\n",
    "watchlist_files = list(WATCHLIST.rglob('watchlist.parquet'))\n",
    "print(f\"Encontrados {len(watchlist_files):,} watchlist files\")\n",
    "\n",
    "wl_parts = []\n",
    "for wl_file in watchlist_files:\n",
    "    # Extract date from path: date=YYYY-MM-DD/watchlist.parquet\n",
    "    date_str = wl_file.parent.name.split('=')[1]\n",
    "    df = pl.read_parquet(wl_file)\n",
    "    df = df.with_columns([pl.lit(date_str).alias('date')])\n",
    "    wl_parts.append(df)\n",
    "\n",
    "wl = pl.concat(wl_parts)\n",
    "print(f\"Total watchlist rows: {wl.height:,}\")\n",
    "\n",
    "# Convertir date a pl.Date\n",
    "wl = wl.with_columns([\n",
    "    pl.col('date').str.strptime(pl.Date, format='%Y-%m-%d')\n",
    "])\n",
    "\n",
    "# Expandir una fila por evento\n",
    "wl_expanded = wl.explode('events').rename({'events': 'event_code'})\n",
    "print(f\"Total event occurrences: {wl_expanded.height:,}\")\n",
    "\n",
    "# Eventos disponibles\n",
    "events_available = sorted(wl_expanded['event_code'].unique().to_list())\n",
    "print(f\"\\nEventos disponibles: {events_available}\")\n",
    "\n",
    "wl_expanded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Funciones de Información Mutua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dib_bars_day(ticker: str, day: datetime.date) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Carga DIB bars de un ticker en un día específico.\n",
    "    \"\"\"\n",
    "    bars_file = BARS_ROOT / ticker / f\"date={day.isoformat()}\" / \"dollar_imbalance.parquet\"\n",
    "    if not bars_file.exists():\n",
    "        return None\n",
    "    return pl.read_parquet(bars_file)\n",
    "\n",
    "\n",
    "def aggregate_day_features(df_bars: pl.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Agrega features intradía de DIB bars a features diarias.\n",
    "    Solo usa columnas básicas: o, h, l, c, v, n, dollar, imbalance_score\n",
    "    \"\"\"\n",
    "    if df_bars is None or df_bars.height == 0:\n",
    "        return None\n",
    "    \n",
    "    # Calcular features agregados del día\n",
    "    agg = df_bars.select([\n",
    "        ((pl.col('c') - pl.col('o')) / pl.col('o')).mean().alias('ret_day'),\n",
    "        ((pl.col('h') - pl.col('l')) / pl.col('o')).mean().alias('range_day'),\n",
    "        pl.col('v').sum().alias('vol_day'),\n",
    "        pl.col('dollar').sum().alias('dollar_day'),\n",
    "        pl.col('imbalance_score').mean().alias('imb_day'),\n",
    "        pl.col('n').sum().alias('n_bars')\n",
    "    ])\n",
    "    \n",
    "    return agg.to_dicts()[0] if agg.height > 0 else None\n",
    "\n",
    "\n",
    "def calculate_mutual_information_discretized(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    bins: int = 10\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calcula mutual information promedio entre features X y target y.\n",
    "    \"\"\"\n",
    "    y_binned = pd.cut(y, bins=bins, labels=False, duplicates='drop')\n",
    "    \n",
    "    mi_scores = []\n",
    "    for col_idx in range(X.shape[1]):\n",
    "        x_col = X[:, col_idx]\n",
    "        x_binned = pd.cut(x_col, bins=bins, labels=False, duplicates='drop')\n",
    "        \n",
    "        valid_mask = ~(pd.isna(x_binned) | pd.isna(y_binned))\n",
    "        if valid_mask.sum() > 10:\n",
    "            mi = mutual_info_score(x_binned[valid_mask], y_binned[valid_mask])\n",
    "            mi_scores.append(mi)\n",
    "    \n",
    "    return np.mean(mi_scores) if mi_scores else 0.0\n",
    "\n",
    "\n",
    "print(\"✓ Funciones de información mutua definidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calcular MI por Día Relativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_information_by_relative_day(\n",
    "    event_code: str,\n",
    "    max_pre: int = 7,\n",
    "    max_post: int = 7,\n",
    "    sample_size: int = 500\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Para un evento, calcula I(X_t; y) para cada día t relativo al evento.\n",
    "    \n",
    "    Returns:\n",
    "        {rel_day: mutual_information_score}\n",
    "    \"\"\"\n",
    "    # Filtrar eventos de este tipo\n",
    "    subset = wl_expanded.filter(pl.col('event_code') == event_code)\n",
    "    \n",
    "    # Sample para acelerar (opcional)\n",
    "    if subset.height > sample_size:\n",
    "        subset = subset.sample(sample_size, seed=42)\n",
    "    \n",
    "    print(f\"\\nAnalizando {event_code}: {subset.height} ocurrencias\")\n",
    "    \n",
    "    # Recolectar datos por día relativo\n",
    "    data_by_day = {}\n",
    "    \n",
    "    for rel_day in range(-max_pre, max_post + 1):\n",
    "        features_list = []\n",
    "        targets_list = []\n",
    "        \n",
    "        for row in subset.iter_rows(named=True):\n",
    "            ticker = row['ticker']\n",
    "            t0 = row['date']\n",
    "            \n",
    "            # Día relativo actual\n",
    "            d = t0 + timedelta(days=rel_day)\n",
    "            bars = load_dib_bars_day(ticker, d)\n",
    "            \n",
    "            if bars is None or bars.height == 0:\n",
    "                continue\n",
    "            \n",
    "            # Features agregados del día\n",
    "            feat = aggregate_day_features(bars)\n",
    "            if feat is None:\n",
    "                continue\n",
    "            \n",
    "            # Target: retorno futuro desde t0 (día evento)\n",
    "            # Usamos bars del día t0+1, t0+2, t0+3 para calcular ret_3d\n",
    "            bars_t0 = load_dib_bars_day(ticker, t0)\n",
    "            bars_t3 = load_dib_bars_day(ticker, t0 + timedelta(days=3))\n",
    "            \n",
    "            if bars_t0 is None or bars_t3 is None:\n",
    "                continue\n",
    "            if bars_t0.height == 0 or bars_t3.height == 0:\n",
    "                continue\n",
    "            \n",
    "            # Calcular retorno 3d\n",
    "            p0 = bars_t0['c'][-1]\n",
    "            p3 = bars_t3['c'][-1]\n",
    "            ret_3d = (p3 - p0) / p0\n",
    "            \n",
    "            features_list.append(list(feat.values()))\n",
    "            targets_list.append(ret_3d)\n",
    "        \n",
    "        if len(features_list) < 50:\n",
    "            data_by_day[rel_day] = 0.0\n",
    "            continue\n",
    "        \n",
    "        X = np.array(features_list)\n",
    "        y = np.array(targets_list)\n",
    "        \n",
    "        # Calcular MI\n",
    "        mi = calculate_mutual_information_discretized(X, y, bins=10)\n",
    "        data_by_day[rel_day] = mi\n",
    "        \n",
    "        print(f\"  t={rel_day:+d}: MI={mi:.4f} (n={len(features_list)})\")\n",
    "    \n",
    "    return data_by_day\n",
    "\n",
    "\n",
    "print(\"✓ Función de análisis por día relativo definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ejecutar Análisis Information Theory\n",
    "\n",
    "**NOTA**: Ajusta `EVENTS_TO_TEST` según necesites:\n",
    "- `[:3]` → Prueba rápida (3 eventos, ~10-15 min)\n",
    "- Sin slice → Análisis completo (11 eventos, ~40-60 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURACIÓN: Ajusta aquí el subset de eventos\n",
    "EVENTS_TO_TEST = events_available[:3]  # Cambiar a events_available para análisis completo\n",
    "MAX_PRE_DAYS = 3\n",
    "MAX_POST_DAYS = 3\n",
    "SAMPLE_SIZE = 200  # Reducir a 100 para más velocidad, aumentar a 500 para más precisión\n",
    "\n",
    "print(f\"Analizando {len(EVENTS_TO_TEST)} eventos con ventana [{-MAX_PRE_DAYS}, {MAX_POST_DAYS}]\")\n",
    "print(f\"Sample size: {SAMPLE_SIZE} ocurrencias por evento\\n\")\n",
    "\n",
    "info_results = {}\n",
    "\n",
    "for event in EVENTS_TO_TEST:\n",
    "    info_by_day = analyze_information_by_relative_day(\n",
    "        event,\n",
    "        max_pre=MAX_PRE_DAYS,\n",
    "        max_post=MAX_POST_DAYS,\n",
    "        sample_size=SAMPLE_SIZE\n",
    "    )\n",
    "    info_results[event] = info_by_day\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ Análisis Information Theory completado\")\n",
    "print(f\"Eventos analizados: {len(info_results)}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizar Información por Día"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(info_results), 1, figsize=(12, 4 * len(info_results)))\n",
    "\n",
    "if len(info_results) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, (event, info_by_day) in enumerate(info_results.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    days = sorted(info_by_day.keys())\n",
    "    mi_scores = [info_by_day[d] for d in days]\n",
    "    \n",
    "    # Normalizar\n",
    "    max_mi = max(mi_scores) if max(mi_scores) > 0 else 1.0\n",
    "    mi_norm = [m / max_mi for m in mi_scores]\n",
    "    \n",
    "    # Plot\n",
    "    ax.bar(days, mi_norm, alpha=0.7, color='steelblue')\n",
    "    ax.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Día Evento (t=0)')\n",
    "    ax.axhline(y=0.1, color='orange', linestyle=':', label='Threshold 10%')\n",
    "    \n",
    "    # Marcar días significativos\n",
    "    significant_days = [d for d, mi in zip(days, mi_norm) if mi >= 0.1]\n",
    "    if significant_days:\n",
    "        t_start, t_end = min(significant_days), max(significant_days)\n",
    "        ax.axvspan(t_start - 0.5, t_end + 0.5, alpha=0.2, color='green',\n",
    "                   label=f'Ventana sugerida: [{t_start}, {t_end}]')\n",
    "    \n",
    "    ax.set_xlabel('Días Relativos al Evento')\n",
    "    ax.set_ylabel('Mutual Information (normalizado)')\n",
    "    ax.set_title(f'{event}: Información por Día Relativo')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('information_by_day_phase1.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Gráfico guardado: information_by_day_phase1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Resumen: Ventanas Sugeridas por MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VENTANAS SUGERIDAS POR MUTUAL INFORMATION (threshold 10%)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "suggested_windows = {}\n",
    "\n",
    "for event, info_by_day in info_results.items():\n",
    "    days = sorted(info_by_day.keys())\n",
    "    mi_scores = [info_by_day[d] for d in days]\n",
    "    max_mi = max(mi_scores) if max(mi_scores) > 0 else 1.0\n",
    "    mi_norm = [m / max_mi for m in mi_scores]\n",
    "    significant_days = [d for d, mi in zip(days, mi_norm) if mi >= 0.1]\n",
    "    \n",
    "    if significant_days:\n",
    "        window = (min(significant_days), max(significant_days))\n",
    "        suggested_windows[event] = window\n",
    "        print(f\"  {event:<25} → [{window[0]:+d}, {window[1]:+d}]\")\n",
    "    else:\n",
    "        suggested_windows[event] = None\n",
    "        print(f\"  {event:<25} → Sin ventana clara (MI muy bajo)\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Guardar Resultados Fase 1\n",
    "\n",
    "**Output**: `phase1_results.pkl` con todos los datos necesarios para Fase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empaquetar resultados\n",
    "results_phase1 = {\n",
    "    'info_results': info_results,\n",
    "    'wl_expanded': wl_expanded,\n",
    "    'events_available': events_available,\n",
    "    'suggested_windows': suggested_windows,\n",
    "    'config': {\n",
    "        'max_pre_days': MAX_PRE_DAYS,\n",
    "        'max_post_days': MAX_POST_DAYS,\n",
    "        'sample_size': SAMPLE_SIZE,\n",
    "        'events_tested': EVENTS_TO_TEST\n",
    "    }\n",
    "}\n",
    "\n",
    "# Guardar a disco\n",
    "output_file = OUTPUT_DIR / 'phase1_results.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(results_phase1, f)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ FASE 1 COMPLETADA\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Resultados guardados en: {output_file.absolute()}\")\n",
    "print(f\"\\nContenido:\")\n",
    "print(f\"  - info_results: {len(info_results)} eventos con MI por día relativo\")\n",
    "print(f\"  - wl_expanded: {wl_expanded.height:,} event occurrences\")\n",
    "print(f\"  - events_available: {len(events_available)} eventos totales\")\n",
    "print(f\"  - suggested_windows: {len([w for w in suggested_windows.values() if w])} ventanas sugeridas\")\n",
    "print(f\"\\nPróximo paso: Ejecutar phase2_model_performance.ipynb\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
