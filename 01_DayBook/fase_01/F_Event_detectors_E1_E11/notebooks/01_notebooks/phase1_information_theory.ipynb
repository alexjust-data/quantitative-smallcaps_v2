{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fase 1: Information Theory - Validación Híbrida Ventanas\n",
    "\n",
    "**Objetivo**: Calcular Mutual Information entre features diarias y retornos futuros para identificar días con información predictiva.\n",
    "\n",
    "**Método**: Information Theory (model-agnostic)\n",
    "- Mutual Information I(X_t; y) por día relativo\n",
    "- Filtrado rápido: descarta días sin señal\n",
    "- Solo usa columnas básicas de DIB bars\n",
    "\n",
    "**Output**: `phase1_results.pkl` con info_results por evento\n",
    "\n",
    "**Tiempo estimado**: 10-20 min (con sample_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mutual_info_score\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Config\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "# Paths\n",
    "BARS_ROOT = Path('../../../../processed/dib_bars/pilot50_validation')\n",
    "WATCHLIST = Path('../../../../processed/universe/pilot50_validation/daily')\n",
    "OUTPUT_DIR = Path('.')\n",
    "\n",
    "print(f\"DIB bars dir exists: {BARS_ROOT.exists()}\")\n",
    "print(f\"Watchlist exists: {WATCHLIST.exists()}\")\n",
    "print(f\"Output dir: {OUTPUT_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cargar Watchlist con Eventos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar todos los watchlists particionados por fecha\n",
    "watchlist_files = list(WATCHLIST.rglob('watchlist.parquet'))\n",
    "print(f\"Encontrados {len(watchlist_files):,} watchlist files\")\n",
    "\n",
    "wl_parts = []\n",
    "for wl_file in watchlist_files:\n",
    "    # Extract date from path: date=YYYY-MM-DD/watchlist.parquet\n",
    "    date_str = wl_file.parent.name.split('=')[1]\n",
    "    df = pl.read_parquet(wl_file)\n",
    "    df = df.with_columns([pl.lit(date_str).alias('date')])\n",
    "    wl_parts.append(df)\n",
    "\n",
    "wl = pl.concat(wl_parts)\n",
    "print(f\"Total watchlist rows: {wl.height:,}\")\n",
    "\n",
    "# Convertir date a pl.Date\n",
    "wl = wl.with_columns([\n",
    "    pl.col('date').str.strptime(pl.Date, format='%Y-%m-%d')\n",
    "])\n",
    "\n",
    "# Expandir una fila por evento\n",
    "wl_expanded = wl.explode('events').rename({'events': 'event_code'})\n",
    "print(f\"Total event occurrences: {wl_expanded.height:,}\")\n",
    "\n",
    "# Eventos disponibles\n",
    "events_available = sorted(wl_expanded['event_code'].unique().to_list())\n",
    "print(f\"\\nEventos disponibles: {events_available}\")\n",
    "\n",
    "wl_expanded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Funciones de Información Mutua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dib_bars_day(ticker: str, day: datetime.date) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Carga DIB bars de un ticker en un día específico.\n",
    "    \"\"\"\n",
    "    bars_file = BARS_ROOT / ticker / f\"date={day.isoformat()}\" / \"dollar_imbalance.parquet\"\n",
    "    if not bars_file.exists():\n",
    "        return None\n",
    "    return pl.read_parquet(bars_file)\n",
    "\n",
    "\n",
    "def aggregate_day_features(df_bars: pl.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Agrega features intradía de DIB bars a features diarias.\n",
    "    Solo usa columnas básicas: o, h, l, c, v, n, dollar, imbalance_score\n",
    "    \"\"\"\n",
    "    if df_bars is None or df_bars.height == 0:\n",
    "        return None\n",
    "    \n",
    "    # Calcular features agregados del día\n",
    "    agg = df_bars.select([\n",
    "        ((pl.col('c') - pl.col('o')) / pl.col('o')).mean().alias('ret_day'),\n",
    "        ((pl.col('h') - pl.col('l')) / pl.col('o')).mean().alias('range_day'),\n",
    "        pl.col('v').sum().alias('vol_day'),\n",
    "        pl.col('dollar').sum().alias('dollar_day'),\n",
    "        pl.col('imbalance_score').mean().alias('imb_day'),\n",
    "        pl.col('n').sum().alias('n_bars')\n",
    "    ])\n",
    "    \n",
    "    return agg.to_dicts()[0] if agg.height > 0 else None\n",
    "\n",
    "\n",
    "def calculate_mutual_information_discretized(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    bins: int = 10\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calcula mutual information promedio entre features X y target y.\n",
    "    \"\"\"\n",
    "    y_binned = pd.cut(y, bins=bins, labels=False, duplicates='drop')\n",
    "    \n",
    "    mi_scores = []\n",
    "    for col_idx in range(X.shape[1]):\n",
    "        x_col = X[:, col_idx]\n",
    "        x_binned = pd.cut(x_col, bins=bins, labels=False, duplicates='drop')\n",
    "        \n",
    "        valid_mask = ~(pd.isna(x_binned) | pd.isna(y_binned))\n",
    "        if valid_mask.sum() > 10:\n",
    "            mi = mutual_info_score(x_binned[valid_mask], y_binned[valid_mask])\n",
    "            mi_scores.append(mi)\n",
    "    \n",
    "    return np.mean(mi_scores) if mi_scores else 0.0\n",
    "\n",
    "\n",
    "print(\"✓ Funciones de información mutua definidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calcular MI por Día Relativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_information_by_relative_day(\n",
    "    event_code: str,\n",
    "    max_pre: int = 7,\n",
    "    max_post: int = 7,\n",
    "    sample_size: int = 500\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Para un evento, calcula I(X_t; y) para cada día t relativo al evento.\n",
    "    \n",
    "    Returns:\n",
    "        {rel_day: mutual_information_score}\n",
    "    \"\"\"\n",
    "    # Filtrar eventos de este tipo\n",
    "    subset = wl_expanded.filter(pl.col('event_code') == event_code)\n",
    "    \n",
    "    # Sample para acelerar (opcional)\n",
    "    if subset.height > sample_size:\n",
    "        subset = subset.sample(sample_size, seed=42)\n",
    "    \n",
    "    print(f\"\\nAnalizando {event_code}: {subset.height} ocurrencias\")\n",
    "    \n",
    "    # Recolectar datos por día relativo\n",
    "    data_by_day = {}\n",
    "    \n",
    "    for rel_day in range(-max_pre, max_post + 1):\n",
    "        features_list = []\n",
    "        targets_list = []\n",
    "        \n",
    "        for row in subset.iter_rows(named=True):\n",
    "            ticker = row['ticker']\n",
    "            t0 = row['date']\n",
    "            \n",
    "            # Día relativo actual\n",
    "            d = t0 + timedelta(days=rel_day)\n",
    "            bars = load_dib_bars_day(ticker, d)\n",
    "            \n",
    "            if bars is None or bars.height == 0:\n",
    "                continue\n",
    "            \n",
    "            # Features agregados del día\n",
    "            feat = aggregate_day_features(bars)\n",
    "            if feat is None:\n",
    "                continue\n",
    "            \n",
    "            # Target: retorno futuro desde t0 (día evento)\n",
    "            # Usamos bars del día t0+1, t0+2, t0+3 para calcular ret_3d\n",
    "            bars_t0 = load_dib_bars_day(ticker, t0)\n",
    "            bars_t3 = load_dib_bars_day(ticker, t0 + timedelta(days=3))\n",
    "            \n",
    "            if bars_t0 is None or bars_t3 is None:\n",
    "                continue\n",
    "            if bars_t0.height == 0 or bars_t3.height == 0:\n",
    "                continue\n",
    "            \n",
    "            # Calcular retorno 3d\n",
    "            p0 = bars_t0['c'][-1]\n",
    "            p3 = bars_t3['c'][-1]\n",
    "            ret_3d = (p3 - p0) / p0\n",
    "            \n",
    "            features_list.append(list(feat.values()))\n",
    "            targets_list.append(ret_3d)\n",
    "        \n",
    "        if len(features_list) < 50:\n",
    "            data_by_day[rel_day] = 0.0\n",
    "            continue\n",
    "        \n",
    "        X = np.array(features_list)\n",
    "        y = np.array(targets_list)\n",
    "        \n",
    "        # Calcular MI\n",
    "        mi = calculate_mutual_information_discretized(X, y, bins=10)\n",
    "        data_by_day[rel_day] = mi\n",
    "        \n",
    "        print(f\"  t={rel_day:+d}: MI={mi:.4f} (n={len(features_list)})\")\n",
    "    \n",
    "    return data_by_day\n",
    "\n",
    "\n",
    "print(\"✓ Función de análisis por día relativo definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ejecutar Análisis Information Theory\n",
    "\n",
    "**NOTA**: Ajusta `EVENTS_TO_TEST` según necesites:\n",
    "- `[:3]` → Prueba rápida (3 eventos, ~10-15 min)\n",
    "- Sin slice → Análisis completo (11 eventos, ~40-60 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURACIÓN: Ajusta aquí el subset de eventos\n",
    "EVENTS_TO_TEST = events_available[:3]  # Cambiar a events_available para análisis completo\n",
    "MAX_PRE_DAYS = 3\n",
    "MAX_POST_DAYS = 3\n",
    "SAMPLE_SIZE = 200  # Reducir a 100 para más velocidad, aumentar a 500 para más precisión\n",
    "\n",
    "print(f\"Analizando {len(EVENTS_TO_TEST)} eventos con ventana [{-MAX_PRE_DAYS}, {MAX_POST_DAYS}]\")\n",
    "print(f\"Sample size: {SAMPLE_SIZE} ocurrencias por evento\\n\")\n",
    "\n",
    "info_results = {}\n",
    "\n",
    "for event in EVENTS_TO_TEST:\n",
    "    info_by_day = analyze_information_by_relative_day(\n",
    "        event,\n",
    "        max_pre=MAX_PRE_DAYS,\n",
    "        max_post=MAX_POST_DAYS,\n",
    "        sample_size=SAMPLE_SIZE\n",
    "    )\n",
    "    info_results[event] = info_by_day\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ Análisis Information Theory completado\")\n",
    "print(f\"Eventos analizados: {len(info_results)}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizar Información por Día"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(info_results), 1, figsize=(12, 4 * len(info_results)))\n",
    "\n",
    "if len(info_results) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, (event, info_by_day) in enumerate(info_results.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    days = sorted(info_by_day.keys())\n",
    "    mi_scores = [info_by_day[d] for d in days]\n",
    "    \n",
    "    # Normalizar\n",
    "    max_mi = max(mi_scores) if max(mi_scores) > 0 else 1.0\n",
    "    mi_norm = [m / max_mi for m in mi_scores]\n",
    "    \n",
    "    # Plot\n",
    "    ax.bar(days, mi_norm, alpha=0.7, color='steelblue')\n",
    "    ax.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Día Evento (t=0)')\n",
    "    ax.axhline(y=0.1, color='orange', linestyle=':', label='Threshold 10%')\n",
    "    \n",
    "    # Marcar días significativos\n",
    "    significant_days = [d for d, mi in zip(days, mi_norm) if mi >= 0.1]\n",
    "    if significant_days:\n",
    "        t_start, t_end = min(significant_days), max(significant_days)\n",
    "        ax.axvspan(t_start - 0.5, t_end + 0.5, alpha=0.2, color='green',\n",
    "                   label=f'Ventana sugerida: [{t_start}, {t_end}]')\n",
    "    \n",
    "    ax.set_xlabel('Días Relativos al Evento')\n",
    "    ax.set_ylabel('Mutual Information (normalizado)')\n",
    "    ax.set_title(f'{event}: Información por Día Relativo')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('information_by_day_phase1.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Gráfico guardado: information_by_day_phase1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Resumen: Ventanas Sugeridas por MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VENTANAS SUGERIDAS POR MUTUAL INFORMATION (threshold 10%)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "suggested_windows = {}\n",
    "\n",
    "for event, info_by_day in info_results.items():\n",
    "    days = sorted(info_by_day.keys())\n",
    "    mi_scores = [info_by_day[d] for d in days]\n",
    "    max_mi = max(mi_scores) if max(mi_scores) > 0 else 1.0\n",
    "    mi_norm = [m / max_mi for m in mi_scores]\n",
    "    significant_days = [d for d, mi in zip(days, mi_norm) if mi >= 0.1]\n",
    "    \n",
    "    if significant_days:\n",
    "        window = (min(significant_days), max(significant_days))\n",
    "        suggested_windows[event] = window\n",
    "        print(f\"  {event:<25} → [{window[0]:+d}, {window[1]:+d}]\")\n",
    "    else:\n",
    "        suggested_windows[event] = None\n",
    "        print(f\"  {event:<25} → Sin ventana clara (MI muy bajo)\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Guardar Resultados Fase 1\n",
    "\n",
    "**Output**: `phase1_results.pkl` con todos los datos necesarios para Fase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empaquetar resultados\n",
    "results_phase1 = {\n",
    "    'info_results': info_results,\n",
    "    'wl_expanded': wl_expanded,\n",
    "    'events_available': events_available,\n",
    "    'suggested_windows': suggested_windows,\n",
    "    'config': {\n",
    "        'max_pre_days': MAX_PRE_DAYS,\n",
    "        'max_post_days': MAX_POST_DAYS,\n",
    "        'sample_size': SAMPLE_SIZE,\n",
    "        'events_tested': EVENTS_TO_TEST\n",
    "    }\n",
    "}\n",
    "\n",
    "# Guardar a disco\n",
    "output_file = OUTPUT_DIR / 'phase1_results.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(results_phase1, f)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ FASE 1 COMPLETADA\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Resultados guardados en: {output_file.absolute()}\")\n",
    "print(f\"\\nContenido:\")\n",
    "print(f\"  - info_results: {len(info_results)} eventos con MI por día relativo\")\n",
    "print(f\"  - wl_expanded: {wl_expanded.height:,} event occurrences\")\n",
    "print(f\"  - events_available: {len(events_available)} eventos totales\")\n",
    "print(f\"  - suggested_windows: {len([w for w in suggested_windows.values() if w])} ventanas sugeridas\")\n",
    "print(f\"\\nPróximo paso: Ejecutar phase2_model_performance.ipynb\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "source": "def export_events_for_tradingview(event_code: str, output_dir: Path = Path('.')):\n    \"\"\"\n    Exporta eventos con timestamps exactos para visualización en TradingView.\n    \n    Para cada evento crea CSV con:\n    - ticker\n    - datetime (timestamp exacto del primer bar del evento)\n    - close_price (precio en el momento del evento)\n    - event_code\n    - window_suggested (ventana sugerida por MI)\n    \"\"\"\n    # Filtrar eventos de este tipo\n    subset = wl_expanded.filter(pl.col('event_code') == event_code)\n    \n    print(f\"\\nProcesando {event_code}: {subset.height:,} ocurrencias\")\n    \n    events_data = []\n    \n    for idx, row in enumerate(subset.iter_rows(named=True)):\n        ticker = row['ticker']\n        event_date = row['date']\n        \n        # Cargar DIB bars del día del evento para obtener timestamp exacto\n        bars = load_dib_bars_day(ticker, event_date)\n        \n        if bars is None or bars.height == 0:\n            continue\n        \n        # Timestamp del PRIMER bar del evento (inicio de la sesión)\n        first_ts = bars['t_open'][0]  # Timestamp de apertura del primer bar\n        close_price = bars['c'][0]  # Precio de cierre del primer bar\n        \n        # Ventana sugerida\n        window = suggested_windows.get(event_code, None)\n        window_str = f\"[{window[0]:+d},{window[1]:+d}]\" if window else \"N/A\"\n        \n        events_data.append({\n            'ticker': ticker,\n            'datetime': first_ts,\n            'close_price': close_price,\n            'event_code': event_code,\n            'window_suggested': window_str,\n            'date': event_date\n        })\n        \n        # Progress cada 100 eventos\n        if (idx + 1) % 100 == 0:\n            print(f\"  Procesados {idx + 1:,} / {subset.height:,} eventos...\")\n    \n    if not events_data:\n        print(f\"⚠️  No se encontraron datos para {event_code}\")\n        return None\n    \n    # Crear DataFrame\n    df = pd.DataFrame(events_data)\n    \n    # Ordenar por datetime\n    df = df.sort_values('datetime')\n    \n    # Exportar CSV\n    output_file = output_dir / f'tradingview_{event_code}.csv'\n    df.to_csv(output_file, index=False)\n    \n    print(f\"✓ Exportado: {output_file}\")\n    print(f\"  Total eventos: {len(df):,}\")\n    print(f\"  Tickers únicos: {df['ticker'].nunique()}\")\n    print(f\"  Rango fechas: {df['date'].min()} a {df['date'].max()}\")\n    print(f\"  Ventana sugerida: {window_str}\")\n    \n    return df\n\n\n# Crear directorio para exports\nTRADINGVIEW_DIR = OUTPUT_DIR / 'tradingview_exports'\nTRADINGVIEW_DIR.mkdir(exist_ok=True)\n\nprint(\"=\"*80)\nprint(\"EXPORTANDO EVENTOS PARA TRADINGVIEW\")\nprint(\"=\"*80)\n\nall_exports = {}\n\n# Exportar TODOS los eventos disponibles (no solo los analizados)\nfor event in events_available:\n    df = export_events_for_tradingview(event, TRADINGVIEW_DIR)\n    if df is not None:\n        all_exports[event] = df\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"✓ EXPORTACIÓN COMPLETADA\")\nprint(\"=\"*80)\nprint(f\"Directorio: {TRADINGVIEW_DIR.absolute()}\")\nprint(f\"Archivos generados: {len(all_exports)}\")\nprint(f\"\\nEventos exportados:\")\nfor event, df in all_exports.items():\n    print(f\"  • {event:<30} → {len(df):>6,} ocurrencias\")\nprint(\"=\"*80)\n\nprint(\"\\n📊 CÓMO USAR EN TRADINGVIEW:\")\nprint(\"1. Abre TradingView con el ticker que quieres analizar\")\nprint(\"2. Importa el CSV correspondiente como 'Custom Indicator'\")\nprint(\"3. Los eventos aparecerán como marcadores en el gráfico\")\nprint(\"4. Filtra por ticker si el CSV contiene múltiples símbolos\")\nprint(\"\\nEjemplo: Para ver E10_FirstGreenBounce en ticker AAPL:\")\nprint(f\"  → Carga: {TRADINGVIEW_DIR / 'tradingview_E10_FirstGreenBounce.csv'}\")\nprint(\"  → Filtra columna 'ticker' = 'AAPL'\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 10. EXPORTAR EVENTOS PARA TRADINGVIEW\n\n**Objetivo**: Generar CSV con timestamps exactos de TODOS los eventos para visualización en TradingView.\n\n**Formato TradingView**:\n- Necesita timestamp exacto (fecha + hora)\n- Un archivo por evento\n- Columnas: ticker, datetime, event_code, price_at_event\n\nEsto te permitirá:\n1. Cargar los eventos como overlays en TradingView\n2. Verificar visualmente cada evento sobre el gráfico de precio\n3. Validar que la detección fue correcta",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 9. INTERPRETACIÓN: ¿Son creíbles los resultados?\n\n**Criterios de validación:**\n\n1. **Captura de rango > 70%**: La ventana debe contener la mayor parte del movimiento\n2. **Volatilidad ratio > 1.5x**: Dentro de ventana debe haber más volatilidad que fuera\n3. **Consistencia visual**: Los gráficos deben mostrar patrones claros\n\n**Posibles problemas detectados:**\n- Si todas las ventanas son [-3, +3] → Threshold demasiado bajo, MI no discrimina\n- Si captura < 50% → Ventana no útil para trading\n- Si vol_ratio < 1.0 → Ventana captura el período tranquilo (ERROR)\n\n**Próximos pasos si resultados no son creíbles:**\n1. Ajustar threshold de MI (probar 30%, 50% en lugar de 10%)\n2. Usar MI absoluto en lugar de normalizado\n3. Comparar con Phase 2 (LightGBM) para validación cruzada",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def get_daily_closes(ticker: str, start_date, end_date):\n    \"\"\"\n    Obtiene precios de cierre diarios para un rango de fechas.\n    Devuelve: list of (date, close_price)\n    \"\"\"\n    prices = []\n    current = start_date\n    \n    while current <= end_date:\n        bars = load_dib_bars_day(ticker, current)\n        if bars is not None and bars.height > 0:\n            close_price = bars['c'][-1]  # Último cierre del día\n            prices.append((current, close_price))\n        current += timedelta(days=1)\n    \n    return prices\n\n\ndef visualize_window_capture(event_code: str, window: tuple, n_examples: int = 6):\n    \"\"\"\n    Visualiza ejemplos reales de eventos con ventana superpuesta.\n    Calcula % de movimiento capturado.\n    \"\"\"\n    if window is None:\n        print(f\"❌ {event_code}: Sin ventana sugerida\")\n        return\n    \n    pre, post = window\n    \n    # Obtener ocurrencias del evento\n    subset = wl_expanded.filter(pl.col('event_code') == event_code)\n    \n    # Tomar muestra aleatoria\n    if subset.height > n_examples:\n        subset = subset.sample(n_examples, seed=123)\n    else:\n        n_examples = subset.height\n    \n    # Configurar grid de subplots\n    n_cols = 3\n    n_rows = (n_examples + n_cols - 1) // n_cols\n    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n    axes = axes.flatten() if n_examples > 1 else [axes]\n    \n    capture_stats = []\n    \n    for idx, row in enumerate(subset.iter_rows(named=True)):\n        if idx >= n_examples:\n            break\n            \n        ticker = row['ticker']\n        t0 = row['date']\n        \n        # Cargar precios desde t-5 hasta t+5 (ventana amplia para contexto)\n        start = t0 - timedelta(days=5)\n        end = t0 + timedelta(days=5)\n        prices = get_daily_closes(ticker, start, end)\n        \n        if len(prices) < 5:  # Muy pocos datos\n            continue\n        \n        # Separar fechas y precios\n        dates = [p[0] for p in prices]\n        closes = [p[1] for p in prices]\n        \n        # Calcular días relativos al evento\n        rel_days = [(d - t0).days for d in dates]\n        \n        # Identificar puntos dentro de ventana sugerida\n        in_window = [(pre <= rd <= post) for rd in rel_days]\n        \n        # Calcular captura de movimiento\n        all_prices = closes\n        window_prices = [p for p, in_w in zip(closes, in_window) if in_w]\n        \n        if len(window_prices) < 2 or len(all_prices) < 2:\n            continue\n        \n        # Rango total vs rango capturado\n        total_range = max(all_prices) - min(all_prices)\n        window_range = max(window_prices) - min(window_prices)\n        capture_pct = (window_range / total_range * 100) if total_range > 0 else 0\n        \n        # Volatilidad dentro vs fuera\n        prices_in = [p for p, in_w in zip(closes, in_window) if in_w]\n        prices_out = [p for p, in_w in zip(closes, in_window) if not in_w]\n        \n        vol_in = np.std(np.diff(prices_in)) if len(prices_in) > 1 else 0\n        vol_out = np.std(np.diff(prices_out)) if len(prices_out) > 1 else 0\n        vol_ratio = vol_in / vol_out if vol_out > 0 else np.nan\n        \n        capture_stats.append({\n            'ticker': ticker,\n            'date': t0,\n            'capture_pct': capture_pct,\n            'vol_ratio': vol_ratio\n        })\n        \n        # Graficar\n        ax = axes[idx]\n        \n        # Precios fuera de ventana (gris)\n        out_days = [rd for rd, in_w in zip(rel_days, in_window) if not in_w]\n        out_prices = [p for p, in_w in zip(closes, in_window) if not in_w]\n        ax.plot(out_days, out_prices, 'o-', color='gray', alpha=0.4, label='Fuera ventana')\n        \n        # Precios dentro de ventana (verde/rojo)\n        in_days = [rd for rd, in_w in zip(rel_days, in_window) if in_w]\n        in_prices = [p for p, in_w in zip(closes, in_window) if in_w]\n        ax.plot(in_days, in_prices, 'o-', color='green', linewidth=2, markersize=8, label='Dentro ventana')\n        \n        # Marcar día evento\n        ax.axvline(x=0, color='red', linestyle='--', linewidth=2, alpha=0.7, label='Evento')\n        \n        # Sombrear ventana\n        ax.axvspan(pre, post, alpha=0.15, color='green')\n        \n        # Título con estadísticas\n        ax.set_title(f\"{ticker} ({t0})\\nCaptura: {capture_pct:.1f}% | Vol ratio: {vol_ratio:.2f}x\", \n                     fontsize=10)\n        ax.set_xlabel('Días relativos al evento')\n        ax.set_ylabel('Precio ($)')\n        ax.grid(True, alpha=0.3)\n        ax.legend(fontsize=8)\n    \n    # Ocultar axes vacíos\n    for idx in range(len(subset), len(axes)):\n        axes[idx].axis('off')\n    \n    plt.tight_layout()\n    plt.savefig(f'validation_{event_code}.png', dpi=150, bbox_inches='tight')\n    plt.show()\n    \n    # Estadísticas agregadas\n    if capture_stats:\n        df_stats = pd.DataFrame(capture_stats)\n        print(f\"\\n{'='*70}\")\n        print(f\"ESTADÍSTICAS DE CAPTURA: {event_code}\")\n        print(f\"Ventana sugerida: [{pre:+d}, {post:+d}]\")\n        print(f\"{'='*70}\")\n        print(f\"  Captura de rango (%):\")\n        print(f\"    - Media:   {df_stats['capture_pct'].mean():.1f}%\")\n        print(f\"    - Mediana: {df_stats['capture_pct'].median():.1f}%\")\n        print(f\"    - Min:     {df_stats['capture_pct'].min():.1f}%\")\n        print(f\"    - Max:     {df_stats['capture_pct'].max():.1f}%\")\n        print(f\"\\n  Volatilidad dentro/fuera:\")\n        print(f\"    - Media:   {df_stats['vol_ratio'].mean():.2f}x\")\n        print(f\"    - Mediana: {df_stats['vol_ratio'].median():.2f}x\")\n        print(f\"{'='*70}\")\n        print(f\"\\n✓ Gráfico guardado: validation_{event_code}.png\\n\")\n    \n    return capture_stats\n\n\n# Ejecutar validación para cada evento\nprint(\"\\n\" + \"=\"*80)\nprint(\"VALIDACIÓN VISUAL: Captura de Movimiento Real\")\nprint(\"=\"*80)\n\nall_stats = {}\nfor event, window in suggested_windows.items():\n    stats = visualize_window_capture(event, window, n_examples=6)\n    all_stats[event] = stats\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"✓ Validación visual completada\")\nprint(\"=\"*80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 8. VALIDACIÓN VISUAL: ¿Las ventanas capturan realmente el movimiento?\n\n**CRÍTICO**: Necesitamos VERIFICAR que las ventanas sugeridas realmente capturan el movimiento de precio, no solo confiar en MI.\n\nEste análisis muestra:\n1. Gráficos de precio real con ventana superpuesta\n2. % del movimiento total capturado por la ventana\n3. Comparación de volatilidad dentro vs fuera de ventana\n4. Ejemplos aleatorios para inspección visual",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}