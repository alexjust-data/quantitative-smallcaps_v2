{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fase 1: Information Theory - Validaci√≥n H√≠brida Ventanas\n",
    "\n",
    "**Objetivo**: Calcular Mutual Information entre features diarias y retornos futuros para identificar d√≠as con informaci√≥n predictiva.\n",
    "\n",
    "**M√©todo**: Information Theory (model-agnostic)\n",
    "- Mutual Information I(X_t; y) por d√≠a relativo\n",
    "- Filtrado r√°pido: descarta d√≠as sin se√±al\n",
    "- Solo usa columnas b√°sicas de DIB bars\n",
    "\n",
    "**Output**: `phase1_results.pkl` con info_results por evento\n",
    "\n",
    "**Tiempo estimado**: 10-20 min (con sample_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mutual_info_score\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Config\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "# Paths\n",
    "BARS_ROOT = Path('../../../../processed/dib_bars/pilot50_validation')\n",
    "WATCHLIST = Path('../../../../processed/universe/pilot50_validation/daily')\n",
    "OUTPUT_DIR = Path('.')\n",
    "\n",
    "print(f\"DIB bars dir exists: {BARS_ROOT.exists()}\")\n",
    "print(f\"Watchlist exists: {WATCHLIST.exists()}\")\n",
    "print(f\"Output dir: {OUTPUT_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cargar Watchlist con Eventos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar todos los watchlists particionados por fecha\n",
    "watchlist_files = list(WATCHLIST.rglob('watchlist.parquet'))\n",
    "print(f\"Encontrados {len(watchlist_files):,} watchlist files\")\n",
    "\n",
    "wl_parts = []\n",
    "for wl_file in watchlist_files:\n",
    "    # Extract date from path: date=YYYY-MM-DD/watchlist.parquet\n",
    "    date_str = wl_file.parent.name.split('=')[1]\n",
    "    df = pl.read_parquet(wl_file)\n",
    "    df = df.with_columns([pl.lit(date_str).alias('date')])\n",
    "    wl_parts.append(df)\n",
    "\n",
    "wl = pl.concat(wl_parts)\n",
    "print(f\"Total watchlist rows: {wl.height:,}\")\n",
    "\n",
    "# Convertir date a pl.Date\n",
    "wl = wl.with_columns([\n",
    "    pl.col('date').str.strptime(pl.Date, format='%Y-%m-%d')\n",
    "])\n",
    "\n",
    "# Expandir una fila por evento\n",
    "wl_expanded = wl.explode('events').rename({'events': 'event_code'})\n",
    "print(f\"Total event occurrences: {wl_expanded.height:,}\")\n",
    "\n",
    "# Eventos disponibles\n",
    "events_available = sorted(wl_expanded['event_code'].unique().to_list())\n",
    "print(f\"\\nEventos disponibles: {events_available}\")\n",
    "\n",
    "wl_expanded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Funciones de Informaci√≥n Mutua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dib_bars_day(ticker: str, day: datetime.date) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Carga DIB bars de un ticker en un d√≠a espec√≠fico.\n",
    "    \"\"\"\n",
    "    bars_file = BARS_ROOT / ticker / f\"date={day.isoformat()}\" / \"dollar_imbalance.parquet\"\n",
    "    if not bars_file.exists():\n",
    "        return None\n",
    "    return pl.read_parquet(bars_file)\n",
    "\n",
    "\n",
    "def aggregate_day_features(df_bars: pl.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Agrega features intrad√≠a de DIB bars a features diarias.\n",
    "    Solo usa columnas b√°sicas: o, h, l, c, v, n, dollar, imbalance_score\n",
    "    \"\"\"\n",
    "    if df_bars is None or df_bars.height == 0:\n",
    "        return None\n",
    "    \n",
    "    # Calcular features agregados del d√≠a\n",
    "    agg = df_bars.select([\n",
    "        ((pl.col('c') - pl.col('o')) / pl.col('o')).mean().alias('ret_day'),\n",
    "        ((pl.col('h') - pl.col('l')) / pl.col('o')).mean().alias('range_day'),\n",
    "        pl.col('v').sum().alias('vol_day'),\n",
    "        pl.col('dollar').sum().alias('dollar_day'),\n",
    "        pl.col('imbalance_score').mean().alias('imb_day'),\n",
    "        pl.col('n').sum().alias('n_bars')\n",
    "    ])\n",
    "    \n",
    "    return agg.to_dicts()[0] if agg.height > 0 else None\n",
    "\n",
    "\n",
    "def calculate_mutual_information_discretized(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    bins: int = 10\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calcula mutual information promedio entre features X y target y.\n",
    "    \"\"\"\n",
    "    y_binned = pd.cut(y, bins=bins, labels=False, duplicates='drop')\n",
    "    \n",
    "    mi_scores = []\n",
    "    for col_idx in range(X.shape[1]):\n",
    "        x_col = X[:, col_idx]\n",
    "        x_binned = pd.cut(x_col, bins=bins, labels=False, duplicates='drop')\n",
    "        \n",
    "        valid_mask = ~(pd.isna(x_binned) | pd.isna(y_binned))\n",
    "        if valid_mask.sum() > 10:\n",
    "            mi = mutual_info_score(x_binned[valid_mask], y_binned[valid_mask])\n",
    "            mi_scores.append(mi)\n",
    "    \n",
    "    return np.mean(mi_scores) if mi_scores else 0.0\n",
    "\n",
    "\n",
    "print(\"‚úì Funciones de informaci√≥n mutua definidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calcular MI por D√≠a Relativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_information_by_relative_day(\n",
    "    event_code: str,\n",
    "    max_pre: int = 7,\n",
    "    max_post: int = 7,\n",
    "    sample_size: int = 500\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Para un evento, calcula I(X_t; y) para cada d√≠a t relativo al evento.\n",
    "    \n",
    "    Returns:\n",
    "        {rel_day: mutual_information_score}\n",
    "    \"\"\"\n",
    "    # Filtrar eventos de este tipo\n",
    "    subset = wl_expanded.filter(pl.col('event_code') == event_code)\n",
    "    \n",
    "    # Sample para acelerar (opcional)\n",
    "    if subset.height > sample_size:\n",
    "        subset = subset.sample(sample_size, seed=42)\n",
    "    \n",
    "    print(f\"\\nAnalizando {event_code}: {subset.height} ocurrencias\")\n",
    "    \n",
    "    # Recolectar datos por d√≠a relativo\n",
    "    data_by_day = {}\n",
    "    \n",
    "    for rel_day in range(-max_pre, max_post + 1):\n",
    "        features_list = []\n",
    "        targets_list = []\n",
    "        \n",
    "        for row in subset.iter_rows(named=True):\n",
    "            ticker = row['ticker']\n",
    "            t0 = row['date']\n",
    "            \n",
    "            # D√≠a relativo actual\n",
    "            d = t0 + timedelta(days=rel_day)\n",
    "            bars = load_dib_bars_day(ticker, d)\n",
    "            \n",
    "            if bars is None or bars.height == 0:\n",
    "                continue\n",
    "            \n",
    "            # Features agregados del d√≠a\n",
    "            feat = aggregate_day_features(bars)\n",
    "            if feat is None:\n",
    "                continue\n",
    "            \n",
    "            # Target: retorno futuro desde t0 (d√≠a evento)\n",
    "            # Usamos bars del d√≠a t0+1, t0+2, t0+3 para calcular ret_3d\n",
    "            bars_t0 = load_dib_bars_day(ticker, t0)\n",
    "            bars_t3 = load_dib_bars_day(ticker, t0 + timedelta(days=3))\n",
    "            \n",
    "            if bars_t0 is None or bars_t3 is None:\n",
    "                continue\n",
    "            if bars_t0.height == 0 or bars_t3.height == 0:\n",
    "                continue\n",
    "            \n",
    "            # Calcular retorno 3d\n",
    "            p0 = bars_t0['c'][-1]\n",
    "            p3 = bars_t3['c'][-1]\n",
    "            ret_3d = (p3 - p0) / p0\n",
    "            \n",
    "            features_list.append(list(feat.values()))\n",
    "            targets_list.append(ret_3d)\n",
    "        \n",
    "        if len(features_list) < 50:\n",
    "            data_by_day[rel_day] = 0.0\n",
    "            continue\n",
    "        \n",
    "        X = np.array(features_list)\n",
    "        y = np.array(targets_list)\n",
    "        \n",
    "        # Calcular MI\n",
    "        mi = calculate_mutual_information_discretized(X, y, bins=10)\n",
    "        data_by_day[rel_day] = mi\n",
    "        \n",
    "        print(f\"  t={rel_day:+d}: MI={mi:.4f} (n={len(features_list)})\")\n",
    "    \n",
    "    return data_by_day\n",
    "\n",
    "\n",
    "print(\"‚úì Funci√≥n de an√°lisis por d√≠a relativo definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ejecutar An√°lisis Information Theory\n",
    "\n",
    "**NOTA**: Ajusta `EVENTS_TO_TEST` seg√∫n necesites:\n",
    "- `[:3]` ‚Üí Prueba r√°pida (3 eventos, ~10-15 min)\n",
    "- Sin slice ‚Üí An√°lisis completo (11 eventos, ~40-60 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURACI√ìN: Ajusta aqu√≠ el subset de eventos\n",
    "EVENTS_TO_TEST = events_available[:3]  # Cambiar a events_available para an√°lisis completo\n",
    "MAX_PRE_DAYS = 3\n",
    "MAX_POST_DAYS = 3\n",
    "SAMPLE_SIZE = 200  # Reducir a 100 para m√°s velocidad, aumentar a 500 para m√°s precisi√≥n\n",
    "\n",
    "print(f\"Analizando {len(EVENTS_TO_TEST)} eventos con ventana [{-MAX_PRE_DAYS}, {MAX_POST_DAYS}]\")\n",
    "print(f\"Sample size: {SAMPLE_SIZE} ocurrencias por evento\\n\")\n",
    "\n",
    "info_results = {}\n",
    "\n",
    "for event in EVENTS_TO_TEST:\n",
    "    info_by_day = analyze_information_by_relative_day(\n",
    "        event,\n",
    "        max_pre=MAX_PRE_DAYS,\n",
    "        max_post=MAX_POST_DAYS,\n",
    "        sample_size=SAMPLE_SIZE\n",
    "    )\n",
    "    info_results[event] = info_by_day\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úì An√°lisis Information Theory completado\")\n",
    "print(f\"Eventos analizados: {len(info_results)}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizar Informaci√≥n por D√≠a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(info_results), 1, figsize=(12, 4 * len(info_results)))\n",
    "\n",
    "if len(info_results) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, (event, info_by_day) in enumerate(info_results.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    days = sorted(info_by_day.keys())\n",
    "    mi_scores = [info_by_day[d] for d in days]\n",
    "    \n",
    "    # Normalizar\n",
    "    max_mi = max(mi_scores) if max(mi_scores) > 0 else 1.0\n",
    "    mi_norm = [m / max_mi for m in mi_scores]\n",
    "    \n",
    "    # Plot\n",
    "    ax.bar(days, mi_norm, alpha=0.7, color='steelblue')\n",
    "    ax.axvline(x=0, color='red', linestyle='--', linewidth=2, label='D√≠a Evento (t=0)')\n",
    "    ax.axhline(y=0.1, color='orange', linestyle=':', label='Threshold 10%')\n",
    "    \n",
    "    # Marcar d√≠as significativos\n",
    "    significant_days = [d for d, mi in zip(days, mi_norm) if mi >= 0.1]\n",
    "    if significant_days:\n",
    "        t_start, t_end = min(significant_days), max(significant_days)\n",
    "        ax.axvspan(t_start - 0.5, t_end + 0.5, alpha=0.2, color='green',\n",
    "                   label=f'Ventana sugerida: [{t_start}, {t_end}]')\n",
    "    \n",
    "    ax.set_xlabel('D√≠as Relativos al Evento')\n",
    "    ax.set_ylabel('Mutual Information (normalizado)')\n",
    "    ax.set_title(f'{event}: Informaci√≥n por D√≠a Relativo')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('information_by_day_phase1.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Gr√°fico guardado: information_by_day_phase1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Resumen: Ventanas Sugeridas por MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VENTANAS SUGERIDAS POR MUTUAL INFORMATION (threshold 10%)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "suggested_windows = {}\n",
    "\n",
    "for event, info_by_day in info_results.items():\n",
    "    days = sorted(info_by_day.keys())\n",
    "    mi_scores = [info_by_day[d] for d in days]\n",
    "    max_mi = max(mi_scores) if max(mi_scores) > 0 else 1.0\n",
    "    mi_norm = [m / max_mi for m in mi_scores]\n",
    "    significant_days = [d for d, mi in zip(days, mi_norm) if mi >= 0.1]\n",
    "    \n",
    "    if significant_days:\n",
    "        window = (min(significant_days), max(significant_days))\n",
    "        suggested_windows[event] = window\n",
    "        print(f\"  {event:<25} ‚Üí [{window[0]:+d}, {window[1]:+d}]\")\n",
    "    else:\n",
    "        suggested_windows[event] = None\n",
    "        print(f\"  {event:<25} ‚Üí Sin ventana clara (MI muy bajo)\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Guardar Resultados Fase 1\n",
    "\n",
    "**Output**: `phase1_results.pkl` con todos los datos necesarios para Fase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empaquetar resultados\n",
    "results_phase1 = {\n",
    "    'info_results': info_results,\n",
    "    'wl_expanded': wl_expanded,\n",
    "    'events_available': events_available,\n",
    "    'suggested_windows': suggested_windows,\n",
    "    'config': {\n",
    "        'max_pre_days': MAX_PRE_DAYS,\n",
    "        'max_post_days': MAX_POST_DAYS,\n",
    "        'sample_size': SAMPLE_SIZE,\n",
    "        'events_tested': EVENTS_TO_TEST\n",
    "    }\n",
    "}\n",
    "\n",
    "# Guardar a disco\n",
    "output_file = OUTPUT_DIR / 'phase1_results.pkl'\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(results_phase1, f)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úì FASE 1 COMPLETADA\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Resultados guardados en: {output_file.absolute()}\")\n",
    "print(f\"\\nContenido:\")\n",
    "print(f\"  - info_results: {len(info_results)} eventos con MI por d√≠a relativo\")\n",
    "print(f\"  - wl_expanded: {wl_expanded.height:,} event occurrences\")\n",
    "print(f\"  - events_available: {len(events_available)} eventos totales\")\n",
    "print(f\"  - suggested_windows: {len([w for w in suggested_windows.values() if w])} ventanas sugeridas\")\n",
    "print(f\"\\nPr√≥ximo paso: Ejecutar phase2_model_performance.ipynb\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "source": "def export_events_for_tradingview(event_code: str, output_dir: Path = Path('.')):\n    \"\"\"\n    Exporta eventos con timestamps exactos para visualizaci√≥n en TradingView.\n    \n    Para cada evento crea CSV con:\n    - ticker\n    - datetime (timestamp exacto del primer bar del evento)\n    - close_price (precio en el momento del evento)\n    - event_code\n    - window_suggested (ventana sugerida por MI)\n    \"\"\"\n    # Filtrar eventos de este tipo\n    subset = wl_expanded.filter(pl.col('event_code') == event_code)\n    \n    print(f\"\\nProcesando {event_code}: {subset.height:,} ocurrencias\")\n    \n    events_data = []\n    \n    for idx, row in enumerate(subset.iter_rows(named=True)):\n        ticker = row['ticker']\n        event_date = row['date']\n        \n        # Cargar DIB bars del d√≠a del evento para obtener timestamp exacto\n        bars = load_dib_bars_day(ticker, event_date)\n        \n        if bars is None or bars.height == 0:\n            continue\n        \n        # Timestamp del PRIMER bar del evento (inicio de la sesi√≥n)\n        first_ts = bars['t_open'][0]  # Timestamp de apertura del primer bar\n        close_price = bars['c'][0]  # Precio de cierre del primer bar\n        \n        # Ventana sugerida\n        window = suggested_windows.get(event_code, None)\n        window_str = f\"[{window[0]:+d},{window[1]:+d}]\" if window else \"N/A\"\n        \n        events_data.append({\n            'ticker': ticker,\n            'datetime': first_ts,\n            'close_price': close_price,\n            'event_code': event_code,\n            'window_suggested': window_str,\n            'date': event_date\n        })\n        \n        # Progress cada 100 eventos\n        if (idx + 1) % 100 == 0:\n            print(f\"  Procesados {idx + 1:,} / {subset.height:,} eventos...\")\n    \n    if not events_data:\n        print(f\"‚ö†Ô∏è  No se encontraron datos para {event_code}\")\n        return None\n    \n    # Crear DataFrame\n    df = pd.DataFrame(events_data)\n    \n    # Ordenar por datetime\n    df = df.sort_values('datetime')\n    \n    # Exportar CSV\n    output_file = output_dir / f'tradingview_{event_code}.csv'\n    df.to_csv(output_file, index=False)\n    \n    print(f\"‚úì Exportado: {output_file}\")\n    print(f\"  Total eventos: {len(df):,}\")\n    print(f\"  Tickers √∫nicos: {df['ticker'].nunique()}\")\n    print(f\"  Rango fechas: {df['date'].min()} a {df['date'].max()}\")\n    print(f\"  Ventana sugerida: {window_str}\")\n    \n    return df\n\n\n# Crear directorio para exports\nTRADINGVIEW_DIR = OUTPUT_DIR / 'tradingview_exports'\nTRADINGVIEW_DIR.mkdir(exist_ok=True)\n\nprint(\"=\"*80)\nprint(\"EXPORTANDO EVENTOS PARA TRADINGVIEW\")\nprint(\"=\"*80)\n\nall_exports = {}\n\n# Exportar TODOS los eventos disponibles (no solo los analizados)\nfor event in events_available:\n    df = export_events_for_tradingview(event, TRADINGVIEW_DIR)\n    if df is not None:\n        all_exports[event] = df\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"‚úì EXPORTACI√ìN COMPLETADA\")\nprint(\"=\"*80)\nprint(f\"Directorio: {TRADINGVIEW_DIR.absolute()}\")\nprint(f\"Archivos generados: {len(all_exports)}\")\nprint(f\"\\nEventos exportados:\")\nfor event, df in all_exports.items():\n    print(f\"  ‚Ä¢ {event:<30} ‚Üí {len(df):>6,} ocurrencias\")\nprint(\"=\"*80)\n\nprint(\"\\nüìä C√ìMO USAR EN TRADINGVIEW:\")\nprint(\"1. Abre TradingView con el ticker que quieres analizar\")\nprint(\"2. Importa el CSV correspondiente como 'Custom Indicator'\")\nprint(\"3. Los eventos aparecer√°n como marcadores en el gr√°fico\")\nprint(\"4. Filtra por ticker si el CSV contiene m√∫ltiples s√≠mbolos\")\nprint(\"\\nEjemplo: Para ver E10_FirstGreenBounce en ticker AAPL:\")\nprint(f\"  ‚Üí Carga: {TRADINGVIEW_DIR / 'tradingview_E10_FirstGreenBounce.csv'}\")\nprint(\"  ‚Üí Filtra columna 'ticker' = 'AAPL'\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 10. EXPORTAR EVENTOS PARA TRADINGVIEW\n\n**Objetivo**: Generar CSV con timestamps exactos de TODOS los eventos para visualizaci√≥n en TradingView.\n\n**Formato TradingView**:\n- Necesita timestamp exacto (fecha + hora)\n- Un archivo por evento\n- Columnas: ticker, datetime, event_code, price_at_event\n\nEsto te permitir√°:\n1. Cargar los eventos como overlays en TradingView\n2. Verificar visualmente cada evento sobre el gr√°fico de precio\n3. Validar que la detecci√≥n fue correcta",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 9. INTERPRETACI√ìN: ¬øSon cre√≠bles los resultados?\n\n**Criterios de validaci√≥n:**\n\n1. **Captura de rango > 70%**: La ventana debe contener la mayor parte del movimiento\n2. **Volatilidad ratio > 1.5x**: Dentro de ventana debe haber m√°s volatilidad que fuera\n3. **Consistencia visual**: Los gr√°ficos deben mostrar patrones claros\n\n**Posibles problemas detectados:**\n- Si todas las ventanas son [-3, +3] ‚Üí Threshold demasiado bajo, MI no discrimina\n- Si captura < 50% ‚Üí Ventana no √∫til para trading\n- Si vol_ratio < 1.0 ‚Üí Ventana captura el per√≠odo tranquilo (ERROR)\n\n**Pr√≥ximos pasos si resultados no son cre√≠bles:**\n1. Ajustar threshold de MI (probar 30%, 50% en lugar de 10%)\n2. Usar MI absoluto en lugar de normalizado\n3. Comparar con Phase 2 (LightGBM) para validaci√≥n cruzada",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def get_daily_closes(ticker: str, start_date, end_date):\n    \"\"\"\n    Obtiene precios de cierre diarios para un rango de fechas.\n    Devuelve: list of (date, close_price)\n    \"\"\"\n    prices = []\n    current = start_date\n    \n    while current <= end_date:\n        bars = load_dib_bars_day(ticker, current)\n        if bars is not None and bars.height > 0:\n            close_price = bars['c'][-1]  # √öltimo cierre del d√≠a\n            prices.append((current, close_price))\n        current += timedelta(days=1)\n    \n    return prices\n\n\ndef visualize_window_capture(event_code: str, window: tuple, n_examples: int = 6):\n    \"\"\"\n    Visualiza ejemplos reales de eventos con ventana superpuesta.\n    Calcula % de movimiento capturado.\n    \"\"\"\n    if window is None:\n        print(f\"‚ùå {event_code}: Sin ventana sugerida\")\n        return\n    \n    pre, post = window\n    \n    # Obtener ocurrencias del evento\n    subset = wl_expanded.filter(pl.col('event_code') == event_code)\n    \n    # Tomar muestra aleatoria\n    if subset.height > n_examples:\n        subset = subset.sample(n_examples, seed=123)\n    else:\n        n_examples = subset.height\n    \n    # Configurar grid de subplots\n    n_cols = 3\n    n_rows = (n_examples + n_cols - 1) // n_cols\n    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n    axes = axes.flatten() if n_examples > 1 else [axes]\n    \n    capture_stats = []\n    \n    for idx, row in enumerate(subset.iter_rows(named=True)):\n        if idx >= n_examples:\n            break\n            \n        ticker = row['ticker']\n        t0 = row['date']\n        \n        # Cargar precios desde t-5 hasta t+5 (ventana amplia para contexto)\n        start = t0 - timedelta(days=5)\n        end = t0 + timedelta(days=5)\n        prices = get_daily_closes(ticker, start, end)\n        \n        if len(prices) < 5:  # Muy pocos datos\n            continue\n        \n        # Separar fechas y precios\n        dates = [p[0] for p in prices]\n        closes = [p[1] for p in prices]\n        \n        # Calcular d√≠as relativos al evento\n        rel_days = [(d - t0).days for d in dates]\n        \n        # Identificar puntos dentro de ventana sugerida\n        in_window = [(pre <= rd <= post) for rd in rel_days]\n        \n        # Calcular captura de movimiento\n        all_prices = closes\n        window_prices = [p for p, in_w in zip(closes, in_window) if in_w]\n        \n        if len(window_prices) < 2 or len(all_prices) < 2:\n            continue\n        \n        # Rango total vs rango capturado\n        total_range = max(all_prices) - min(all_prices)\n        window_range = max(window_prices) - min(window_prices)\n        capture_pct = (window_range / total_range * 100) if total_range > 0 else 0\n        \n        # Volatilidad dentro vs fuera\n        prices_in = [p for p, in_w in zip(closes, in_window) if in_w]\n        prices_out = [p for p, in_w in zip(closes, in_window) if not in_w]\n        \n        vol_in = np.std(np.diff(prices_in)) if len(prices_in) > 1 else 0\n        vol_out = np.std(np.diff(prices_out)) if len(prices_out) > 1 else 0\n        vol_ratio = vol_in / vol_out if vol_out > 0 else np.nan\n        \n        capture_stats.append({\n            'ticker': ticker,\n            'date': t0,\n            'capture_pct': capture_pct,\n            'vol_ratio': vol_ratio\n        })\n        \n        # Graficar\n        ax = axes[idx]\n        \n        # Precios fuera de ventana (gris)\n        out_days = [rd for rd, in_w in zip(rel_days, in_window) if not in_w]\n        out_prices = [p for p, in_w in zip(closes, in_window) if not in_w]\n        ax.plot(out_days, out_prices, 'o-', color='gray', alpha=0.4, label='Fuera ventana')\n        \n        # Precios dentro de ventana (verde/rojo)\n        in_days = [rd for rd, in_w in zip(rel_days, in_window) if in_w]\n        in_prices = [p for p, in_w in zip(closes, in_window) if in_w]\n        ax.plot(in_days, in_prices, 'o-', color='green', linewidth=2, markersize=8, label='Dentro ventana')\n        \n        # Marcar d√≠a evento\n        ax.axvline(x=0, color='red', linestyle='--', linewidth=2, alpha=0.7, label='Evento')\n        \n        # Sombrear ventana\n        ax.axvspan(pre, post, alpha=0.15, color='green')\n        \n        # T√≠tulo con estad√≠sticas\n        ax.set_title(f\"{ticker} ({t0})\\nCaptura: {capture_pct:.1f}% | Vol ratio: {vol_ratio:.2f}x\", \n                     fontsize=10)\n        ax.set_xlabel('D√≠as relativos al evento')\n        ax.set_ylabel('Precio ($)')\n        ax.grid(True, alpha=0.3)\n        ax.legend(fontsize=8)\n    \n    # Ocultar axes vac√≠os\n    for idx in range(len(subset), len(axes)):\n        axes[idx].axis('off')\n    \n    plt.tight_layout()\n    plt.savefig(f'validation_{event_code}.png', dpi=150, bbox_inches='tight')\n    plt.show()\n    \n    # Estad√≠sticas agregadas\n    if capture_stats:\n        df_stats = pd.DataFrame(capture_stats)\n        print(f\"\\n{'='*70}\")\n        print(f\"ESTAD√çSTICAS DE CAPTURA: {event_code}\")\n        print(f\"Ventana sugerida: [{pre:+d}, {post:+d}]\")\n        print(f\"{'='*70}\")\n        print(f\"  Captura de rango (%):\")\n        print(f\"    - Media:   {df_stats['capture_pct'].mean():.1f}%\")\n        print(f\"    - Mediana: {df_stats['capture_pct'].median():.1f}%\")\n        print(f\"    - Min:     {df_stats['capture_pct'].min():.1f}%\")\n        print(f\"    - Max:     {df_stats['capture_pct'].max():.1f}%\")\n        print(f\"\\n  Volatilidad dentro/fuera:\")\n        print(f\"    - Media:   {df_stats['vol_ratio'].mean():.2f}x\")\n        print(f\"    - Mediana: {df_stats['vol_ratio'].median():.2f}x\")\n        print(f\"{'='*70}\")\n        print(f\"\\n‚úì Gr√°fico guardado: validation_{event_code}.png\\n\")\n    \n    return capture_stats\n\n\n# Ejecutar validaci√≥n para cada evento\nprint(\"\\n\" + \"=\"*80)\nprint(\"VALIDACI√ìN VISUAL: Captura de Movimiento Real\")\nprint(\"=\"*80)\n\nall_stats = {}\nfor event, window in suggested_windows.items():\n    stats = visualize_window_capture(event, window, n_examples=6)\n    all_stats[event] = stats\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"‚úì Validaci√≥n visual completada\")\nprint(\"=\"*80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 8. VALIDACI√ìN VISUAL: ¬øLas ventanas capturan realmente el movimiento?\n\n**CR√çTICO**: Necesitamos VERIFICAR que las ventanas sugeridas realmente capturan el movimiento de precio, no solo confiar en MI.\n\nEste an√°lisis muestra:\n1. Gr√°ficos de precio real con ventana superpuesta\n2. % del movimiento total capturado por la ventana\n3. Comparaci√≥n de volatilidad dentro vs fuera de ventana\n4. Ejemplos aleatorios para inspecci√≥n visual",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}