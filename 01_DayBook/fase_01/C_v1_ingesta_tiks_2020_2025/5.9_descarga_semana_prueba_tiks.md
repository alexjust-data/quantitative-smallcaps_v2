¡Perfecto! Vamos a por **Opción A** (validación rápida con los 109 tickers info-rich) y te dejo un **script listo** para descargar **ticks (trades)** de Polygon de forma **rápida, estable y reanudable**, con 2 modos:

* **Modo 1 – “months” (12–24 meses recientes)**: baja por **mes** (evita JSONs gigantes), ideal para validar rápido.
* **Modo 2 – “watchlists” (sólo días info-rich)**: baja **únicamente** los días donde hubo señal (ahorra muchísimo storage/cpu).

Abajo tienes el script `download_trades_optimized.py` y ejemplos de ejecución.

---

# Script: `download_trades_optimized.py`

* **Optimizado**: mensualización, `PAGE_LIMIT=50_000`, rate-limit adaptativo, sesión HTTP con **keep-alive**, compresión **ZSTD**, **_SUCCESS** por partición y **resume** automático.
* **Robusto**: SSL con `certifi` (Windows), backoff con distinción de errores (SSL vs. memoria vs. 429), timeouts claros.
* **Salida**:

  * **Modo months**: `raw/polygon/trades/{ticker}/year=YYYY/month=MM/trades.parquet`
  * **Modo watchlists**: `raw/polygon/trades/{ticker}/date=YYYY-MM-DD/trades.parquet`

> Pega esto como `scripts/fase_C_ingesta_tiks/download_trades_optimized.py`.

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
download_trades_optimized.py
Descarga trades (ticks) de Polygon con:
- Modo 1 (months): por meses completos, ideal 12–24 meses de validación
- Modo 2 (watchlists): sólo días info-rich (máxima eficiencia)
Optimizado: PAGE_LIMIT=50k, keep-alive, ZSTD, _SUCCESS, resume, backoff diferenciando errores.

Uso (months):
  python download_trades_optimized.py \
    --tickers-csv processed/universe/info_rich/info_rich_tickers_20251015_20251021.csv \
    --outdir raw/polygon/trades \
    --from 2024-01-01 --to 2025-10-21 \
    --mode months --page-limit 50000 --rate-limit 0.15 --workers 8 --resume

Uso (watchlists):
  python download_trades_optimized.py \
    --tickers-csv processed/universe/info_rich/info_rich_tickers_20251015_20251021.csv \
    --watchlist-root processed/universe/info_rich/daily \
    --outdir raw/polygon/trades \
    --from 2024-01-01 --to 2025-10-21 \
    --mode watchlists --page-limit 50000 --rate-limit 0.15 --workers 8 --resume
"""

import os, sys, time, math, argparse, itertools, json
from pathlib import Path
from datetime import datetime, timedelta, date
from typing import List, Dict, Tuple, Optional

import polars as pl
import requests
import certifi

API_BASE = "https://api.polygon.io"
PAGE_LIMIT_DEFAULT = 50_000
TIMEOUT = (10, 60)  # connect, read

def log(msg: str):
    print(f"[{datetime.now():%Y-%m-%d %H:%M:%S}] {msg}", flush=True)

def ensure_ssl_env():
    # Hereda o fija SSL_CERT_FILE para Windows
    os.environ.setdefault("SSL_CERT_FILE", certifi.where())

def build_session() -> requests.Session:
    s = requests.Session()
    adapter = requests.adapters.HTTPAdapter(pool_connections=2, pool_maxsize=3, max_retries=0)
    s.mount("https://", adapter)
    # Evita gzip si te daba "Unable to allocate output buffer" (descomenta para forzar sin compresión)
    # s.headers.update({"Accept-Encoding": "identity"})
    return s

def month_iter(dfrom: date, dto: date) -> List[Tuple[date, date]]:
    # Devuelve [(primer_dia_mes, primer_dia_mes_siguiente), ...] intersectado con [dfrom, dto]
    cur = date(dfrom.year, dfrom.month, 1)
    res = []
    while cur <= dto:
        if cur.month == 12:
            nxt = date(cur.year + 1, 1, 1)
        else:
            nxt = date(cur.year, cur.month + 1, 1)
        start = max(cur, dfrom)
        end = min(dto + timedelta(days=1), nxt)  # end exclusive
        if start < end:
            res.append((start, end))
        cur = nxt
    return res

def load_tickers(csv_path: Path) -> List[str]:
    df = pl.read_csv(csv_path)
    col = "ticker" if "ticker" in df.columns else df.columns[0]
    return df[col].unique().to_list()

def load_info_rich_days(watchlist_root: Path, dfrom: date, dto: date, allowed_tickers: Optional[set]) -> Dict[str, List[date]]:
    # Lee todos los watchlist.parquet en [dfrom, dto] y devuelve {ticker: [dates...]} para info_rich True
    out: Dict[str, List[date]] = {}
    # paths tipo: processed/universe/info_rich/daily/date=YYYY-MM-DD/watchlist.parquet
    for day in (dfrom + timedelta(n) for n in range((dto - dfrom).days + 1)):
        p = watchlist_root / f"date={day.isoformat()}" / "watchlist.parquet"
        if not p.exists():
            continue
        df = pl.read_parquet(p)
        if "info_rich" not in df.columns:
            continue
        sub = df.filter(pl.col("info_rich") == True).select(["ticker"])
        if allowed_tickers:
            sub = sub.filter(pl.col("ticker").is_in(list(allowed_tickers)))
        for t in sub["ticker"].to_list():
            out.setdefault(t, []).append(day)
    return out

def success_marker(path: Path):
    (path / "_SUCCESS").touch(exist_ok=True)

def exists_success(path: Path) -> bool:
    return (path / "_SUCCESS").exists()

def http_get_trades(session: requests.Session, ticker: str, t_from_iso: str, t_to_iso: str,
                    page_limit: int, api_key: str, cursor: Optional[str] = None) -> dict:
    # v3 trades con timestamp range; usa paginación por cursor
    url = f"{API_BASE}/v3/trades/{ticker}"
    params = {
        "limit": page_limit,
        "sort": "asc",
        "timestamp.gte": f"{t_from_iso}T00:00:00Z",
        "timestamp.lt":  f"{t_to_iso}T00:00:00Z",
        "apiKey": api_key,
    }
    headers = {}
    if cursor:
        # Polygon permite cursor en query o next_url completo; preferimos query limpia
        params["cursor"] = cursor
    r = session.get(url, params=params, headers=headers, timeout=TIMEOUT)
    r.raise_for_status()
    return r.json()

def to_parquet_trades(out_parquet: Path, records: List[dict]):
    if not records:
        # crea parquet vacío con esquema mínimo
        schema = {
            "tx": pl.Utf8, "t": pl.Datetime, "p": pl.Float64, "s": pl.Int64, "c": pl.List(pl.Utf8)
        }
        pl.DataFrame(schema=schema).write_parquet(out_parquet, compression="zstd", compression_level=2, statistics=False)
        return
    # Campos típicos v3: sip_timestamp (t), price (p), size (s), conditions (c), exchange (x)...
    df = pl.from_records(records)
    # Normalización de columnas
    cols = df.columns
    mapping = {}
    if "sip_timestamp" in cols: mapping["sip_timestamp"] = "t"
    if "price" in cols:         mapping["price"]         = "p"
    if "size" in cols:          mapping["size"]          = "s"
    if "conditions" in cols:    mapping["conditions"]    = "c"
    if mapping:
        df = df.rename(mapping)
    # Tipos
    if "t" in df.columns:
        df = df.with_columns(pl.col("t").cast(pl.Datetime(time_unit="us")))  # polygon devuelve µs/ns; ajusta si ves desajuste
    # Escribe
    out_parquet.parent.mkdir(parents=True, exist_ok=True)
    df.write_parquet(out_parquet, compression="zstd", compression_level=2, statistics=False)

def backoff_sleep(k: int, kind: str, base: float) -> float:
    # Diferencia SSL / memoria / 429
    if kind == "ssl":
        return min(5.0, base)  # rápido para SSL intermitente
    if kind == "mem":
        return min(60.0, base * 2.0 + 2.0)  # más pausado
    if kind == "429":
        return min(60.0, base * 2.0 + 5.0)
    return min(30.0, base)

def download_span(session: requests.Session, ticker: str, span_from: date, span_to: date,
                  outdir: Path, page_limit: int, api_key: str, rate_limit: float,
                  layout: str, resume: bool):
    """
    layout:
     - "months": escribe a year=YYYY/month=MM
     - "watchlists": escribe a date=YYYY-MM-DD (día a día)
    """
    if layout == "months":
        # un span puede ser un mes exacto; escribe un parquet por mes
        y, m = span_from.year, span_from.month
        month_path = outdir / ticker / f"year={y:04d}" / f"month={m:02d}"
        if resume and exists_success(month_path):
            log(f"{ticker} {y}-{m:02d}: resume skip (_SUCCESS)")
            return
        records: List[dict] = []
        cursor = None
        base_sleep = rate_limit
        try:
            while True:
                try:
                    data = http_get_trades(session, ticker, span_from.isoformat(), span_to.isoformat(),
                                           page_limit, api_key, cursor)
                except requests.HTTPError as e:
                    code = e.response.status_code if e.response is not None else 0
                    if code == 429:
                        sl = backoff_sleep(0, "429", base_sleep)
                        log(f"{ticker} {y}-{m:02d}: 429 Too Many Requests -> sleep {sl}s")
                        time.sleep(sl)
                        continue
                    raise
                except requests.exceptions.SSLError:
                    sl = backoff_sleep(0, "ssl", base_sleep)
                    log(f"{ticker} {y}-{m:02d}: SSL error -> sleep {sl}s")
                    time.sleep(sl); continue
                except requests.exceptions.RequestException as e:
                    sl = backoff_sleep(0, "net", base_sleep)
                    log(f"{ticker} {y}-{m:02d}: NET {e} -> sleep {sl}s")
                    time.sleep(sl); continue

                r = data.get("results", [])
                if r:
                    records.extend(r)
                cursor = data.get("next_url") or data.get("nextUrl") or data.get("next_url".upper())
                # Polygon suele devolver next_url completo; extrae cursor si procede
                if cursor and "cursor=" in cursor:
                    cursor = cursor.split("cursor=")[-1]
                else:
                    cursor = data.get("next_page_token") or None

                time.sleep(rate_limit)
                if not cursor:
                    break

            out_parquet = month_path / "trades.parquet"
            to_parquet_trades(out_parquet, records)
            success_marker(month_path)
            log(f"{ticker} {y}-{m:02d}: OK ({len(records):,} trades)")

        except Exception as e:
            log(f"{ticker} {y}-{m:02d}: ERROR {e}")

    else:
        # layout watchlists: un parquet por día (date=YYYY-MM-DD)
        d = span_from
        day_path = outdir / ticker / f"date={d.isoformat()}"
        if resume and exists_success(day_path):
            log(f"{ticker} {d}: resume skip (_SUCCESS)")
            return
        records: List[dict] = []
        cursor = None
        base_sleep = rate_limit
        try:
            while True:
                try:
                    data = http_get_trades(session, ticker, d.isoformat(), (d + timedelta(days=1)).isoformat(),
                                           page_limit, api_key, cursor)
                except requests.HTTPError as e:
                    code = e.response.status_code if e.response is not None else 0
                    if code == 429:
                        sl = backoff_sleep(0, "429", base_sleep)
                        log(f"{ticker} {d}: 429 Too Many Requests -> sleep {sl}s")
                        time.sleep(sl); continue
                    raise
                except requests.exceptions.SSLError:
                    sl = backoff_sleep(0, "ssl", base_sleep)
                    log(f"{ticker} {d}: SSL error -> sleep {sl}s")
                    time.sleep(sl); continue
                except requests.exceptions.RequestException as e:
                    sl = backoff_sleep(0, "net", base_sleep)
                    log(f"{ticker} {d}: NET {e} -> sleep {sl}s")
                    time.sleep(sl); continue

                r = data.get("results", [])
                if r:
                    records.extend(r)

                cursor = data.get("next_url") or data.get("nextUrl") or data.get("next_url".upper())
                if cursor and "cursor=" in cursor:
                    cursor = cursor.split("cursor=")[-1]
                else:
                    cursor = data.get("next_page_token") or None

                time.sleep(rate_limit)
                if not cursor:
                    break

            out_parquet = day_path / "trades.parquet"
            to_parquet_trades(out_parquet, records)
            success_marker(day_path)
            log(f"{ticker} {d}: OK ({len(records):,} trades)")

        except Exception as e:
            log(f"{ticker} {d}: ERROR {e}")

def parse_args():
    ap = argparse.ArgumentParser(description="Descarga optimizada de trades Polygon")
    ap.add_argument("--tickers-csv", required=True, help="CSV con columna 'ticker'")
    ap.add_argument("--outdir", required=True, help="Directorio de salida (raw/polygon/trades)")
    ap.add_argument("--from", dest="date_from", required=True, help="YYYY-MM-DD")
    ap.add_argument("--to", dest="date_to", required=True, help="YYYY-MM-DD")
    ap.add_argument("--mode", choices=["months","watchlists"], default="months")
    ap.add_argument("--watchlist-root", default="processed/universe/info_rich/daily", help="Necesario en modo watchlists")
    ap.add_argument("--page-limit", type=int, default=PAGE_LIMIT_DEFAULT)
    ap.add_argument("--rate-limit", type=float, default=0.15)
    ap.add_argument("--workers", type=int, default=8, help="Procesos concurrentes")
    ap.add_argument("--resume", action="store_true")
    return ap.parse_args()

def chunked(lst: List[str], size: int) -> List[List[str]]:
    return [lst[i:i+size] for i in range(0, len(lst), size)]

def main():
    ensure_ssl_env()
    args = parse_args()
    api_key = os.getenv("POLYGON_API_KEY")
    if not api_key:
        sys.exit("ERROR: falta POLYGON_API_KEY en el entorno")

    outdir = Path(args.outdir); outdir.mkdir(parents=True, exist_ok=True)
    tickers = load_tickers(Path(args.tickers_csv))
    dfrom = datetime.strptime(args.date_from, "%Y-%m-%d").date()
    dto   = datetime.strptime(args.date_to, "%Y-%m-%d").date()

    # Construye tareas (ticker × spans)
    tasks: List[Tuple[str, date, date, str]] = []
    if args.mode == "months":
        spans = month_iter(dfrom, dto)
        for t in tickers:
            for (a,b) in spans:
                tasks.append((t, a, b, "months"))
    else:
        # watchlists: sólo días info-rich
        days_by_ticker = load_info_rich_days(Path(args.watchlist_root), dfrom, dto, set(tickers))
        for t, days in days_by_ticker.items():
            for d in days:
                tasks.append((t, d, d, "watchlists"))
        log(f"Tareas (watchlists): {sum(len(v) for v in days_by_ticker.values()):,} días info-rich en {len(days_by_ticker)} tickers")

    # Ejecuta en micro-batches (evitar procesos zombis / fuga RAM)
    BATCH = 20  # 20 tareas por micro-batch; ajusta si ves RAM alta
    rate_limit = args.rate_limit
    page_limit = args.page_limit
    resume = args.resume

    from concurrent.futures import ProcessPoolExecutor, as_completed

    def run_batch(batch_tasks: List[Tuple[str,date,date,str]]):
        # Un proceso → 1 sesión compartida (keep-alive)
        session = build_session()
        for (t, a, b, layout) in batch_tasks:
            download_span(session, t, a, b, outdir, page_limit, api_key, rate_limit, layout, resume)

    log(f"Tickers: {len(tickers):,} | Tareas: {len(tasks):,} | Workers: {args.workers} | Mode: {args.mode}")
    batches = chunked(tasks, BATCH)
    started = time.time()
    with ProcessPoolExecutor(max_workers=args.workers) as ex:
        futs = [ex.submit(run_batch, bt) for bt in batches]
        done = 0
        for f in as_completed(futs):
            done += 1
            log(f"Progreso: {done}/{len(batches)} batches ({done/len(batches)*100:.1f}%)")
    log(f"FIN. Elapsed: {(time.time()-started)/60:.1f} min")

if __name__ == "__main__":
    main()
```

---

## Cómo lanzarlo (con tus 109 tickers)

### A) **12–24 meses recientes** (validación rápida)

```bash
# 12 meses
python scripts/fase_C_ingesta_tiks/download_trades_optimized.py ^
  --tickers-csv processed/universe/info_rich/info_rich_tickers_20251015_20251021.csv ^
  --outdir raw/polygon/trades ^
  --from 2024-10-01 --to 2025-10-21 ^
  --mode months --page-limit 50000 --rate-limit 0.15 --workers 8 --resume

# 24 meses (si lo ves fluido)
python scripts/fase_C_ingesta_tiks/download_trades_optimized.py ^
  --tickers-csv processed/universe/info_rich/info_rich_tickers_20251015_20251021.csv ^
  --outdir raw/polygon/trades ^
  --from 2023-10-01 --to 2025-10-21 ^
  --mode months --page-limit 50000 --rate-limit 0.18 --workers 8 --resume
```

### B) **Sólo días info-rich** (máxima eficiencia)

```bash
python scripts/fase_C_ingesta_tiks/download_trades_optimized.py ^
  --tickers-csv processed/universe/info_rich/info_rich_tickers_20251015_20251021.csv ^
  --watchlist-root processed/universe/info_rich/daily ^
  --outdir raw/polygon/trades ^
  --from 2024-01-01 --to 2025-10-21 ^
  --mode watchlists --page-limit 50000 --rate-limit 0.15 --workers 8 --resume
```

> **Cuándo usar cada uno**
>
> * **Months (12–24m)**: validar rápido, construir barras y etiquetar en regímenes recientes.
> * **Watchlists**: cuando vayas a escalar y no quieras “relleno”. Es ideal para **DIB/VIB** + **triple barrier** solo en sesiones con señal.

---

## Notas de estabilidad/rendimiento

* Si ves **SSL**: exporta `SSL_CERT_FILE` al `certifi.where()`. El script ya intenta heredarlo.
* Si aparece “**Unable to allocate output buffer**”:

  * Sube el `--rate-limit` (0.18–0.25).
  * (Opcional) Descomprime en claro (descomenta `Accept-Encoding: identity`).
* **Workers**: 8 suele ser buen “punto dulce” con 16–32 GB RAM. Ajusta si hace falta.
* **Resume**: `_SUCCESS` por partición evita rehacer lo ya descargado.

---

## ¿Por qué 12–24m y no 5 años?

* **Costo/tiempo**: 5 años × 109 tickers = *muchísimo* (TBs y días).
* **Alpha actual**: para tus setups intradía (VWAP reclaim, LDF, OGD, FRD), el regimen reciente pesa más; **12–24m** suelen bastar para ajustar **umbrales + labeling + pesos**.
* Si una **familia de setups** mejora materialmente con microestructura profunda, **extiendes hacia atrás**… **solo para esos tickers** (o **sólo** para sus **días info-rich**). Es la “descarga **inteligente**” que buscábamos.

¿Te va bien así? Si quieres, te paso un mini-launcher `.ps1` para este script, igual al patrón que ya usas, con variables al principio y logs/pids.
