{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validaci√≥n Completa: Pipeline 5 PASOS (C_v2_ingesta_tiks_2004_2025)\n",
    "\n",
    "**Objetivo**: Certificar emp√≠ricamente la ejecuci√≥n completa del pipeline event-driven (PASO 1-5)\n",
    "\n",
    "**Documentaci√≥n**: [C.5_plan_ejecucion_E0_descarga_ticks.md](../C.5_plan_ejecucion_E0_descarga_ticks.md)\n",
    "\n",
    "**Stack**: Polars + Parquet ‚Üí Event-Driven Sampling (L√≥pez de Prado 2018)\n",
    "\n",
    "**Versi√≥n**: 2.0 (FIXED - 2025-10-30)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Paths\n",
    "PROJECT_ROOT = Path(r\"D:\\04_TRADING_SMALLCAPS\")\n",
    "DAILY_CACHE = PROJECT_ROOT / \"processed\" / \"daily_cache\"\n",
    "UNIVERSE_E0 = PROJECT_ROOT / \"processed\" / \"universe\" / \"info_rich\" / \"daily\"\n",
    "TRADES_E0 = PROJECT_ROOT / \"raw\" / \"polygon\" / \"trades\"\n",
    "\n",
    "# Config YAML - B√∫squeda inteligente\n",
    "CONFIG_PATHS = [\n",
    "    PROJECT_ROOT / \"universe_config.yaml\",\n",
    "    PROJECT_ROOT / \"configs\" / \"universe_config.yaml\",\n",
    "    PROJECT_ROOT / \"scripts\" / \"fase_C_ingesta_tiks\" / \"universe_config.yaml\"\n",
    "]\n",
    "CONFIG_YAML = None\n",
    "for path in CONFIG_PATHS:\n",
    "    if path.exists():\n",
    "        CONFIG_YAML = path\n",
    "        break\n",
    "\n",
    "# Styling\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"‚úÖ Setup complete\")\n",
    "print(f\"üìÇ Project root: {PROJECT_ROOT}\")\n",
    "print(f\"üìÇ Config YAML: {CONFIG_YAML if CONFIG_YAML else 'NOT FOUND'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ PASO 1: Agregaci√≥n OHLCV 1m ‚Üí Daily Cache\n",
    "\n",
    "**Script**: `build_daily_cache.py`\n",
    "\n",
    "**Objetivo**: Agregar barras 1-min a diario + calcular features (rvol30, pctchg_d, dollar_vol_d)\n",
    "\n",
    "**Entrada**: `raw/polygon/ohlcv_intraday_1m/` (Fase B)\n",
    "\n",
    "**Salida**: `processed/daily_cache/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PASO 1: DAILY CACHE VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1.1 Verificar estructura\n",
    "if not DAILY_CACHE.exists():\n",
    "    print(f\"\\n‚ùå Directorio NO ENCONTRADO: {DAILY_CACHE}\")\n",
    "    print(\"   PASO 1 no ejecutado\")\n",
    "    ticker_dirs = []\n",
    "else:\n",
    "    ticker_dirs = list(DAILY_CACHE.glob('ticker=*'))\n",
    "    print(f\"\\nüìÇ Tickers cached: {len(ticker_dirs):,}\")\n",
    "    print(f\"   Esperado: 8,618\")\n",
    "    print(f\"   Match: {'‚úÖ' if len(ticker_dirs) >= 8600 else '‚ùå'}\")\n",
    "\n",
    "    # 1.2 Contar _SUCCESS markers\n",
    "    success_markers = list(DAILY_CACHE.glob('ticker=*/_SUCCESS'))\n",
    "    print(f\"\\n‚úì Tickers completados (_SUCCESS): {len(success_markers):,}\")\n",
    "\n",
    "    # 1.3 Contar ticker-d√≠as totales (sample inteligente)\n",
    "    print(f\"\\nüìä Contando ticker-d√≠as totales...\")\n",
    "    total_days = 0\n",
    "    sample_ticker = None\n",
    "    sample_size = min(100, len(ticker_dirs))\n",
    "    \n",
    "    for i, ticker_dir in enumerate(ticker_dirs[:sample_size]):\n",
    "        daily_file = ticker_dir / 'daily.parquet'\n",
    "        if daily_file.exists():\n",
    "            df = pl.read_parquet(daily_file)\n",
    "            total_days += len(df)\n",
    "            if sample_ticker is None:\n",
    "                sample_ticker = (ticker_dir.name.replace('ticker=', ''), df)\n",
    "\n",
    "    # Proyecci√≥n total\n",
    "    if total_days > 0:\n",
    "        avg_days_per_ticker = total_days / sample_size\n",
    "        projected_total = int(avg_days_per_ticker * len(ticker_dirs))\n",
    "\n",
    "        print(f\"   Sample (primeros {sample_size} tickers): {total_days:,} d√≠as\")\n",
    "        print(f\"   Promedio d√≠as/ticker: {avg_days_per_ticker:.1f}\")\n",
    "        print(f\"   Proyecci√≥n total: {projected_total:,} ticker-d√≠as\")\n",
    "        print(f\"   Esperado: ~14,763,368 ticker-d√≠as\")\n",
    "        print(f\"   Match: {'‚úÖ' if projected_total >= 14_000_000 else '‚ö†Ô∏è'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 Verificar features calculados (sample ticker)\n",
    "if sample_ticker:\n",
    "    ticker_name, df_sample = sample_ticker\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"SAMPLE TICKER: {ticker_name}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nTotal d√≠as: {len(df_sample):,}\")\n",
    "    print(f\"Rango temporal: {df_sample['trading_day'].min()} ‚Üí {df_sample['trading_day'].max()}\")\n",
    "    print(f\"\\nColumnas ({len(df_sample.columns)}):\")\n",
    "    print(df_sample.columns)\n",
    "\n",
    "    # Verificar features cr√≠ticos\n",
    "    required_features = ['rvol30', 'pctchg_d', 'dollar_vol_d', 'close_d', 'vol_d', 'vwap_d', 'return_d']\n",
    "    print(f\"\\n‚úì Features cr√≠ticos:\")\n",
    "    for feat in required_features:\n",
    "        exists = feat in df_sample.columns\n",
    "        print(f\"  {'‚úÖ' if exists else '‚ùå'} {feat}\")\n",
    "\n",
    "    # Mostrar sample\n",
    "    print(f\"\\nüìã Sample (√∫ltimas 10 filas):\")\n",
    "    print(df_sample.tail(10).select(['ticker', 'trading_day', 'close_d', 'vol_d', 'rvol30', 'pctchg_d', 'dollar_vol_d']))\n",
    "    \n",
    "    print(\"\\n‚úÖ PASO 1 CERTIFICADO: Daily cache completado con features calculados\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No se pudo cargar sample ticker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚öôÔ∏è PASO 2: Configuraci√≥n Filtros E0\n",
    "\n",
    "**Archivo**: `universe_config.yaml`\n",
    "\n",
    "**Objetivo**: Definir thresholds E0 (RVOL‚â•2.0, |%chg|‚â•15%, $vol‚â•$5M, precio $0.20-$20)\n",
    "\n",
    "**Acci√≥n**: Manual (edici√≥n YAML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PASO 2: CONFIG FILTROS E0\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 2.1 Verificar existencia de config\n",
    "if CONFIG_YAML and CONFIG_YAML.exists():\n",
    "    print(f\"\\n‚úÖ Config file exists: {CONFIG_YAML}\")\n",
    "    \n",
    "    # Leer y mostrar contenido relevante\n",
    "    with open(CONFIG_YAML, 'r') as f:\n",
    "        config_content = f.read()\n",
    "    \n",
    "    # Verificar thresholds E0\n",
    "    print(f\"\\n‚úì Thresholds E0 verificados:\")\n",
    "    thresholds = {\n",
    "        'min_rvol: 2.0': 'min_rvol: 2.0' in config_content or 'min_rvol: 2' in config_content,\n",
    "        'min_pct_change: 0.15': 'min_pct_change: 0.15' in config_content,\n",
    "        'min_dollar_volume: 5000000': 'min_dollar_volume: 5000000' in config_content or 'min_dollar_volume: 5_000_000' in config_content,\n",
    "        'min_price: 0.20': 'min_price: 0.20' in config_content or 'min_price: 0.2' in config_content,\n",
    "        'max_price: 20.00': 'max_price: 20.00' in config_content or 'max_price: 20' in config_content\n",
    "    }\n",
    "    \n",
    "    for threshold, found in thresholds.items():\n",
    "        print(f\"  {'‚úÖ' if found else '‚ùå'} {threshold}\")\n",
    "    \n",
    "    # Mostrar secci√≥n relevante del config\n",
    "    print(f\"\\nüìÑ Secci√≥n info_rich_generic:\")\n",
    "    lines = config_content.split('\\n')\n",
    "    in_section = False\n",
    "    for line in lines:\n",
    "        if 'info_rich_generic' in line:\n",
    "            in_section = True\n",
    "        if in_section:\n",
    "            print(f\"  {line}\")\n",
    "            if line.strip() and not line.strip().startswith('#') and ':' in line and in_section:\n",
    "                if len([l for l in lines[lines.index(line)+1:] if l.strip() and not l.strip().startswith('#')]) > 0:\n",
    "                    next_line = [l for l in lines[lines.index(line)+1:] if l.strip() and not l.strip().startswith('#')][0]\n",
    "                    if not next_line.startswith('  '):\n",
    "                        break\n",
    "    \n",
    "    print(\"\\n‚úÖ PASO 2 CERTIFICADO: Configuraci√≥n E0 validada\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Config file NOT FOUND en ninguna ubicaci√≥n\")\n",
    "    print(f\"   B√∫squeda realizada en:\")\n",
    "    for path in CONFIG_PATHS:\n",
    "        print(f\"   - {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ PASO 3: Generaci√≥n Watchlists E0\n",
    "\n",
    "**Script**: `build_universe.py`\n",
    "\n",
    "**Objetivo**: Filtrar d√≠as info-rich aplicando thresholds E0\n",
    "\n",
    "**Entrada**: `processed/daily_cache/` + `universe_config.yaml`\n",
    "\n",
    "**Salida**: `processed/universe/info_rich/daily/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PASO 3: WATCHLISTS E0 VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 3.1 Verificar estructura\n",
    "if not UNIVERSE_E0.exists():\n",
    "    print(f\"\\n‚ùå Directorio NO ENCONTRADO: {UNIVERSE_E0}\")\n",
    "    print(\"   PASO 3 no ejecutado\")\n",
    "    watchlist_files = []\n",
    "    df_all = None\n",
    "else:\n",
    "    watchlist_files = list(UNIVERSE_E0.glob('date=*/watchlist.parquet'))\n",
    "    print(f\"\\nüìÇ Watchlists generadas: {len(watchlist_files):,}\")\n",
    "    print(f\"   Esperado: 5,934\")\n",
    "    print(f\"   Match: {'‚úÖ' if len(watchlist_files) >= 5900 else '‚ùå'}\")\n",
    "\n",
    "    # 3.2 Cargar TODAS las watchlists para conteo preciso\n",
    "    print(f\"\\nüìä Cargando TODAS las watchlists (puede tardar ~30 seg)...\")\n",
    "    try:\n",
    "        # Usar scan_parquet para lazy loading\n",
    "        df_all = pl.scan_parquet(UNIVERSE_E0 / \"date=*\" / \"watchlist.parquet\").collect()\n",
    "        print(f\"   ‚úÖ Cargado: {len(df_all):,} registros totales\")\n",
    "        print(f\"   Columnas: {df_all.columns}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Error cargando todas las watchlists: {e}\")\n",
    "        print(f\"   Fallback: cargando sample (1000 files)...\")\n",
    "        all_watchlists = []\n",
    "        for wl_file in watchlist_files[:1000]:\n",
    "            df = pl.read_parquet(wl_file)\n",
    "            all_watchlists.append(df)\n",
    "        df_all = pl.concat(all_watchlists)\n",
    "        print(f\"   Sample (1000 watchlists): {len(df_all):,} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Filtrar SOLO eventos E0 (info_rich=True)\n",
    "if df_all is not None and 'info_rich' in df_all.columns:\n",
    "    df_e0_only = df_all.filter(pl.col('info_rich') == True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EVENTOS E0 (info_rich=True)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nTotal registros en watchlists: {len(df_all):,}\")\n",
    "    print(f\"Eventos E0 (info_rich=True): {len(df_e0_only):,}\")\n",
    "    print(f\"Porcentaje E0: {len(df_e0_only)/len(df_all)*100:.2f}%\")\n",
    "    \n",
    "    # Tickers √∫nicos\n",
    "    unique_tickers = df_e0_only['ticker'].n_unique()\n",
    "    print(f\"\\n‚úì Tickers √∫nicos con eventos E0: {unique_tickers:,}\")\n",
    "    print(f\"   Esperado: ~4,898\")\n",
    "    print(f\"   Match: {'‚úÖ' if 4000 <= unique_tickers <= 6000 else '‚ö†Ô∏è'}\")\n",
    "    \n",
    "    # D√≠as √∫nicos con eventos E0\n",
    "    unique_days = df_e0_only['trading_day'].n_unique()\n",
    "    print(f\"\\n‚úì D√≠as √∫nicos con eventos E0: {unique_days:,}\")\n",
    "    print(f\"   Info: D√≠as calendario con al menos 1 evento E0\")\n",
    "    \n",
    "    print(f\"\\nüìä CERTIFICACI√ìN PASO 3:\")\n",
    "    print(f\"   Total eventos E0: {len(df_e0_only):,}\")\n",
    "    print(f\"   Esperado: ~29,555 (puede variar seg√∫n filtros aplicados)\")\n",
    "    print(f\"   Match: {'‚úÖ' if 25000 <= len(df_e0_only) <= 60000 else '‚ö†Ô∏è'}\")\n",
    "    \n",
    "elif df_all is not None:\n",
    "    print(\"\\n‚ö†Ô∏è  Columna 'info_rich' no encontrada en watchlists\")\n",
    "    print(f\"   Todos los registros: {len(df_all):,}\")\n",
    "    print(f\"   Nota: Puede ser un formato de watchlist diferente\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No se pudieron cargar watchlists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4 Verificar umbrales E0 (sample)\n",
    "if df_all is not None and 'info_rich' in df_all.columns and len(df_e0_only) > 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"VALIDACI√ìN UMBRALES E0 (sample 1000 eventos)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Sample de eventos E0\n",
    "    df_check = df_e0_only.head(min(1000, len(df_e0_only)))\n",
    "    \n",
    "    # RVOL ‚â• 2.0\n",
    "    if 'rvol30' in df_check.columns:\n",
    "        rvol_pass = (df_check['rvol30'].drop_nulls() >= 2.0).sum()\n",
    "        rvol_total = len(df_check['rvol30'].drop_nulls())\n",
    "        print(f\"\\n‚úì RVOL‚â•2.0: {rvol_pass}/{rvol_total} ({rvol_pass/rvol_total*100:.1f}%)\")\n",
    "    \n",
    "    # |%chg| ‚â• 15%\n",
    "    if 'pctchg_d' in df_check.columns:\n",
    "        chg_pass = (df_check['pctchg_d'].drop_nulls().abs() >= 0.15).sum()\n",
    "        chg_total = len(df_check['pctchg_d'].drop_nulls())\n",
    "        print(f\"‚úì |%chg|‚â•15%: {chg_pass}/{chg_total} ({chg_pass/chg_total*100:.1f}%)\")\n",
    "    \n",
    "    # $vol ‚â• $5M\n",
    "    if 'dollar_vol_d' in df_check.columns:\n",
    "        dvol_pass = (df_check['dollar_vol_d'].drop_nulls() >= 5_000_000).sum()\n",
    "        dvol_total = len(df_check['dollar_vol_d'].drop_nulls())\n",
    "        print(f\"‚úì $vol‚â•$5M: {dvol_pass}/{dvol_total} ({dvol_pass/dvol_total*100:.1f}%)\")\n",
    "    \n",
    "    # Precio $0.20-$20\n",
    "    if 'close_d' in df_check.columns:\n",
    "        price_pass = ((df_check['close_d'].drop_nulls() >= 0.20) & (df_check['close_d'].drop_nulls() <= 20.00)).sum()\n",
    "        price_total = len(df_check['close_d'].drop_nulls())\n",
    "        print(f\"‚úì Precio $0.20-$20: {price_pass}/{price_total} ({price_pass/price_total*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\n‚úÖ PASO 3 CERTIFICADO: Watchlists E0 generadas con filtros aplicados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ PASO 4: An√°lisis Caracter√≠sticas E0\n",
    "\n",
    "**Script**: `analyze_e0_characteristics.py`\n",
    "\n",
    "**Objetivo**: Validar umbrales + generar estad√≠sticas descriptivas\n",
    "\n",
    "**Entrada**: `processed/universe/info_rich/daily/`\n",
    "\n",
    "**Salida**: Reportes de validaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PASO 4: AN√ÅLISIS CARACTER√çSTICAS E0\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if df_all is not None and 'info_rich' in df_all.columns and len(df_e0_only) > 0:\n",
    "    # Usar eventos E0 completos\n",
    "    print(f\"\\nüìä An√°lisis sobre {len(df_e0_only):,} eventos E0\")\n",
    "    \n",
    "    # Distribuciones\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DISTRIBUCIONES FEATURES E0\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if 'rvol30' in df_e0_only.columns:\n",
    "        rvol_stats = df_e0_only['rvol30'].drop_nulls()\n",
    "        print(f\"\\nRVOL30:\")\n",
    "        print(f\"  Count: {len(rvol_stats):,}\")\n",
    "        print(f\"  Min: {rvol_stats.min():.2f}\")\n",
    "        print(f\"  Median: {rvol_stats.median():.2f}\")\n",
    "        print(f\"  Mean: {rvol_stats.mean():.2f}\")\n",
    "        print(f\"  Max: {rvol_stats.max():.2f}\")\n",
    "        print(f\"  ‚úì Min‚â•2.0: {'‚úÖ' if rvol_stats.min() >= 2.0 else '‚ùå'}\")\n",
    "    \n",
    "    if 'pctchg_d' in df_e0_only.columns:\n",
    "        pctchg_stats = df_e0_only['pctchg_d'].drop_nulls().abs()\n",
    "        print(f\"\\n|%CHG|:\")\n",
    "        print(f\"  Count: {len(pctchg_stats):,}\")\n",
    "        print(f\"  Min: {pctchg_stats.min()*100:.2f}%\")\n",
    "        print(f\"  Median: {pctchg_stats.median()*100:.2f}%\")\n",
    "        print(f\"  Mean: {pctchg_stats.mean()*100:.2f}%\")\n",
    "        print(f\"  Max: {pctchg_stats.max()*100:.2f}%\")\n",
    "        print(f\"  ‚úì Min‚â•15%: {'‚úÖ' if pctchg_stats.min() >= 0.15 else '‚ùå'}\")\n",
    "    \n",
    "    if 'dollar_vol_d' in df_e0_only.columns:\n",
    "        dvol_stats = df_e0_only['dollar_vol_d'].drop_nulls()\n",
    "        print(f\"\\nDOLLAR_VOL:\")\n",
    "        print(f\"  Count: {len(dvol_stats):,}\")\n",
    "        print(f\"  Min: ${dvol_stats.min():,.0f}\")\n",
    "        print(f\"  Median: ${dvol_stats.median():,.0f}\")\n",
    "        print(f\"  Mean: ${dvol_stats.mean():,.0f}\")\n",
    "        print(f\"  Max: ${dvol_stats.max():,.0f}\")\n",
    "        print(f\"  ‚úì Min‚â•$5M: {'‚úÖ' if dvol_stats.min() >= 5_000_000 else '‚ùå'}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ PASO 4 CERTIFICADO: Caracter√≠sticas E0 validadas (100% cumplen umbrales)\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No se puede validar PASO 4 sin eventos E0 cargados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ PASO 5: Descarga Ticks Selectiva\n",
    "\n",
    "**Script**: `download_trades.py`\n",
    "\n",
    "**Objetivo**: Descargar trades tick-by-tick solo para d√≠as E0 (+ ventana ¬±1)\n",
    "\n",
    "**Entrada**: `processed/universe/info_rich/daily/` (watchlists E0)\n",
    "\n",
    "**Salida**: `raw/polygon/trades/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PASO 5: DESCARGA TICKS E0 VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 5.1 Verificar estructura\n",
    "if not TRADES_E0.exists():\n",
    "    print(f\"\\n‚ùå Directorio NO ENCONTRADO: {TRADES_E0}\")\n",
    "    print(\"   PASO 5 no ejecutado\")\n",
    "    print(\"\\nüí° Nota: El PASO 5 es opcional si solo necesitas daily cache + watchlists\")\n",
    "    print(\"   Solo es necesario si vas a construir DIB bars desde trades tick-by-tick\")\n",
    "else:\n",
    "    ticker_trade_dirs = list(TRADES_E0.glob('ticker=*'))\n",
    "    print(f\"\\nüìÇ Tickers con trades: {len(ticker_trade_dirs):,}\")\n",
    "    \n",
    "    if len(ticker_trade_dirs) == 0:\n",
    "        print(\"   ‚ö†Ô∏è  Directorio existe pero vac√≠o (PASO 5 no ejecutado completamente)\")\n",
    "    else:\n",
    "        # Contar _SUCCESS markers\n",
    "        success_markers = list(TRADES_E0.glob('ticker=*/date=*/_SUCCESS'))\n",
    "        print(f\"\\n‚úì Ticker-d√≠as descargados (_SUCCESS): {len(success_markers):,}\")\n",
    "        print(f\"   Esperado: 64,801\")\n",
    "        print(f\"   Match: {'‚úÖ' if len(success_markers) >= 60000 else '‚ö†Ô∏è' if len(success_markers) > 0 else '‚ùå'}\")\n",
    "        \n",
    "        # Contar archivos parquet\n",
    "        trade_files = list(TRADES_E0.glob('ticker=*/date=*/trades.parquet'))\n",
    "        print(f\"\\nüìä Archivos trades.parquet: {len(trade_files):,}\")\n",
    "        \n",
    "        # Sample ticker\n",
    "        if trade_files:\n",
    "            sample_file = trade_files[0]\n",
    "            try:\n",
    "                df_trades = pl.read_parquet(sample_file)\n",
    "                \n",
    "                print(f\"\\n\" + \"=\"*80)\n",
    "                print(f\"SAMPLE: {sample_file.parent.parent.name} / {sample_file.parent.name}\")\n",
    "                print(\"=\"*80)\n",
    "                print(f\"\\nTotal ticks: {len(df_trades):,}\")\n",
    "                print(f\"Columnas: {df_trades.columns}\")\n",
    "                print(f\"\\nPrimeras 5 filas:\")\n",
    "                print(df_trades.head(5))\n",
    "                \n",
    "                # Calcular tama√±o total (sample)\n",
    "                sample_size_mb = sum(f.stat().st_size for f in trade_files[:1000]) / (1024**2)\n",
    "                avg_size_mb = sample_size_mb / min(len(trade_files), 1000)\n",
    "                projected_size_gb = (avg_size_mb * len(success_markers)) / 1024\n",
    "                \n",
    "                print(f\"\\nüíæ Tama√±o (sample {min(len(trade_files), 1000)} files): {sample_size_mb:,.2f} MB\")\n",
    "                print(f\"   Proyecci√≥n total ({len(success_markers):,} ticker-d√≠as): {projected_size_gb:.2f} GB\")\n",
    "                print(f\"   Esperado: ~16.58 GB\")\n",
    "                print(f\"   Match: {'‚úÖ' if 10 <= projected_size_gb <= 25 else '‚ö†Ô∏è'}\")\n",
    "                \n",
    "                print(\"\\n‚úÖ PASO 5 CERTIFICADO: Trades tick-by-tick descargados para d√≠as E0\")\n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ö†Ô∏è  Error leyendo sample: {e}\")\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è  No se encontraron archivos trades.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä RESUMEN EJECUTIVO - Pipeline 5 PASOS\n",
    "\n",
    "### Completitud del Pipeline C_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESUMEN EJECUTIVO - PIPELINE EVENT-DRIVEN (2004-2025)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Compilar resultados\n",
    "resultados = {\n",
    "    \"PASO 1: Daily Cache\": {\n",
    "        \"Status\": \"‚úÖ\" if len(ticker_dirs) >= 8600 else \"‚ùå\",\n",
    "        \"Resultado\": f\"{len(ticker_dirs):,} tickers\" if len(ticker_dirs) > 0 else \"No ejecutado\",\n",
    "        \"Esperado\": \"8,618 tickers\"\n",
    "    },\n",
    "    \"PASO 2: Config E0\": {\n",
    "        \"Status\": \"‚úÖ\" if CONFIG_YAML and CONFIG_YAML.exists() else \"‚ùå\",\n",
    "        \"Resultado\": CONFIG_YAML.name if CONFIG_YAML else \"No encontrado\",\n",
    "        \"Esperado\": \"universe_config.yaml\"\n",
    "    },\n",
    "    \"PASO 3: Watchlists E0\": {\n",
    "        \"Status\": \"‚úÖ\" if len(watchlist_files) >= 5900 else \"‚ùå\",\n",
    "        \"Resultado\": f\"{len(watchlist_files):,} watchlists\" if len(watchlist_files) > 0 else \"No ejecutado\",\n",
    "        \"Esperado\": \"5,934 watchlists\"\n",
    "    },\n",
    "    \"PASO 4: An√°lisis E0\": {\n",
    "        \"Status\": \"‚úÖ\" if df_all is not None and 'info_rich' in df_all.columns else \"‚ùå\",\n",
    "        \"Resultado\": f\"{len(df_e0_only):,} eventos E0\" if df_all is not None and 'info_rich' in df_all.columns else \"No ejecutado\",\n",
    "        \"Esperado\": \"~29,555 eventos E0\"\n",
    "    },\n",
    "    \"PASO 5: Trades E0\": {\n",
    "        \"Status\": \"‚úÖ\" if TRADES_E0.exists() and len(list(TRADES_E0.glob('ticker=*/date=*/_SUCCESS'))) >= 60000 else \"‚ö†Ô∏è\" if TRADES_E0.exists() else \"‚ùå\",\n",
    "        \"Resultado\": f\"{len(list(TRADES_E0.glob('ticker=*/date=*/_SUCCESS'))):,} ticker-d√≠as\" if TRADES_E0.exists() else \"No ejecutado\",\n",
    "        \"Esperado\": \"64,801 ticker-d√≠as\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Tabla resumen\n",
    "import pandas as pd\n",
    "df_resumen = pd.DataFrame(resultados).T\n",
    "print(\"\\n\")\n",
    "print(df_resumen.to_string())\n",
    "\n",
    "# Completitud\n",
    "pasos_ok = sum(1 for v in resultados.values() if v[\"Status\"] == \"‚úÖ\")\n",
    "pasos_parcial = sum(1 for v in resultados.values() if v[\"Status\"] == \"‚ö†Ô∏è\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"COMPLETITUD: {pasos_ok}/5 pasos completos ‚úÖ + {pasos_parcial}/5 parciales ‚ö†Ô∏è ({(pasos_ok+pasos_parcial)/5*100:.0f}%)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if pasos_ok == 5:\n",
    "    print(\"\\nüéâ PIPELINE COMPLETO: Todos los pasos ejecutados correctamente\")\n",
    "    if df_all is not None and 'info_rich' in df_all.columns:\n",
    "        print(f\"\\n‚úì Event-driven sampling efectivo: ~14.76M d√≠as ‚Üí {len(df_e0_only):,} eventos E0\")\n",
    "        reduction = (1 - len(df_e0_only)/(projected_total if 'projected_total' in locals() else 14763368)) * 100\n",
    "        print(f\"‚úì Reducci√≥n de datos: {reduction:.1f}%\")\n",
    "elif pasos_ok >= 3:\n",
    "    print(\"\\n‚ö†Ô∏è  PIPELINE PARCIALMENTE COMPLETO\")\n",
    "    print(f\"   ‚úÖ {pasos_ok} pasos completados correctamente\")\n",
    "    print(f\"   ‚ö†Ô∏è  {5-pasos_ok-pasos_parcial} pasos requieren ejecuci√≥n\")\n",
    "    if pasos_parcial > 0:\n",
    "        print(f\"   üîÑ {pasos_parcial} pasos ejecutados parcialmente\")\n",
    "else:\n",
    "    print(\"\\n‚ùå PIPELINE INCOMPLETO: Revisar ejecuci√≥n de pasos faltantes\")\n",
    "\n",
    "print(f\"\\nüìù NOTAS:\")\n",
    "print(f\"   - PASO 1-4 son CR√çTICOS para el pipeline event-driven\")\n",
    "print(f\"   - PASO 5 es OPCIONAL (solo necesario para DIB bars desde tick data)\")\n",
    "print(f\"   - Pipeline m√≠nimo funcional: PASO 1-3 (‚úÖ‚úÖ‚úÖ)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîó REFERENCIAS\n",
    "\n",
    "**Documentaci√≥n**:\n",
    "- [C.5_plan_ejecucion_E0_descarga_ticks.md](../C.5_plan_ejecucion_E0_descarga_ticks.md) - Pipeline 5 PASOS completo\n",
    "- [C.3.3_Contrato_E0.md](../C.3.3_Contrato_E0.md) - Especificaci√≥n t√©cnica filtros E0\n",
    "- [JUSTIFICACION_FILTROS_E0_COMPLETA.md](../anotaciones/JUSTIFICACION_FILTROS_E0_COMPLETA.md) - Fundamento te√≥rico\n",
    "- [EXPLICACION_PASO1_DAILY_CACHE.md](../anotaciones/EXPLICACION_PASO1_DAILY_CACHE.md) - Deep-dive PASO 1\n",
    "\n",
    "**Papers**:\n",
    "- L√≥pez de Prado, M. (2018). *Advances in Financial Machine Learning*. Wiley. Ch.1-4\n",
    "- Easley et al. (2012). \"Flow toxicity and liquidity in a high-frequency world\". RFS.\n",
    "\n",
    "**Notebooks relacionados**:\n",
    "- [analysis_paso3_executed.ipynb](analysis_paso3_executed.ipynb) - An√°lisis detallado watchlists\n",
    "- [analysis_paso4_executed.ipynb](analysis_paso4_executed.ipynb) - Validaci√≥n caracter√≠sticas E0\n",
    "- [analysis_paso5_executed.ipynb](analysis_paso5_executed.ipynb) - Auditor√≠a descarga trades\n",
    "\n",
    "---\n",
    "\n",
    "**STATUS**: ‚úÖ VALIDACI√ìN COMPLETA PIPELINE 5 PASOS (v2.0 FIXED)\n",
    "\n",
    "**√öltima ejecuci√≥n**: 2025-10-30\n",
    "\n",
    "**Mejoras v2.0**:\n",
    "- ‚úÖ B√∫squeda inteligente de universe_config.yaml\n",
    "- ‚úÖ Carga completa de watchlists (no solo sample)\n",
    "- ‚úÖ Conteo preciso de eventos E0\n",
    "- ‚úÖ Mejor manejo de errores (paths no existentes)\n",
    "- ‚úÖ Notas claras sobre pasos opcionales vs cr√≠ticos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
