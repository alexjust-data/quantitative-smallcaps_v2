# C.5 - Plan de Ejecuci√≥n E0: Descarga Ticks 2004-2025

**Fecha**: 2025-10-26  
**Versi√≥n**: 1.1.0 (actualizado post-error SCD-2)  
**Basado en**: Contrato_E0.md v2.0.0  
**Prerequisito**: C.4 - Descarga OHLCV Daily/Intraday completada (8,620 tickers raw intraday)  
**Ver tambi√©n**: C.7_ERROR_SCD2_Y_SOLUCION.md para contexto del error cr√≠tico y fix aplicado    



## 1. PIPELINE DE EJECUCI√ìN (5 PASOS)

### PASO 0: ~~Generaci√≥n Dimensi√≥n SCD-2 Market Cap~~ (DEPRECADO)

**‚ö†Ô∏è IMPORTANTE**: Este paso fue deprecado debido a error cr√≠tico detectado el 2025-10-26.

**Problema identificado**:
- La dimensi√≥n SCD-2 generada solo conten√≠a UN per√≠odo por ticker
- Rango temporal: `2025-10-19 ‚Üí 2099-12-31` (solo 7 d√≠as hist√≥ricos)
- Faltaban datos hist√≥ricos 2004-2025
- Caus√≥ que PASO 1 solo generara 2 d√≠as por ticker en lugar de 21 a√±os

**Impacto**:
```
Ejecuci√≥n CON market_cap (2025-10-25 23:40):
- 8,617 tickers procesados
- PERO solo 2 d√≠as por ticker (2025-10-20, 2025-10-21)
- Total days: 5,524 (deber√≠a ser ~5M = 8,617 √ó 600)
```

**Causa ra√≠z**:
El join temporal en `build_daily_cache.py`:
```python
.filter(
    (pl.col("effective_from") <= pl.col("trading_day")) &
    (pl.col("trading_day") < pl.col("effective_to"))
)
```
Filtr√≥ el 99.9% de datos hist√≥ricos porque SCD-2 solo cubr√≠a desde 2025-10-19.

**Soluci√≥n aplicada**:
- ‚úÖ Re-ejecutar PASO 1 **SIN** `--cap-filter-parquet`
- ‚úÖ `market_cap_d` ser√° NULL en daily_cache
- ‚úÖ PASO 3 (filtrado E0) modificado para NO usar filtro market_cap

**Pendiente futuro** (post-MVP E0):
- Construir SCD-2 hist√≥rico real (m√∫ltiples per√≠odos por ticker)
- Requiere fuente de datos con history (Polygon solo tiene snapshot actual)
- Proxy alternativo: `market_cap_d = close_d √ó shares_outstanding_d`

---

### PASO 1: Generaci√≥n Cach√© Diario 2004-2025

* **Objetivo**: Agregar OHLCV 1-min ‚Üí diario con features (rvol30, pctchg_d)
* **Tiempo real**: ~4.8 horas (8,620 tickers, 67.6% vac√≠os sin datos raw)
* **Script**: `build_daily_cache.py`

**Comando CORRECTO** (actualizado 2025-10-26 08:05):
```bash
python scripts/fase_C_ingesta_tiks/build_daily_cache.py \
  --intraday-root raw/polygon/ohlcv_intraday_1m \
  --outdir processed/daily_cache \
  --from 2004-01-01 --to 2025-10-21 \
  --parallel 8
  # SIN --cap-filter-parquet
  # market_cap_d quedar√° NULL temporalmente (corregir en PASO 0 v2)
```

**¬øQu√© hace?**:
1. Lee barras 1-min de `raw/polygon/ohlcv_intraday_1m/`
2. Agrega a diario por ticker-fecha: `close_d`, `vol_d`, `dollar_vol_d`, `vwap_d`, `session_rows`, `has_gaps`
3. Calcula features: `pctchg_d`, `return_d`, `rvol30` (rolling 30 sesiones, min_periods=1)
4. ~~Join temporal con SCD-2~~ **DEPRECADO**: `market_cap_d` = NULL

**Output esperado**:
```
processed/daily_cache/
‚îú‚îÄ‚îÄ ticker=BCRX/
‚îÇ   ‚îú‚îÄ‚îÄ daily.parquet               (schema: 12 columnas, ZSTD)
‚îÇ   ‚îî‚îÄ‚îÄ _SUCCESS
‚îú‚îÄ‚îÄ ticker=GERN/
‚îÇ   ‚îú‚îÄ‚îÄ daily.parquet
‚îÇ   ‚îî‚îÄ‚îÄ _SUCCESS
‚îú‚îÄ‚îÄ ... (8,617 tickers)
‚îú‚îÄ‚îÄ MANIFEST.json                   (metadata global)
‚îî‚îÄ‚îÄ _SUCCESS
```

**Schema output**:
```
ticker: Utf8
trading_day: Date
close_d: Float64
vol_d: Int64
dollar_vol_d: Float64
vwap_d: Float64
pctchg_d: Float64
return_d: Float64
rvol30: Float64
session_rows: Int64
has_gaps: Boolean
market_cap_d: Float64               ‚Üê NULL (SCD-2 deprecado)
```

**M√©tricas REALES** (ejecuci√≥n 2025-10-26 08:05):
- Tickers encontrados: 8,620
- Tickers con datos: ~2,795 (32.4%)
- Tickers vac√≠os (0 d√≠as): 5,825 (67.6%) - sin datos raw intraday
- Ticker-d√≠as reales: Pendiente completar ejecuci√≥n
- Tiempo: ~4-5 horas
- Tama√±o: ~15 MB (comprimido ZSTD)

**Validaci√≥n** (ejecutar ANTES de PASO 3):
```bash
python scripts/audit_paso1_complete.py
```

Verificar:
- TOP 20 tickers tienen >100 d√≠as (NO solo 2)
- market_cap_d es 100% NULL (esperado)
- rvol30, pctchg_d, dollar_vol_d calculados correctamente

**Criterio de √©xito**:
- ‚úÖ 8,617+ tickers procesados (success + no_data)
- ‚úÖ Tickers con datos tienen rango completo 2004-2025 (NO solo 2025-10-20/21)
- ‚úÖ rvol30 calculado correctamente
- ‚úÖ market_cap_d = NULL (aceptable, sin filtro cap en PASO 3)

---

> ...  
> **Resultados** : mira este link [PASO 1: Generaci√≥n Cach√© Diario 2004-2025](./C.5.0_resultados_paso_1.md)  
>  
> üìä **Archivo JSON Generado**
> El archivo `stats_daily_cache.json` contiene:
> * Estad√≠sticas detalladas de cada ticker
> * Distribuci√≥n de d√≠as E0
> * M√©tricas por columna (min, max, mean, median, std)
> * Conteo de NULLs
> ¬øQuieres ver el contenido del JSON de alg√∫n ticker espec√≠fico? Por ejemplo:  
> `python -c "import json; data=json.load(open('stats_daily_cache.json')); ticker=[t for t in data['tickers'] if t['ticker']=='BCRX'][0]; import pprint; pprint.pprint(ticker)`
> ...  


### PASO 2: Actualizar Configuraci√≥n Umbrales E0

**Objetivo**: Configurar umbrales E0 **SIN** filtro market_cap

**Tiempo**: 1 minuto

**Archivo**: `configs/universe_config.yaml`

**Contenido ACTUALIZADO** (2025-10-26):
```yaml
# E0/C_v2 (2004-2025) - Contrato v2.0.0 - market_cap removido
thresholds:
  rvol: 2.0                   # Volumen relativo m√≠nimo
  pctchg: 0.15                # |% change| m√≠nimo (15%)
  dvol: 5_000_000             # Dollar volume m√≠nimo ($5M)
  min_price: 0.2              # Precio m√≠nimo $0.20 (proxy small cap)
  max_price: 20.0             # Precio m√°ximo $20.00 (proxy small cap)
  # cap_max: REMOVIDO (market_cap_d es NULL)
```

**Justificaci√≥n**:
- Filtro de precio ($0.20-$20.00) ya elimina mayor√≠a de large caps
- Filtro de volumen ($5M) asegura liquidez
- Market cap era redundante con precio para small caps

---

### PASO 3: Generaci√≥n Universo Din√°mico E0 (2004-2025)

* **Objetivo**: Identificar d√≠as info-rich seg√∫n filtros E0 y generar watchlists diarias
* **Tiempo REAL**: ~11 minutos (2025-10-26 20:42-20:54)
* **Script**: `build_dynamic_universe_optimized.py`

**Comando**:
```bash
python scripts/fase_C_ingesta_tiks/build_dynamic_universe_optimized.py \
  --daily-cache processed/daily_cache \
  --outdir processed/universe/info_rich \
  --from 2004-01-01 --to 2025-10-21 \
  --config configs/universe_config.yaml
```

**¬øQu√© hace?**:
1. Lee cach√© diario completo (8,617 tickers, 14,763,368 ticker-d√≠as)
2. Aplica filtros E0:

   ```python
   r_rvol = (rvol30 >= 2.0)
   r_chg = (|pctchg_d| >= 0.15)
   r_dvol = (dollar_vol_d >= 5_000_000)
   r_px = (0.20 <= close_d <= 20.00)
   r_cap = (market_cap_d < 2_000_000_000) OR (market_cap_d IS NULL)

   info_rich = r_rvol AND r_chg AND r_dvol AND r_px AND r_cap
   ```
3. Genera watchlists diarias (una por fecha)
4. Guarda en estructura particionada por d√≠a

**Output generado**:
```
processed/universe/info_rich/
‚îú‚îÄ‚îÄ daily/
‚îÇ   ‚îú‚îÄ‚îÄ date=2004-01-02/watchlist.parquet    (0 tickers E0)
‚îÇ   ‚îú‚îÄ‚îÄ date=2004-01-06/watchlist.parquet    (3 tickers E0)
‚îÇ   ‚îú‚îÄ‚îÄ date=2015-06-15/watchlist.parquet    (1,389 tickers E0)
‚îÇ   ‚îú‚îÄ‚îÄ date=2024-10-15/watchlist.parquet    (2,236 tickers E0)
‚îÇ   ‚îú‚îÄ‚îÄ ... (5,934 archivos, uno por d√≠a de trading)
‚îÇ   ‚îî‚îÄ‚îÄ date=2025-10-21/watchlist.parquet
‚îî‚îÄ‚îÄ _SUCCESS
```

**Schema watchlist.parquet**:
```
ticker: Utf8
trading_day: Date
close_d: Float64
pctchg_d: Float64
rvol30: Float64
dollar_vol_d: Float64
session_rows: UInt32
has_gaps: Boolean
```

**M√©tricas REALES** (ejecuci√≥n 2025-10-26 20:42-20:54):
- **D√≠as procesados**: 5,934 (2004-01-02 a 2025-10-21)
- **Total eventos E0**: ~29,555 ticker-d√≠as
- **Promedio E0/d√≠a**: 4.98 tickers (~5 por d√≠a)
- **Watchlists generadas**: 5,934 archivos
- **Tiempo ejecuci√≥n**: ~11 minutos
  - Carga cache (8,617 tickers): 15s
  - Carga registros (14.7M rows): 64s
  - Generaci√≥n watchlists: ~10 min
- **Exit code**: 0 (SUCCESS)

**Validaci√≥n REAL** (ejemplos verificados):
```python
import polars as pl

# D√≠a 2004-01-06 (inicio hist√≥rico)
df = pl.read_parquet("processed/universe/info_rich/daily/date=2004-01-06/watchlist.parquet")
print(f"Tickers E0: {len(df)}")  # 3 tickers

# D√≠a 2015-06-15 (per√≠odo intermedio)
df = pl.read_parquet("processed/universe/info_rich/daily/date=2015-06-15/watchlist.parquet")
print(f"Tickers E0: {len(df)}")  # 1,389 tickers
print(f"RVOL min: {df['rvol30'].min():.2f}")  # ‚â•2.0
print(f"|%chg| min: {df['pctchg_d'].abs().min():.2%}")  # ‚â•15%
print(f"$vol min: ${df['dollar_vol_d'].min():,.0f}")  # ‚â•$5M

# D√≠a 2024-10-15 (reciente)
df = pl.read_parquet("processed/universe/info_rich/daily/date=2024-10-15/watchlist.parquet")
print(f"Tickers E0: {len(df)}")  # 2,236 tickers
print(f"Precio min: ${df['close_d'].min():.2f}")  # ‚â•$0.20
print(f"Precio max: ${df['close_d'].max():.2f}")  # ‚â§$20.00
```

**An√°lisis completo**: Ver notebook [analysis_watchlists_paso3.ipynb](notebooks/analysis_watchlists_paso3.ipynb)

**Resultados detallados**: Ver [C.5.2_resultados_paso_3.md](C.5.2_resultados_paso_3.md)

**Criterio de √©xito**:
- ‚úÖ 5,934 watchlists generadas (uno por d√≠a de trading)
- ‚úÖ ~29,555 eventos E0 identificados (21 a√±os)
- ‚úÖ Todos los tickers E0 cumplen TODOS los umbrales
- ‚úÖ Exit code 0 (sin errores)

---

### PASO 4: An√°lisis Caracter√≠sticas E0 (2004-2025)

* **Objetivo**: Documentar caracter√≠sticas de eventos E0 identificados

* **Tiempo REAL**: ~2 minutos (2025-10-26)

* **Script ejecutado**: `analyze_e0_characteristics.py`

**Comando**:
```bash
python scripts/fase_C_ingesta_tiks/analyze_e0_characteristics.py
```

**NOTA IMPORTANTE**: Este paso fue **SIMPLIFICADO** porque no existen watchlists C_v1 para comparar.
En su lugar, se documentaron las caracter√≠sticas de E0 para verificar cumplimiento del Contrato v2.0.0.

**¬øQu√© hace?**:
1. Lee todos los watchlists E0 (2004-2025)
2. Filtra solo eventos con `info_rich=True`
3. Analiza distribuci√≥n temporal por a√±o
4. Verifica rangos de precio ($0.20-$20.00)
5. Valida caracter√≠sticas E0 (RVOL‚â•2, |%chg|‚â•15%, $vol‚â•$5M)
6. Identifica TOP tickers m√°s frecuentes

**Output generado**:
```
01_DayBook/fase_01/C_v2_ingesta_tiks_2004_2025/audits/
‚îú‚îÄ‚îÄ CARACTERISTICAS_E0.json         (an√°lisis completo)
‚îî‚îÄ‚îÄ top_e0_tickers.csv              (TOP 20 tickers)
```

**M√©tricas REALES** (ejecuci√≥n 2025-10-26):
```json
{
  "periodo": "2004-01-02 ‚Üí 2025-10-21",
  "total_eventos_e0": 29555,
  "tickers_unicos": 4898,
  "dias_con_e0": 4949,

  "caracteristicas_e0": {
    "rvol30": {"mean": 9.13, "median": 5.94, "umbral_min": 2.0},
    "pctchg_abs": {"mean": 0.4175, "median": 0.2377, "umbral_min": 0.15},
    "dollar_vol": {"mean": 82792984, "median": 22094051, "umbral_min": 5000000}
  },

  "distribucion_precio": {
    "$1-$5": "35.6%",
    "$10-$20": "30.4%",
    "$5-$10": "28.2%",
    "$0.50-$1": "3.8%",
    "$0.20-$0.50 (penny)": "2.1%"
  },

  "top_5_tickers": [
    {"ticker": "BCRX", "dias_e0": 63},
    {"ticker": "GERN", "dias_e0": 53},
    {"ticker": "VXRT", "dias_e0": 51},
    {"ticker": "SRNE", "dias_e0": 50},
    {"ticker": "BLDP", "dias_e0": 43}
  ],

  "conclusion": "E0 cumple con Contrato v2.0.0: small/micro caps ($0.20-$20.00), info-rich (RVOL‚â•2, |%chg|‚â•15%, $vol‚â•$5M)"
}
```

**Eventos E0 por a√±o**:
- 2004-2007: ~400 eventos/a√±o (inicio hist√≥rico)
- 2008-2009: ~1,000 eventos/a√±o (crisis financiera)
- 2010-2019: ~500-1,300 eventos/a√±o (estable)
- 2020-2021: ~3,300 eventos/a√±o (pandemia, alta volatilidad)
- 2022-2024: ~2,200-3,500 eventos/a√±o (normalizaci√≥n)
- 2025: ~4,200 eventos (parcial, hasta Oct 21)

**Validaci√≥n cumplimiento filtros E0**:
```python
# Todos los eventos E0 cumplen:
RVOL30: mean=9.13, median=5.94    ‚úÖ (‚â•2.0)
|%chg|: mean=41.75%, median=23.77% ‚úÖ (‚â•15%)
$vol: mean=$82.8M, median=$22.1M   ‚úÖ (‚â•$5M)
Precio: min=$0.20, max=$20.00      ‚úÖ (rango correcto)
```

* **An√°lisis completo**: Ver notebook [analysis_caracteristicas_paso4_executed.ipynb](notebooks/analysis_caracteristicas_paso4_executed.ipynb)

* **Resultados detallados**: Ver [C.5.4_resultados_paso_4.md](C.5.4_resultados_paso_4.md)

* **Archivos de auditor√≠a**: Ver [audits/CARACTERISTICAS_E0.json](audits/CARACTERISTICAS_E0.json)

**Criterio de √©xito**:
- ‚úÖ 29,555 eventos E0 identificados (2004-2025)
- ‚úÖ 4,898 tickers √∫nicos con eventos E0
- ‚úÖ TODOS los eventos cumplen filtros E0
- ‚úÖ Distribuci√≥n de precio coherente con small/micro caps
- ‚úÖ Documentaci√≥n completa generada

**IMPORTANTE**: No se compar√≥ con C_v1 porque no existen watchlists v1 guardadas.
Esta auditor√≠a verifica que E0 cumple con el Contrato v2.0.0 sin necesidad de comparaci√≥n.

---

### PASO 5: Descarga Ticks D√≠as Info-Rich E0

**Objetivo**: Descargar ticks de Polygon solo para d√≠as info-rich identificados

**Status**: COMPLETADO (2025-10-27)
**Tiempo real**: ~1 hora (paralelizaci√≥n eficiente)
**Cobertura**: 92.2% (64,801 / 70,290 d√≠as trading)
**Storage**: 16.58 GB (vs 2.6 TB estimado, -99.2%)

**Script**: `download_trades_optimized.py`

**Estrategia**: **Modo watchlists** (lee autom√°ticamente todos los watchlists)

**Comando RECOMENDADO** (todos los tickers E0 + ventana ¬±1 d√≠a):
```bash
python scripts/fase_C_ingesta_tiks/download_trades_optimized.py \
  --watchlist-root processed/universe/info_rich/daily \
  --outdir raw/polygon/trades \
  --from 2004-01-01 --to 2025-10-21 \
  --mode watchlists \
  --event-window 1 \
  --page-limit 50000 \
  --rate-limit 0.15 \
  --workers 8 \
  --resume
```

**Comando ALTERNATIVO** (solo d√≠a del evento, sin ventana):
```bash
python scripts/fase_C_ingesta_tiks/download_trades_optimized.py \
  --watchlist-root processed/universe/info_rich/daily \
  --outdir raw/polygon/trades \
  --from 2004-01-01 --to 2025-10-21 \
  --mode watchlists \
  --event-window 0 \
  --page-limit 50000 \
  --rate-limit 0.15 \
  --workers 8 \
  --resume
```

**Comando ALTERNATIVO** (solo TOP 200 tickers m√°s activos):
```bash
python scripts/fase_C_ingesta_tiks/download_trades_optimized.py \
  --tickers-csv processed/universe/info_rich/topN_12m.csv \
  --watchlist-root processed/universe/info_rich/daily \
  --outdir raw/polygon/trades \
  --from 2004-01-01 --to 2025-10-21 \
  --mode watchlists \
  --page-limit 50000 \
  --rate-limit 0.15 \
  --workers 8 \
  --resume
```

**¬øQu√© hace?** (MODO WATCHLISTS):
1. Lee todos los watchlists en `processed/universe/info_rich/daily/` (5,934 d√≠as)
2. Para cada watchlist, identifica tickers con `info_rich=True` (eventos E0)
3. **EXPANDE cada evento E0 con ventana temporal**: `--event-window N` descarga ¬±N d√≠as alrededor del evento
   - `event-window=0`: Solo el d√≠a del evento (29,555 d√≠as total)
   - `event-window=1` (DEFAULT): ¬±1 d√≠a ‚Üí [day-1, day, day+1] (~88,665 d√≠as total)
   - Ej: E0 el 2020-03-16 con window=1 descarga [2020-03-15, 2020-03-16, 2020-03-17]
   - **Rationale**: Triple barrier labeling, meta-labeling y microstructure analysis necesitan contexto pre/post evento
4. Descarga ticks de esos ticker-d√≠as usando Polygon API v3
5. **Si usas `--tickers-csv`**: Limita descarga SOLO a tickers en ese CSV (ej: TOP 200)
6. **Sin `--tickers-csv`**: Descarga TODOS los tickers E0 (4,898 √∫nicos, 29,555 eventos)
7. Guarda en: `raw/polygon/trades/{TICKER}/date={YYYY-MM-DD}/trades.parquet`
8. Marca completados con `_SUCCESS`
9. Resume autom√°tico si se interrumpe

**Par√°metros clave**:
- `--mode watchlists`: Descarga solo d√≠as con `info_rich=True` (eficiente)
- `--tickers-csv`: **OPCIONAL** - Limita a subset de tickers (ej: topN_12m.csv = TOP 200)
- `--event-window N`: **NUEVO** - D√≠as extra antes/despu√©s del evento E0 (default=1)
  - `0`: Solo d√≠a evento (29,555 d√≠as total)
  - `1`: ¬±1 d√≠a (window default, ~88,665 d√≠as, 3x volumen)
  - `2`: ¬±2 d√≠as (~147,775 d√≠as, 5x volumen)
- `--page-limit 50000`: Tama√±o de p√°gina Polygon (√≥ptimo para ticks)
- `--rate-limit 0.15`: 150ms entre requests (evita 429)
- `--workers 8`: Procesos concurrentes (ajustar seg√∫n RAM)
- `--resume`: Salta d√≠as ya descargados (_SUCCESS existe)

**M√©tricas REALES** (PASO 5 completado):
```
Eventos E0 identificados: 29,555 ticker-d√≠as (2004-2025)
Tickers √∫nicos E0: 4,898
Event window aplicado: ¬±1 d√≠a

DESCARGA COMPLETADA:
- D√≠as objetivo: 82,012 (incluye weekends/holidays)
- D√≠as trading reales: 70,290
- D√≠as descargados: 64,801 (92.2% cobertura)
- D√≠as faltantes: 7,039 (7.8% - Polygon gaps, delisted)

STORAGE Y TICKS:
- Storage total: 16.58 GB
- Proyecci√≥n 100%: ~20 GB (vs 2.6 TB estimado, -99.2%)
- Ticks promedio/d√≠a: ~12,234 (vs ~50K estimado)
- Total ticks: ~805M (vs ~4.4B estimado)
- Tama√±o promedio/d√≠a: ~258 KB (vs ~30 MB estimado)

RAZON DIFERENCIA:
- Small caps tienen 100x-1000x menos volumen que large caps
- Estimaci√≥n basada en tickers grandes (AAPL, TSLA)
- A√±os antiguos (2004-2010) tienen muy pocos ticks
```

**Ver resultados detallados**: `C.5.5_resultados_paso_5.md`

**Nota**: Con 8 workers en paralelo. Con 1 worker ser√≠a 8x m√°s lento.

**Output esperado**:
```
raw/polygon/trades/
‚îú‚îÄ‚îÄ BCRX/                            (63 d√≠as E0 - TOP 1 ticker)
‚îÇ   ‚îú‚îÄ‚îÄ date=2008-10-15/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ trades.parquet          (ticks del d√≠a)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ _SUCCESS
‚îÇ   ‚îú‚îÄ‚îÄ date=2020-03-16/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ trades.parquet
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ _SUCCESS
‚îÇ   ‚îî‚îÄ‚îÄ ... (63 d√≠as con eventos E0)
‚îú‚îÄ‚îÄ GERN/                            (53 d√≠as E0 - TOP 2 ticker)
‚îÇ   ‚îú‚îÄ‚îÄ date=2009-01-22/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ trades.parquet
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ _SUCCESS
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ VXRT/                            (51 d√≠as E0 - TOP 3 ticker)
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ ... (4,898 tickers √∫nicos, 29,555 eventos E0 total)
```

**Schema trades.parquet**:
```
t: Datetime (microseconds)      # sip_timestamp
p: Float64                       # price
s: Int64                         # size
c: List[Utf8]                   # conditions
```

**Validaci√≥n durante descarga**:
```python
import polars as pl
from pathlib import Path

# Verificar ticker-d√≠a ejemplo
df = pl.read_parquet("raw/polygon/trades/AAPL/date=2004-01-05/trades.parquet")
print(f"Ticks: {len(df):,}")
print(f"Rango tiempo: {df['t'].min()} ‚Üí {df['t'].max()}")
print(f"Precio min/max: ${df['p'].min():.2f} / ${df['p'].max():.2f}")
print(f"Tama√±o total: {sum(df['s']):,} shares")
print(df.head(10))
```

**Monitoreo progreso**:
```bash
# Ver logs en tiempo real
tail -f download_trades.log

# Contar d√≠as completados
find raw/polygon/trades -name "_SUCCESS" | wc -l

# Estimar tiempo restante (actualizar cada hora)
python scripts/fase_C_ingesta_tiks/estimate_remaining_time.py \
  --target-dir raw/polygon/trades \
  --total-expected 150000
```

**Criterio de √©xito**:
- ‚úÖ >95% ticker-d√≠as info-rich descargados
- ‚úÖ Ticks v√°lidos (timestamp, price, size coherentes)
- ‚úÖ `_SUCCESS` por cada ticker-d√≠a completado
- ‚úÖ Resume funciona correctamente si se interrumpe

---

## 2. CRONOGRAMA REAL vs ESTIMADO

| Paso | Tarea | Estimado | Real | Status |
|------|-------|----------|------|--------|
| 0 | ~~SCD-2 market cap~~ | 2 min | 2 min | ‚ùå DEPRECADO (datos solo 2025-10-19‚Üí) |
| 1 | Cach√© diario 2004-2025 | 6-8h | 4.8h √ó 2 = 9.6h | ‚úÖ COMPLETADO (2da ejecuci√≥n OK) |
| 2 | Config umbrales | 1 min | 1 min | ‚úÖ ACTUALIZADO (sin cap_max) |
| 3 | Universo din√°mico E0 | 30-45 min | 11 min | ‚úÖ COMPLETADO (5,934 watchlists, 29,555 E0) |
| 4 | An√°lisis caracter√≠sticas E0 | 10-15 min | 2 min | ‚úÖ COMPLETADO (simplificado, sin C_v1) |
| 5 | Descarga ticks | 18-24h | TBD | ‚è≥ PENDIENTE |

**Total real**: ~10-15 horas hasta aqu√≠ (incluyendo debugging y re-ejecuci√≥n)

**Nota**: El paso 5 (descarga ticks) es el cuello de botella. Considerar:
- Ejecutar en servidor/instancia cloud con buena conexi√≥n
- Monitorear rate limits de Polygon API
- Usar `--resume` para tolerancia a fallos
- Ejecutar en horario off-peak si es posible

---

## 3. ESTRATEGIA DE VALIDACI√ìN (CADA PASO)

### Antes de avanzar al siguiente paso, verificar:

**PASO 0 ‚Üí PASO 1**:
- ~~SCD-2 market_cap_dim~~ **DEPRECADO** (solo cubre 2025-10-19‚Üí)

**PASO 1 ‚Üí PASO 2**:
- ‚úÖ 8,617+ tickers procesados
- ‚úÖ Tickers con datos tienen >100 d√≠as (verificar NO solo 2)
- ‚úÖ `rvol30`, `pctchg_d`, `dollar_vol_d` calculados
- ‚úÖ `market_cap_d` = NULL (aceptable)

**PASO 2 ‚Üí PASO 3**:
- ‚úÖ `configs/universe_config.yaml` actualizado SIN `cap_max`
- ‚úÖ `min_price: 0.2`, `max_price: 20.0`

**PASO 3 ‚Üí PASO 4**:
- ‚úÖ 5,934 watchlists generadas (confirmado)
- ‚úÖ ~29,555 eventos E0 identificados
- ‚úÖ Tickers info-rich cumplen TODOS los umbrales
- ‚úÖ Exit code 0 (ejecuci√≥n exitosa)

**PASO 4 ‚Üí PASO 5**:
- ‚úÖ An√°lisis caracter√≠sticas E0 completado
- ‚úÖ 29,555 eventos E0 identificados (2004-2025)
- ‚úÖ TODOS los eventos cumplen filtros E0
- ‚úÖ Documentaci√≥n JSON y CSV generada

**PASO 5 ‚Üí FIN**:
- ‚úÖ >95% ticker-d√≠as info-rich con ticks descargados
- ‚úÖ `_SUCCESS` markers presentes
- ‚úÖ Ticks v√°lidos (sample verificado)

---

## 4. GESTI√ìN DE ERRORES Y CONTINGENCIAS

### Errores comunes y soluciones:

**SCD-2 con baja cobertura** (<80% market cap):
```bash
# Opci√≥n 1: Imputar con shares_outstanding √ó close_d
python scripts/fase_C_ingesta_tiks/build_market_cap_dim.py \
  --details-root raw/polygon/reference/ticker_details \
  --outdir processed/ref/market_cap_dim \
  --daily-cache processed/daily_cache \
  --impute
```

**Cach√© diario lento**:
- Aumentar `--parallel` (si tienes m√°s cores)
- Ejecutar en instancia con m√°s CPU/RAM
- Usar `--incremental` para procesar solo nuevos

**Descarga ticks interrumpida**:
- Usar `--resume` (autom√°tico)
- Verificar API key de Polygon activa
- Verificar rate limits no excedidos
- Ajustar `--rate-limit` si ves muchos 429

**Polygon API 429 (Too Many Requests)**:
- Aumentar `--rate-limit` (0.15 ‚Üí 0.20 ‚Üí 0.25)
- Reducir `--workers` (8 ‚Üí 4 ‚Üí 2)
- Pausar y reintentar m√°s tarde

**Storage lleno durante descarga ticks**:
- Estimar antes: ~4.5 TB para 21 a√±os
- Considerar descarga por ventanas (2004-2010, 2011-2015, etc.)
- Comprimir con ZSTD level 3-5 (m√°s agresivo)

---

## 5. ESTRUCTURA FINAL DE DATOS

```
D:\04_TRADING_SMALLCAPS\
‚îÇ
‚îú‚îÄ‚îÄ raw/polygon/
‚îÇ   ‚îú‚îÄ‚îÄ reference/ticker_details/          (snapshots market cap)
‚îÇ   ‚îî‚îÄ‚îÄ trades/                             (ticks E0)
‚îÇ       ‚îú‚îÄ‚îÄ AAPL/date=2004-01-05/trades.parquet
‚îÇ       ‚îî‚îÄ‚îÄ ... (~150,000 ticker-d√≠as)
‚îÇ
‚îú‚îÄ‚îÄ processed/
‚îÇ   ‚îú‚îÄ‚îÄ ref/market_cap_dim/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ market_cap_dim.parquet         (SCD-2 temporal)
‚îÇ   ‚îú‚îÄ‚îÄ daily_cache/                        (cach√© diario agregado)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ticker=AAPL/daily.parquet
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ... (3,107 tickers)
‚îÇ   ‚îî‚îÄ‚îÄ universe/info_rich/
‚îÇ       ‚îú‚îÄ‚îÄ daily/                          (watchlists E0)
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ date=2004-01-02/watchlist.parquet
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ ... (~5,292 d√≠as)
‚îÇ       ‚îú‚îÄ‚îÄ topN_12m.parquet
‚îÇ       ‚îî‚îÄ‚îÄ topN_12m.csv
‚îÇ
‚îú‚îÄ‚îÄ audits/
‚îÇ   ‚îú‚îÄ‚îÄ AUDITORIA_INCLUSION_C_v1_vs_E0.json
‚îÇ   ‚îú‚îÄ‚îÄ only_C_v1_midcaps.parquet
‚îÇ   ‚îî‚îÄ‚îÄ only_E0_pennystocks.parquet
‚îÇ
‚îî‚îÄ‚îÄ 01_DayBook/fase_01/C_v2_ingesta_tiks_2004_2025/
    ‚îú‚îÄ‚îÄ Contrato_E0.md                      (contrato inmutable v2.0.0)
    ‚îú‚îÄ‚îÄ C.4_anotacion_descarga_tiks_daily.md
    ‚îî‚îÄ‚îÄ C.5_plan_ejecucion_E0_descarga_ticks.md  (este documento)
```

---

## 6. PR√ìXIMOS PASOS POST-DESCARGA

Una vez completada la descarga de ticks E0:

1. **Construcci√≥n de Barras Informativas** (DIB/VIB):
   - Dollar Imbalance Bars
   - Volume Imbalance Bars
   - Tick Imbalance Bars

2. **Triple Barrier Labeling**:
   - Profit target / stop loss
   - Time-based exit
   - Meta-labeling

3. **Feature Engineering**:
   - Microestructura (spread, VWAP deviation, order flow)
   - Volume Profile
   - Tick-level features

4. **Sample Weighting**:
   - Bootstrap weights
   - Sequential bootstrap
   - Time decay

5. **Incorporaci√≥n eventos E1-E13**:
   - Parabolic (E4)
   - First Red Day (E7)
   - Dilution (E9)
   - etc.

6. **Dataset Maestro 2004-2025**:
   - Unificaci√≥n E0 ‚à™ E1 ‚à™ ... ‚à™ E13
   - Particionado por r√©gimen
   - Versionado para ML

---

## 6. EVENTOS FUTUROS E1-E17 (NO IMPLEMENTADOS A√öN)

### 6.1 Estado Actual vs. Futuro

**ACTUALMENTE IMPLEMENTADO** (C_v2 Fase 1 - PASO 5 en ejecuci√≥n):
```
‚úÖ E0 (Generic Info-Rich)
   - RVOL ‚â• 2.0
   - |%chg| ‚â• 15%
   - $vol ‚â• $5M
   - Precio $0.20-$20
   - Event window: ¬±1 d√≠a (3 d√≠as total)
   - Status: DESCARGANDO AHORA
   - Archivos: raw/polygon/trades/{TICKER}/date={YYYY-MM-DD}/
```

**PLANIFICADO PARA FUTURO** (C_v2 Fases 2-N):

Seg√∫n [C.1_estrategia_descarga_ticks_eventos.md](C.1_estrategia_descarga_ticks_eventos.md):

| Evento | Descripci√≥n | Ventana | D√≠as/Evento | Status |
|--------|-------------|---------|-------------|--------|
| E1 | Volume Explosion (RVOL > 5√ó) | ‚àí1 ‚Üí +2 | 4 d√≠as | ‚è≥ Pendiente |
| E2 | Gap Up > 10% | ‚àí1 ‚Üí +1 | 3 d√≠as | ‚è≥ Pendiente |
| E3 | Spike Intraday | mismo d√≠a | 1 d√≠a | ‚è≥ Pendiente |
| E4 | Parabolic Move (+50% en ‚â§5 d√≠as) | ‚àí2 ‚Üí +3 | 6 d√≠as | ‚è≥ Pendiente |
| E5 | Breakout ATH | ‚àí1 ‚Üí +2 | 4 d√≠as | ‚è≥ Pendiente |
| E7 | First Red Day (FRD) | ‚àí1 ‚Üí +2 | 4 d√≠as | ‚è≥ Pendiente |
| E8 | Gap Down > 15% | ‚àí1 ‚Üí +1 | 3 d√≠as | ‚è≥ Pendiente |
| E10 | First Green Bounce | ‚àí1 ‚Üí +1 | 3 d√≠as | ‚è≥ Pendiente |
| E12-E14 | Dilution events | ‚àí2 ‚Üí +2 | 5 d√≠as | ‚è≥ Pendiente |
| E15-E17 | Halts, SSR, spread | mismo d√≠a | 1 d√≠a | ‚è≥ Pendiente |

**Promedio ponderado** (seg√∫n frecuencia): ~3‚Äì5 d√≠as por evento

### 6.2 Pregunta Clave: ¬øD√≥nde se Guardar√°n los Otros E's?

**Respuesta**: Cuando implementes E1-E17, tienes **DOS opciones arquitect√≥nicas**:

#### Opci√≥n A: Watchlists Separados por Tipo de Evento (NO RECOMENDADA)

```
processed/universe/
‚îú‚îÄ‚îÄ E0_generic_info_rich/
‚îÇ   ‚îî‚îÄ‚îÄ daily/
‚îÇ       ‚îî‚îÄ‚îÄ date=2020-03-16/
‚îÇ           ‚îî‚îÄ‚îÄ watchlist.parquet (info_rich=True)
‚îÇ
‚îú‚îÄ‚îÄ E1_volume_explosion/
‚îÇ   ‚îî‚îÄ‚îÄ daily/
‚îÇ       ‚îî‚îÄ‚îÄ date=2020-03-16/
‚îÇ           ‚îî‚îÄ‚îÄ watchlist.parquet (volume_explosion=True, RVOL>5.0)
‚îÇ
‚îú‚îÄ‚îÄ E2_gap_up/
‚îÇ   ‚îî‚îÄ‚îÄ daily/...
‚îÇ
‚îî‚îÄ‚îÄ E4_parabolic/
    ‚îî‚îÄ‚îÄ daily/...

raw/polygon/trades/
‚îú‚îÄ‚îÄ E0/
‚îÇ   ‚îî‚îÄ‚îÄ BCRX/date=2020-03-16/trades.parquet  ‚Üê Duplica ticks
‚îú‚îÄ‚îÄ E1/
‚îÇ   ‚îî‚îÄ‚îÄ BCRX/date=2020-03-16/trades.parquet  ‚Üê Duplica ticks
‚îî‚îÄ‚îÄ E4/
    ‚îî‚îÄ‚îÄ BCRX/date=2020-03-16/trades.parquet  ‚Üê Duplica ticks
```

**Ventajas**:
- Separaci√≥n clara por tipo de evento
- F√°cil auditar cada evento independientemente

**Desventajas**:
- ‚ùå **Duplicaci√≥n de ticks**: Un ticker-d√≠a con E0+E1+E4 ‚Üí 3√ó storage
- ‚ùå Ineficiente en disco (un ticker puede tener m√∫ltiples eventos simult√°neos)
- ‚ùå Confuso para downstream analysis

#### Opci√≥n B: Columnas Multi-Evento en UN SOLO Watchlist (‚úÖ RECOMENDADA)

```
processed/universe/multi_event/
‚îî‚îÄ‚îÄ daily/
    ‚îî‚îÄ‚îÄ date=2020-03-16/
        ‚îî‚îÄ‚îÄ watchlist.parquet
            Columnas:
            - ticker
            - trading_day
            - E0_info_rich: bool
            - E1_volume_explosion: bool
            - E2_gap_up: bool
            - E4_parabolic: bool
            - E5_breakout_ath: bool
            - E7_first_red: bool
            - E8_gap_down: bool
            - E10_first_green: bool
            - E12_dilution: bool
            - E15_halt: bool
            - event_types: List[str]  ‚Üê ["E0", "E4"] para multi-events
            - max_event_window: int   ‚Üê Max(ventana E0, E4) = max(3, 6) = 6

raw/polygon/trades/
‚îî‚îÄ‚îÄ BCRX/
    ‚îî‚îÄ‚îÄ date=2020-03-16/
        ‚îú‚îÄ‚îÄ trades.parquet    ‚Üê UN solo archivo (sin duplicados)
        ‚îú‚îÄ‚îÄ _SUCCESS
        ‚îî‚îÄ‚îÄ events.json       ‚Üê Metadata: {"events": ["E0", "E4"], "windows": {"E0": 3, "E4": 6}}
```

**Ejemplo de fila en watchlist**:
```python
{
  'ticker': 'BCRX',
  'trading_day': '2020-03-16',
  'E0_info_rich': True,          # ‚úì Cumple E0 (RVOL‚â•2.0, |%chg|‚â•15%)
  'E1_volume_explosion': False,
  'E2_gap_up': False,
  'E4_parabolic': True,          # ‚úì Cumple E4 (+50% en ‚â§5 d√≠as)
  'E5_breakout_ath': False,
  'event_types': ['E0', 'E4'],   # Multi-evento simult√°neo
  'max_event_window': 6          # Max de (E0=3, E4=6) = 6 d√≠as
}
```

**Ventajas**:
- ‚úÖ **UN solo archivo de ticks por ticker-d√≠a** (sin duplicados)
- ‚úÖ Eficiente en storage (~3-5x menos espacio vs Opci√≥n A)
- ‚úÖ Puedes filtrar por cualquier combinaci√≥n de eventos
- ‚úÖ Downstream analysis m√°s f√°cil (un ticker-d√≠a = un archivo)
- ‚úÖ Metadata clara sobre qu√© eventos aplican

**Desventaja**:
- Watchlist m√°s complejo (13+ columnas booleanas)

### 6.3 Implementaci√≥n Futura Recomendada

Cuando implementes E1-E17, sigue estos pasos:

#### Paso 1: Modificar `build_dynamic_universe_optimized.py`

Agregar c√°lculo de TODAS las E's en un solo pass:

```python
def calculate_all_events(df: pl.DataFrame) -> pl.DataFrame:
    """
    Calcula todos los eventos E0-E17 en un solo pass del daily_cache.
    """
    return df.with_columns([
        # E0: Generic Info-Rich (ya implementado)
        ((pl.col('rvol30') >= 2.0) &
         (pl.col('pctchg_d').abs() >= 0.15) &
         (pl.col('dollar_vol_d') >= 5_000_000) &
         (pl.col('close_d') >= 0.20) &
         (pl.col('close_d') <= 20.00)).alias('E0_info_rich'),

        # E1: Volume Explosion
        (pl.col('rvol30') >= 5.0).alias('E1_volume_explosion'),

        # E2: Gap Up > 10%
        (pl.col('gap_pct') > 0.10).alias('E2_gap_up'),

        # E4: Parabolic Move (+50% en ‚â§5 d√≠as)
        (pl.col('change_5d') >= 0.50).alias('E4_parabolic'),

        # E5: Breakout ATH
        (pl.col('close_d') >= pl.col('ath_252d')).alias('E5_breakout_ath'),

        # ... E7, E8, E10, E12-E17

        # Metadata: lista de eventos activos
        pl.when(pl.col('E0_info_rich')).then(pl.lit('E0')).otherwise(pl.lit(None)).alias('e0_flag'),
        pl.when(pl.col('E1_volume_explosion')).then(pl.lit('E1')).otherwise(pl.lit(None)).alias('e1_flag'),
        # ... agregar todos los flags
    ]).with_columns([
        # Consolidar eventos en una lista
        pl.concat_list(['e0_flag', 'e1_flag', 'e4_flag', ...])
          .list.drop_nulls()
          .alias('event_types'),

        # Calcular ventana m√°xima
        pl.when(pl.col('E4_parabolic')).then(pl.lit(6))
          .when(pl.col('E5_breakout_ath')).then(pl.lit(4))
          .when(pl.col('E1_volume_explosion')).then(pl.lit(4))
          .when(pl.col('E0_info_rich')).then(pl.lit(3))
          .otherwise(pl.lit(1))
          .alias('max_event_window')
    ])
```

#### Paso 2: Modificar `download_trades_optimized.py`

Agregar soporte para multi-eventos:

```python
def load_multi_event_days(watchlist_root: Path, dfrom: date, dto: date,
                          event_types: List[str], allowed_tickers: Optional[set]) -> Dict[str, List[date]]:
    """
    Lee watchlists multi-evento y expande seg√∫n ventana m√°xima por ticker-d√≠a.

    Args:
        event_types: ['E0', 'E1', 'E4'] - qu√© eventos descargar
        Si None, descarga TODOS los eventos encontrados
    """
    days_by_ticker = {}

    for watchlist_path in sorted(watchlist_root.glob('date=*/watchlist.parquet')):
        df = pl.read_parquet(watchlist_path)
        day = datetime.strptime(watchlist_path.parent.name.split('=')[1], '%Y-%m-%d').date()

        # Filtrar por eventos solicitados (si se especifican)
        if event_types:
            event_filter = pl.lit(False)
            for event_type in event_types:
                event_filter = event_filter | pl.col(f'{event_type}_event')
            df_events = df.filter(event_filter)
        else:
            # Cualquier ticker con al menos un evento activo
            df_events = df.filter(pl.col('event_types').list.len() > 0)

        # Expandir seg√∫n max_event_window de cada ticker-d√≠a
        for row in df_events.iter_rows(named=True):
            ticker = row['ticker']
            if allowed_tickers and ticker not in allowed_tickers:
                continue

            max_window = row['max_event_window']

            # Expandir ventana: [day - max_window, day, day + max_window]
            for offset in range(-max_window, max_window + 1):
                expanded_day = day + timedelta(days=offset)
                if expanded_day >= dfrom and expanded_day <= dto:
                    days_by_ticker.setdefault(ticker, []).append(expanded_day)

    # Deduplicar y ordenar fechas
    for ticker in days_by_ticker:
        days_by_ticker[ticker] = sorted(set(days_by_ticker[ticker]))

    return days_by_ticker

# Agregar argumento --event-types al argparse
ap.add_argument("--event-types", type=str, default=None,
                help="Comma-separated list de eventos a descargar (ej: 'E0,E1,E4'). "
                     "Si no se especifica, descarga TODOS los eventos detectados.")
```

#### Paso 3: Guardar Metadata de Eventos

Al descargar cada ticker-d√≠a, guardar metadata:

```python
def download_span_with_metadata(ticker: str, day: date, watchlist_data: dict, ...):
    """
    Descarga ticks + guarda metadata de eventos.
    """
    # ... descargar ticks como siempre ...

    # Guardar metadata de eventos
    events_json = {
        'ticker': ticker,
        'date': str(day),
        'events': watchlist_data['event_types'],  # ["E0", "E4"]
        'event_windows': {
            'E0': 3,
            'E4': 6
        },
        'max_window_used': watchlist_data['max_event_window']
    }

    events_path = day_path / 'events.json'
    with open(events_path, 'w') as f:
        json.dump(events_json, f, indent=2)
```

### 6.4 Estructura Final de Datos (Multi-Evento)

```
raw/polygon/trades/
‚îî‚îÄ‚îÄ BCRX/
    ‚îú‚îÄ‚îÄ date=2020-03-15/          ‚Üê Ventana pre-evento
    ‚îÇ   ‚îú‚îÄ‚îÄ trades.parquet
    ‚îÇ   ‚îú‚îÄ‚îÄ _SUCCESS
    ‚îÇ   ‚îî‚îÄ‚îÄ events.json           ‚Üê {"events": ["E0", "E4"], "reason": "window"}
    ‚îú‚îÄ‚îÄ date=2020-03-16/          ‚Üê D√çA DEL EVENTO
    ‚îÇ   ‚îú‚îÄ‚îÄ trades.parquet
    ‚îÇ   ‚îú‚îÄ‚îÄ _SUCCESS
    ‚îÇ   ‚îî‚îÄ‚îÄ events.json           ‚Üê {"events": ["E0", "E4"], "reason": "event_day"}
    ‚îú‚îÄ‚îÄ date=2020-03-17/          ‚Üê Ventana post-evento (day+1)
    ‚îÇ   ‚îú‚îÄ‚îÄ trades.parquet
    ‚îÇ   ‚îú‚îÄ‚îÄ _SUCCESS
    ‚îÇ   ‚îî‚îÄ‚îÄ events.json           ‚Üê {"events": ["E0", "E4"], "reason": "window"}
    ‚îú‚îÄ‚îÄ date=2020-03-18/          ‚Üê Ventana extendida (E4 requiere hasta day+3)
    ‚îÇ   ‚îú‚îÄ‚îÄ trades.parquet
    ‚îÇ   ‚îú‚îÄ‚îÄ _SUCCESS
    ‚îÇ   ‚îî‚îÄ‚îÄ events.json           ‚Üê {"events": ["E4"], "reason": "window_E4"}
    ‚îî‚îÄ‚îÄ ...
```

### 6.5 Uso Downstream (Multi-Evento)

```python
import polars as pl
from pathlib import Path

# Cargar todos los eventos
watchlists = pl.scan_parquet('processed/universe/multi_event/daily/**/*.parquet')

# Filtrar por eventos de inter√©s
e0_and_e4_events = watchlists.filter(
    pl.col('E0_info_rich') & pl.col('E4_parabolic')
).collect()

print(f"Tickers con E0+E4 simult√°neos: {len(e0_and_e4_events)}")

# Cargar ticks para un evento
for row in e0_and_e4_events.head(10).iter_rows(named=True):
    ticker = row['ticker']
    event_date = row['trading_day']

    # Cargar ticks del d√≠a del evento
    ticks_path = f'raw/polygon/trades/{ticker}/date={event_date}/trades.parquet'
    if Path(ticks_path).exists():
        ticks = pl.read_parquet(ticks_path)
        print(f"{ticker} {event_date}: {len(ticks):,} ticks - Eventos: {row['event_types']}")
```

### 6.6 Resumen: Respuesta a "¬øD√≥nde se Guardar√°n los Otros E's?"

1. **Ahora mismo**: NO SE GUARDAN porque E1-E17 no est√°n implementados (solo E0 existe)

2. **Cuando los implementes** (recomendaci√≥n):
   - **Watchlists**: UN solo archivo diario con columnas booleanas por cada evento
   - **Ticks**: UN solo archivo por ticker-d√≠a (sin duplicados)
   - **Metadata**: `events.json` indica qu√© eventos aplican a cada ticker-d√≠a
   - **Sin duplicaci√≥n**: Si BCRX tiene E0+E1+E4 el 2020-03-16 ‚Üí un solo `trades.parquet`

3. **Beneficios**:
   - Eficiencia de storage (~3-5x menos espacio)
   - An√°lisis downstream simplificado
   - Trazabilidad completa (metadata en JSON)
   - Flexible: puedes filtrar por cualquier combinaci√≥n de eventos

---

## 7. COMANDOS R√ÅPIDOS (RESUMEN)

```bash
# PASO 0: SCD-2 - DEPRECADO (solo cubre 2025-10-19‚Üí)
# NO ejecutar

# PASO 1: Cach√© diario (SIN market_cap)
python scripts/fase_C_ingesta_tiks/build_daily_cache.py \
  --intraday-root raw/polygon/ohlcv_intraday_1m \
  --outdir processed/daily_cache \
  --from 2004-01-01 --to 2025-10-21 \
  --parallel 8
  # SIN --cap-filter-parquet

# PASO 2: Actualizar config (remover cap_max)
# Editar: configs/universe_config.yaml
# Comentar o eliminar: cap_max: 2_000_000_000

# PASO 3: Universo E0 (sin filtro market_cap)
python scripts/fase_C_ingesta_tiks/build_dynamic_universe_optimized.py \
  --daily-cache processed/daily_cache \
  --outdir processed/universe/info_rich \
  --from 2004-01-01 --to 2025-10-21 \
  --config configs/universe_config.yaml

# PASO 4: Verificaci√≥n (script a crear)
python scripts/fase_C_ingesta_tiks/verify_inclusion_C_v1_vs_E0.py \
  --c_v1-watchlists processed/universe/info_rich_v1/daily \
  --e0-watchlists processed/universe/info_rich/daily \
  --from 2020-01-01 --to 2025-10-21 \
  --outdir audits

# PASO 5: Descarga ticks
python scripts/fase_C_ingesta_tiks/download_trades_optimized.py \
  --tickers-csv processed/universe/info_rich/topN_12m.csv \
  --watchlist-root processed/universe/info_rich/daily \
  --outdir raw/polygon/trades \
  --from 2004-01-01 --to 2025-10-21 \
  --mode watchlists \
  --page-limit 50000 \
  --rate-limit 0.15 \
  --workers 8 \
  --resume
```

---

## 8. NOTAS FINALES

### Validaci√≥n conceptual
Este plan ejecuta la estrategia documentada en Contrato_E0.md v2.0.0:
- ‚úÖ E0 como "pegamento legal" entre C_v1 y C_v2
- ‚úÖ Filtros coherentes con mandato Small/MicroCap
- ‚úÖ Documentaci√≥n de diferencias C_v1 vs E0
- ‚úÖ Trazabilidad completa de exclusiones/inclusiones

### Garant√≠a de coherencia
- Pipeline √∫nico unificado (no C_v1 + C_v2 separados)
- Versionado formal (Contrato v2.0.0)
- Auditor√≠a emp√≠rica de inclusi√≥n
- Justificaci√≥n documentada de diferencias

### Escalabilidad a 21 a√±os
- SCD-2 para market cap hist√≥rico correcto
- Join temporal evita survivorship bias
- Watchlists por d√≠a permiten descarga selectiva
- Resume tolerance para ejecuciones largas

### Listo para ML
- Features pre-calculadas (rvol30, pctchg_d)
- Ticks listos para barras informativas
- Etiquetas por evento (E0, E1, ..., E13)
- Particionado por r√©gimen macro

---

**Documento creado**: 2025-10-25
**Autor**:  Alex Just Rodriguez
**Versi√≥n**: 1.0.0
**Basado en**: Contrato_E0.md v2.0.0
**Status**: LISTO PARA EJECUCI√ìN

**FIN DEL PLAN DE EJECUCI√ìN**
