# 3.3 Splits & Dividends - Datos Globales Ya Descargados

**Fecha**: 2025-10-24
**Estado**: ‚úÖ DATOS GLOBALES DESCARGADOS (2025-10-19/20) - Listos para reutilizar
**Cobertura**: 8,686 tickers de nuestro universo cubiertos

---

## üîç An√°lisis de Splits & Dividends - Reutilizaci√≥n de Datos

### ‚úÖ **CONCLUSI√ìN: S√ç se pueden reutilizar estos datos**

Los datos de **splits y dividends** descargados el 2025-10-19/20 son **GLOBALES de toda la API de Polygon**, NO fueron filtrados para un universo espec√≠fico.

---

### üìä Estad√≠sticas de los Datos Descargados

**Endpoint usado**: `/v3/reference/splits` y `/v3/reference/dividends` (sin filtros)

| Dataset | Total Registros | Tickers √önicos | Per√≠odo |
|---------|----------------|----------------|---------|
| **Splits** | 26,641 | 18,423 | 1978-2025 |
| **Dividends** | 1,878,357 | 75,198 | 2000-2030 |

**Caracter√≠sticas**:
- ‚úÖ **Descarga global** - Incluye TODOS los tickers de Polygon (stocks, ETFs, fondos, ADRs, etc.)
- ‚úÖ **Sin filtro de exchange** - Incluye NASDAQ, NYSE, ARCA, OTC, etc.
- ‚úÖ **Sin filtro de tipo** - Incluye CS, ETF, FUND, ADR, WARRANT, etc.
- ‚úÖ **Hist√≥rico completo** - Desde 1978 (splits) y 2000 (dividends) hasta 2030

---

### üéØ Cobertura de Nuestro Universo (8,686 tickers)

**Universo actual**: `cs_xnas_xnys_hybrid_enriched_2025-10-24.parquet`

| M√©trica | Valor | Porcentaje |
|---------|-------|------------|
| **Tickers en nuestro universo** | 8,679 | 100% |
| **Con splits disponibles** | 2,420 | 27.9% |
| **Con dividends disponibles** | 2,723 | 31.4% |
| **SIN splits** | 6,259 | 72.1% |
| **SIN dividends** | 5,956 | 68.6% |

---

### üí° Interpretaci√≥n de los Datos

**¬øPor qu√© solo 27.9% tiene splits y 31.4% dividends?**

1. **Inactivos (5,594 tickers - 64.4% del universo)**:
   - Muchos fueron delisted ANTES de hacer splits
   - Muchos nunca pagaron dividends (small caps que fracasaron)
   - Es ESPERADO que no tengan datos

2. **Activos small caps (3,092 tickers - 35.6%)**:
   - **Small caps rara vez pagan dividends** (reinvierten capital)
   - **Splits son menos comunes** en < $2B market cap
   - Es NORMAL que solo ~30% tenga estos eventos

3. **Datos hist√≥ricos completos**:
   - Los datos van desde 1978 (splits) y 2000 (dividends)
   - Si un ticker de nuestro universo tuvo alg√∫n split/dividend en TODA su historia, est√° incluido

---

### ‚úÖ Recomendaci√≥n: **REUTILIZAR estos datos**

**Razones**:

1. ‚úÖ **Cobertura completa**: Los 8,686 tickers de nuestro universo est√°n cubiertos (si tienen splits/dividends, est√°n en los datos)

2. ‚úÖ **Datos globales**: No hay necesidad de volver a descargar, ya que NO fueron filtrados por universo espec√≠fico

3. ‚úÖ **Hist√≥rico completo**: 47 a√±os de splits (1978-2025) y 31 a√±os de dividends (2000-2030)

4. ‚úÖ **Ahorro de tiempo**: Evitas ~2 horas de descarga (especialmente dividends con 1.8M registros)

5. ‚úÖ **Datos validados**: Ya pasaron por limpieza, de-duplicaci√≥n y particionamiento por a√±o

---

### üìÅ Ubicaci√≥n de los Datos

- **Splits**: `raw/polygon/reference/splits/year=*/splits.parquet`
- **Dividends**: `raw/polygon/reference/dividends/year=*/dividends.parquet`
- **Particionamiento**: Por a√±o (31 particiones cada uno)

---

### ‚ö†Ô∏è √önica Consideraci√≥n

Si quieres datos **M√ÅS RECIENTES** de splits/dividends posteriores a octubre 2025, tendr√≠as que volver a descargar. Pero para backtesting hist√≥rico, estos datos son **PERFECTOS y COMPLETOS**.

---

## üìã Script de Referencia Original (Ya Ejecutado - NO volver a ejecutar)

El siguiente script fue usado para la descarga original del 2025-10-19/20. Los datos ya est√°n descargados.

```python
#!/usr/bin/env python
# ingest_splits_dividends.py
import os, sys, time, argparse, datetime as dt
from pathlib import Path
import requests, polars as pl

BASE_URL = "https://api.polygon.io"
LIMIT = 1000
TIMEOUT = 25

def log(msg): print(f"[{dt.datetime.now():%F %T}] {msg}", flush=True)

def http_get(url, api_key, params=None):
    headers = {"Authorization": f"Bearer {api_key}"}
    for k in range(8):
        try:
            r = requests.get(url, headers=headers, params=params or {}, timeout=TIMEOUT)
            if r.status_code == 429:
                time.sleep(int(r.headers.get("Retry-After", "2")))
                continue
            if 500 <= r.status_code < 600:
                time.sleep(1.6 ** k)
                continue
            r.raise_for_status()
            return r.json()
        except Exception:
            time.sleep(1.6 ** k)
    return {}

def fetch_paged(path, api_key, extra_params=None):
    url = f"{BASE_URL}{path}"
    params = {"limit": LIMIT}
    if extra_params: params.update(extra_params)
    cursor = None
    total = 0
    while True:
        p = params.copy()
        if cursor: p["cursor"] = cursor
        data = http_get(url, api_key, p) or {}
        res = data.get("results") or []
        for x in res: yield x
        total += len(res)
        cursor = data.get("next_url_cursor") or data.get("cursor") or data.get("next_cursor")
        if not cursor: break
    log(f"{path}: {total:,} filas")

def clean_splits(df: pl.DataFrame) -> pl.DataFrame:
    if df.height == 0: return df
    cast = {
        "ticker": pl.Utf8, "execution_date": pl.Utf8,
        "split_from": pl.Float64, "split_to": pl.Float64,
        "declared_date": pl.Utf8
    }
    for c,t in cast.items():
        if c in df.columns: df = df.with_columns(pl.col(c).cast(t))
    if all(c in df.columns for c in ("split_from","split_to")):
        df = df.with_columns((pl.col("split_from")/pl.col("split_to")).alias("ratio"))
    if "execution_date" in df.columns:
        df = df.sort(["ticker","execution_date"]).unique(subset=["ticker","execution_date","split_from","split_to"], keep="last")
    return df

def clean_dividends(df: pl.DataFrame) -> pl.DataFrame:
    if df.height == 0: return df
    cast = {
        "ticker": pl.Utf8, "ex_dividend_date": pl.Utf8,
        "cash_amount": pl.Float64, "declaration_date": pl.Utf8,
        "record_date": pl.Utf8, "payable_date": pl.Utf8,
        "frequency": pl.Utf8, "dividend_type": pl.Utf8
    }
    for c,t in cast.items():
        if c in df.columns: df = df.with_columns(pl.col(c).cast(t))
    if "ex_dividend_date" in df.columns and "cash_amount" in df.columns:
        df = df.sort(["ticker","ex_dividend_date"]).unique(subset=["ticker","ex_dividend_date","cash_amount"], keep="last")
    return df

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--outdir", required=True, help="Base de salida raw/polygon/reference")
    args = ap.parse_args()

    api_key = os.getenv("POLYGON_API_KEY")
    if not api_key: sys.exit("Falta POLYGON_API_KEY")

    base = Path(args.outdir); base.mkdir(parents=True, exist_ok=True)

    # Splits
    splits = list(fetch_paged("/v3/reference/splits", api_key))
    df_s = clean_splits(pl.from_dicts(splits) if splits else pl.DataFrame())
    if df_s.height:
        # particiona por a√±o de execution_date
        df_s = df_s.with_columns(pl.col("execution_date").str.slice(0,4).alias("year"))
        for year, part in df_s.group_by("year"):
            outdir = base / "splits" / f"year={year}"
            outdir.mkdir(parents=True, exist_ok=True)
            part.drop("year").write_parquet(outdir / "splits.parquet")
        log(f"Splits escritos en {base/'splits'}")

    # Dividends
    dividends = list(fetch_paged("/v3/reference/dividends", api_key))
    df_d = clean_dividends(pl.from_dicts(dividends) if dividends else pl.DataFrame())
    if df_d.height:
        df_d = df_d.with_columns(pl.col("ex_dividend_date").str.slice(0,4).alias("year"))
        for year, part in df_d.group_by("year"):
            outdir = base / "dividends" / f"year={year}"
            outdir.mkdir(parents=True, exist_ok=True)
            part.drop("year").write_parquet(outdir / "dividends.parquet")
        log(f"Dividends escritos en {base/'dividends'}")

if __name__ == "__main__":
    main()
```

**Ejemplo:**

```bash
python ingest_splits_dividends.py --outdir raw/polygon/reference
```

---

# 3) Construcci√≥n de `tickers_dim` (SCD-2)

Este script compara **dos snapshots** consecutivos y actualiza la dimensi√≥n con ventanas `effective_from / effective_to`. Si a√∫n no tienes dimensi√≥n, la crea con el snapshot actual.

```python
#!/usr/bin/env python
# build_tickers_dim_scd2.py
import argparse, datetime as dt
from pathlib import Path
import polars as pl

KEY_COLS = ["ticker"]  # business key
TRACK_COLS = [
  "name","primary_exchange","active","market","locale","type",
  "currency_name","composite_figi","share_class_figi",
  "sector","industry","sic_code","cik","list_date","delisted_utc"
]

def log(m): print(f"[{dt.datetime.now():%F %T}] {m}", flush=True)

def load_snapshot(snapdir: Path) -> pl.DataFrame:
    df = pl.read_parquet(snapdir / "tickers.parquet")
    # aseguramos que las columnas existen
    for c in TRACK_COLS:
        if c not in df.columns: df = df.with_columns(pl.lit(None).alias(c))
    return df

def initial_dim(snapshot: pl.DataFrame) -> pl.DataFrame:
    snap_date = snapshot["snapshot_date"][0]
    return (snapshot
        .select(KEY_COLS + TRACK_COLS + ["snapshot_date"])
        .with_columns([
            pl.col("snapshot_date").alias("effective_from"),
            pl.lit(None, dtype=pl.Utf8).alias("effective_to")
        ])
        .drop("snapshot_date")
    )

def scd2_merge(dim: pl.DataFrame, prev_snap: pl.DataFrame, curr_snap: pl.DataFrame) -> pl.DataFrame:
    # join prev vs curr por KEY_COLS
    on = KEY_COLS
    prev = prev_snap.select(KEY_COLS + TRACK_COLS + ["snapshot_date"]).rename({c:f"prev_{c}" for c in TRACK_COLS+["snapshot_date"]})
    curr = curr_snap.select(KEY_COLS + TRACK_COLS + ["snapshot_date"]).rename({c:f"curr_{c}" for c in TRACK_COLS+["snapshot_date"]})
    j = prev.join(curr, on=on, how="outer")

    # detecta cambios (cualquiera de TRACK_COLS)
    def any_change(row) -> bool:
        for c in TRACK_COLS:
            if row[f"prev_{c}"] != row[f"curr_{c}"]:
                return True
        return False

    # filas que nacen en curr pero no estaban en prev ‚Üí nuevas altas
    new_mask = j["prev_name"].is_null() & j["curr_name"].is_not_null()
    new_rows = (j.filter(new_mask)
                  .select(on + [f"curr_{c}" for c in TRACK_COLS] + ["curr_snapshot_date"])
                  .rename({f"curr_{c}": c for c in TRACK_COLS} | {"curr_snapshot_date":"effective_from"})
                  .with_columns(pl.lit(None, dtype=pl.Utf8).alias("effective_to")))

    # filas que estaban en prev y cambian en curr ‚Üí cerrar antiguo y abrir nuevo
    change_mask = (~j["prev_name"].is_null()) & (~j["curr_name"].is_null())
    changed = j.filter(change_mask)

    # registros con cambio en TRACK_COLS
    changed = changed.with_columns(
        pl.any_horizontal([pl.col(f"prev_{c}") != pl.col(f"curr_{c}") for c in TRACK_COLS]).alias("changed")
    )

    # cerrar antiguos
    to_close = (changed.filter(pl.col("changed"))
                .select(on + ["prev_snapshot_date"])
                .rename({"prev_snapshot_date":"effective_to"}))

    # abrir nuevos
    to_open = (changed.filter(pl.col("changed"))
               .select(on + [f"curr_{c}" for c in TRACK_COLS] + ["curr_snapshot_date"])
               .rename({f"curr_{c}": c for c in TRACK_COLS} | {"curr_snapshot_date":"effective_from"})
               .with_columns(pl.lit(None, dtype=pl.Utf8).alias("effective_to")))

    # aplicar cierre a dim existente (match por key y effective_to is null)
    if to_close.height:
        dim_open = dim.filter(pl.col("effective_to").is_null())
        dim_closed = dim.filter(pl.col("effective_to").is_not_null())
        dim_open = dim_open.join(to_close, on=on, how="left")
        dim_open = dim_open.with_columns(
            pl.when(pl.col("effective_to").is_null() & pl.col("effective_to_right").is_not_null())
              .then(pl.col("effective_to_right"))
              .otherwise(pl.col("effective_to")).alias("effective_to")
        ).drop("effective_to_right")
        dim = pl.concat([dim_closed, dim_open], how="vertical_relaxed")

    # a√±adir nuevos y nuevas altas
    additions = pl.concat([to_open, new_rows], how="vertical_relaxed") if (to_open.height or new_rows.height) else None
    if additions is not None and additions.height:
        dim = pl.concat([dim, additions], how="vertical_relaxed")

    return dim.sort(on + ["effective_from"])

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--dimdir", required=True, help="Salida: processed/ref/tickers_dim")
    ap.add_argument("--prev-snapshot", required=False, help="raw/polygon/reference/tickers_snapshot/snapshot_date=YYYY-MM-DD (opcional la 1¬™ vez)")
    ap.add_argument("--curr-snapshot", required=True, help="raw/polygon/reference/tickers_snapshot/snapshot_date=YYYY-MM-DD")
    args = ap.parse_args()

    dimdir = Path(args.dimdir); dimdir.mkdir(parents=True, exist_ok=True)

    curr = load_snapshot(Path(args.curr_snapshot))
    if args.prev_snapshot:
        prev = load_snapshot(Path(args.prev_snapshot))
        # cargar dim si existe; si no, in√≠ciala con prev
        dim_path = dimdir / "tickers_dim.parquet"
        if dim_path.exists():
            dim = pl.read_parquet(dim_path)
        else:
            dim = initial_dim(prev)
        dim = scd2_merge(dim, prev, curr)
    else:
        dim = initial_dim(curr)

    out = dimdir / "tickers_dim.parquet"
    dim.write_parquet(out)
    print(f"Escrito: {out} ({dim.height:,} filas)")

if __name__ == "__main__":
    from pathlib import Path
    main()
```

**Ejemplos:**

```bash
# 1¬™ vez (sin snapshot previo)
python build_tickers_dim_scd2.py \
  --dimdir processed/ref/tickers_dim \
  --curr-snapshot raw/polygon/reference/tickers_snapshot/snapshot_date=2025-10-19

# Siguientes d√≠as (con snapshots N-1 y N)
python build_tickers_dim_scd2.py \
  --dimdir processed/ref/tickers_dim \
  --prev-snapshot raw/polygon/reference/tickers_snapshot/snapshot_date=2025-10-18 \
  --curr-snapshot raw/polygon/reference/tickers_snapshot/snapshot_date=2025-10-19
```

---

## üéØ Filtrado para Nuestro Universo de 8,686 Tickers - EJECUTADO

**Fecha de ejecuci√≥n**: 2025-10-24
**Script**: [`filter_splits_dividends_universe.py`](../../../scripts/fase_A_universo/filter_splits_dividends_universe.py)

### Proceso Ejecutado

Los datos globales de splits y dividends (26,641 y 1,878,357 registros respectivamente) fueron **filtrados** para extraer √∫nicamente los registros de nuestro universo h√≠brido de 8,686 tickers.

```bash
python scripts/fase_A_universo/filter_splits_dividends_universe.py
```

### Resultados del Filtrado

**De datos globales ‚Üí A nuestro universo (8,679 tickers)**

| Dataset | Registros Globales | Filtrados | Reducci√≥n |
|---------|-------------------|-----------|-----------|
| **Splits** | 26,641 | 4,012 | 84.9% |
| **Dividends** | 1,878,357 | 94,546 | 95.0% |

### Cobertura del Universo

| M√©trica | Valor | Porcentaje |
|---------|-------|------------|
| **Tickers con splits** | 2,420 | 27.9% |
| **Tickers con dividends** | 2,723 | 31.4% |
| **Tickers SIN splits** | 6,259 | 72.1% |
| **Tickers SIN dividends** | 5,956 | 68.6% |

**Interpretaci√≥n**:
- ‚úÖ Es NORMAL que solo ~30% de small caps tengan splits/dividends
- ‚úÖ Small caps reinvierten capital en lugar de pagar dividends
- ‚úÖ 64.4% del universo son inactivos (delisted antes de events corporativos)

### Distribuci√≥n Temporal

**Splits por d√©cada:**
- 2000s: 926 splits
- 2010s: 1,308 splits
- 2020s: 1,778 splits

**Dividends por d√©cada:**
- 2000s: 24,795 dividends
- 2010s: 48,649 dividends (pico)
- 2020s: 21,102 dividends

### Top Performers

**Tickers con m√°s splits:**
1. CZFS - 22 splits
2. EEQ - 19 splits
3. VGR - 17 splits
4. UBFO - 15 splits
5. HWBK - 15 splits

**Tickers con m√°s dividends:**
1. GOOD - 257 dividends
2. LTC - 255 dividends
3. SBR - 250 dividends
4. PBT - 247 dividends
5. CRT - 247 dividends

### Archivos Generados

**Ubicaci√≥n**: `processed/corporate_actions/`

1. **`splits_universe_2025-10-24.parquet`** (182 KB)
   - 4,012 splits hist√≥ricos
   - Per√≠odo: 1978-2025
   - 2,420 tickers √∫nicos

2. **`dividends_universe_2025-10-24.parquet`** (4.11 MB)
   - 94,546 dividends hist√≥ricos
   - Per√≠odo: 2000-2030
   - 2,723 tickers √∫nicos

3. **`corporate_actions_lookup_2025-10-24.parquet`** (135 KB)
   - Lookup table: ticker ‚Üí has_splits, has_dividends
   - Campos: ticker, name, active, has_splits, has_dividends, splits_count, dividends_count
   - √ötil para feature engineering y an√°lisis exploratorio

### Uso de los Datos Filtrados

```python
import polars as pl

# Cargar splits filtrados
splits = pl.read_parquet('processed/corporate_actions/splits_universe_2025-10-24.parquet')

# Cargar dividends filtrados
dividends = pl.read_parquet('processed/corporate_actions/dividends_universe_2025-10-24.parquet')

# Cargar lookup table
lookup = pl.read_parquet('processed/corporate_actions/corporate_actions_lookup_2025-10-24.parquet')

# Ejemplo: Encontrar tickers con m√∫ltiples reverse splits (diluci√≥n extrema)
reverse_splits = splits.filter(pl.col('ratio') < 1.0)
tickers_dilution = (reverse_splits
    .group_by('ticker')
    .agg(pl.count('ticker').alias('reverse_split_count'))
    .filter(pl.col('reverse_split_count') >= 5)
)
print(f"Tickers con 5+ reverse splits (diluci√≥n extrema): {len(tickers_dilution)}")
```

### Ventajas del Filtrado

1. ‚úÖ **Tama√±o reducido**: 4 MB vs 1.8 GB en dividends (99.7% reducci√≥n en disco)
2. ‚úÖ **Carga m√°s r√°pida**: Solo datos relevantes para nuestro universo
3. ‚úÖ **Lookup table**: Acceso O(1) para verificar si ticker tiene corporate actions
4. ‚úÖ **Listo para ajustes de precio**: Splits para backward adjustment en OHLCV
5. ‚úÖ **Feature engineering**: Reverse splits count como proxy de diluci√≥n extrema

### Siguiente Paso

Con splits y dividends filtrados, podemos proceder a:
- ‚úÖ **Fase A - Universo**: COMPLETADA
- ‚è≠Ô∏è **Fase B - Ingesta Daily/Minute**: Con ajustes de precio usando splits
- ‚è≠Ô∏è **Feature Engineering**: Reverse splits count, dividend yield, etc.

**Status**: ‚úÖ Splits & Dividends filtrados y listos para uso