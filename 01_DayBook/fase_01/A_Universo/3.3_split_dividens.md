# 2) Splits & Dividends (landing + limpieza básica)

```python
#!/usr/bin/env python
# ingest_splits_dividends.py
import os, sys, time, argparse, datetime as dt
from pathlib import Path
import requests, polars as pl

BASE_URL = "https://api.polygon.io"
LIMIT = 1000
TIMEOUT = 25

def log(msg): print(f"[{dt.datetime.now():%F %T}] {msg}", flush=True)

def http_get(url, api_key, params=None):
    headers = {"Authorization": f"Bearer {api_key}"}
    for k in range(8):
        try:
            r = requests.get(url, headers=headers, params=params or {}, timeout=TIMEOUT)
            if r.status_code == 429:
                time.sleep(int(r.headers.get("Retry-After", "2")))
                continue
            if 500 <= r.status_code < 600:
                time.sleep(1.6 ** k)
                continue
            r.raise_for_status()
            return r.json()
        except Exception:
            time.sleep(1.6 ** k)
    return {}

def fetch_paged(path, api_key, extra_params=None):
    url = f"{BASE_URL}{path}"
    params = {"limit": LIMIT}
    if extra_params: params.update(extra_params)
    cursor = None
    total = 0
    while True:
        p = params.copy()
        if cursor: p["cursor"] = cursor
        data = http_get(url, api_key, p) or {}
        res = data.get("results") or []
        for x in res: yield x
        total += len(res)
        cursor = data.get("next_url_cursor") or data.get("cursor") or data.get("next_cursor")
        if not cursor: break
    log(f"{path}: {total:,} filas")

def clean_splits(df: pl.DataFrame) -> pl.DataFrame:
    if df.height == 0: return df
    cast = {
        "ticker": pl.Utf8, "execution_date": pl.Utf8,
        "split_from": pl.Float64, "split_to": pl.Float64,
        "declared_date": pl.Utf8
    }
    for c,t in cast.items():
        if c in df.columns: df = df.with_columns(pl.col(c).cast(t))
    if all(c in df.columns for c in ("split_from","split_to")):
        df = df.with_columns((pl.col("split_from")/pl.col("split_to")).alias("ratio"))
    if "execution_date" in df.columns:
        df = df.sort(["ticker","execution_date"]).unique(subset=["ticker","execution_date","split_from","split_to"], keep="last")
    return df

def clean_dividends(df: pl.DataFrame) -> pl.DataFrame:
    if df.height == 0: return df
    cast = {
        "ticker": pl.Utf8, "ex_dividend_date": pl.Utf8,
        "cash_amount": pl.Float64, "declaration_date": pl.Utf8,
        "record_date": pl.Utf8, "payable_date": pl.Utf8,
        "frequency": pl.Utf8, "dividend_type": pl.Utf8
    }
    for c,t in cast.items():
        if c in df.columns: df = df.with_columns(pl.col(c).cast(t))
    if "ex_dividend_date" in df.columns and "cash_amount" in df.columns:
        df = df.sort(["ticker","ex_dividend_date"]).unique(subset=["ticker","ex_dividend_date","cash_amount"], keep="last")
    return df

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--outdir", required=True, help="Base de salida raw/polygon/reference")
    args = ap.parse_args()

    api_key = os.getenv("POLYGON_API_KEY")
    if not api_key: sys.exit("Falta POLYGON_API_KEY")

    base = Path(args.outdir); base.mkdir(parents=True, exist_ok=True)

    # Splits
    splits = list(fetch_paged("/v3/reference/splits", api_key))
    df_s = clean_splits(pl.from_dicts(splits) if splits else pl.DataFrame())
    if df_s.height:
        # particiona por año de execution_date
        df_s = df_s.with_columns(pl.col("execution_date").str.slice(0,4).alias("year"))
        for year, part in df_s.group_by("year"):
            outdir = base / "splits" / f"year={year}"
            outdir.mkdir(parents=True, exist_ok=True)
            part.drop("year").write_parquet(outdir / "splits.parquet")
        log(f"Splits escritos en {base/'splits'}")

    # Dividends
    dividends = list(fetch_paged("/v3/reference/dividends", api_key))
    df_d = clean_dividends(pl.from_dicts(dividends) if dividends else pl.DataFrame())
    if df_d.height:
        df_d = df_d.with_columns(pl.col("ex_dividend_date").str.slice(0,4).alias("year"))
        for year, part in df_d.group_by("year"):
            outdir = base / "dividends" / f"year={year}"
            outdir.mkdir(parents=True, exist_ok=True)
            part.drop("year").write_parquet(outdir / "dividends.parquet")
        log(f"Dividends escritos en {base/'dividends'}")

if __name__ == "__main__":
    main()
```

**Ejemplo:**

```bash
python ingest_splits_dividends.py --outdir raw/polygon/reference
```

---

# 3) Construcción de `tickers_dim` (SCD-2)

Este script compara **dos snapshots** consecutivos y actualiza la dimensión con ventanas `effective_from / effective_to`. Si aún no tienes dimensión, la crea con el snapshot actual.

```python
#!/usr/bin/env python
# build_tickers_dim_scd2.py
import argparse, datetime as dt
from pathlib import Path
import polars as pl

KEY_COLS = ["ticker"]  # business key
TRACK_COLS = [
  "name","primary_exchange","active","market","locale","type",
  "currency_name","composite_figi","share_class_figi",
  "sector","industry","sic_code","cik","list_date","delisted_utc"
]

def log(m): print(f"[{dt.datetime.now():%F %T}] {m}", flush=True)

def load_snapshot(snapdir: Path) -> pl.DataFrame:
    df = pl.read_parquet(snapdir / "tickers.parquet")
    # aseguramos que las columnas existen
    for c in TRACK_COLS:
        if c not in df.columns: df = df.with_columns(pl.lit(None).alias(c))
    return df

def initial_dim(snapshot: pl.DataFrame) -> pl.DataFrame:
    snap_date = snapshot["snapshot_date"][0]
    return (snapshot
        .select(KEY_COLS + TRACK_COLS + ["snapshot_date"])
        .with_columns([
            pl.col("snapshot_date").alias("effective_from"),
            pl.lit(None, dtype=pl.Utf8).alias("effective_to")
        ])
        .drop("snapshot_date")
    )

def scd2_merge(dim: pl.DataFrame, prev_snap: pl.DataFrame, curr_snap: pl.DataFrame) -> pl.DataFrame:
    # join prev vs curr por KEY_COLS
    on = KEY_COLS
    prev = prev_snap.select(KEY_COLS + TRACK_COLS + ["snapshot_date"]).rename({c:f"prev_{c}" for c in TRACK_COLS+["snapshot_date"]})
    curr = curr_snap.select(KEY_COLS + TRACK_COLS + ["snapshot_date"]).rename({c:f"curr_{c}" for c in TRACK_COLS+["snapshot_date"]})
    j = prev.join(curr, on=on, how="outer")

    # detecta cambios (cualquiera de TRACK_COLS)
    def any_change(row) -> bool:
        for c in TRACK_COLS:
            if row[f"prev_{c}"] != row[f"curr_{c}"]:
                return True
        return False

    # filas que nacen en curr pero no estaban en prev → nuevas altas
    new_mask = j["prev_name"].is_null() & j["curr_name"].is_not_null()
    new_rows = (j.filter(new_mask)
                  .select(on + [f"curr_{c}" for c in TRACK_COLS] + ["curr_snapshot_date"])
                  .rename({f"curr_{c}": c for c in TRACK_COLS} | {"curr_snapshot_date":"effective_from"})
                  .with_columns(pl.lit(None, dtype=pl.Utf8).alias("effective_to")))

    # filas que estaban en prev y cambian en curr → cerrar antiguo y abrir nuevo
    change_mask = (~j["prev_name"].is_null()) & (~j["curr_name"].is_null())
    changed = j.filter(change_mask)

    # registros con cambio en TRACK_COLS
    changed = changed.with_columns(
        pl.any_horizontal([pl.col(f"prev_{c}") != pl.col(f"curr_{c}") for c in TRACK_COLS]).alias("changed")
    )

    # cerrar antiguos
    to_close = (changed.filter(pl.col("changed"))
                .select(on + ["prev_snapshot_date"])
                .rename({"prev_snapshot_date":"effective_to"}))

    # abrir nuevos
    to_open = (changed.filter(pl.col("changed"))
               .select(on + [f"curr_{c}" for c in TRACK_COLS] + ["curr_snapshot_date"])
               .rename({f"curr_{c}": c for c in TRACK_COLS} | {"curr_snapshot_date":"effective_from"})
               .with_columns(pl.lit(None, dtype=pl.Utf8).alias("effective_to")))

    # aplicar cierre a dim existente (match por key y effective_to is null)
    if to_close.height:
        dim_open = dim.filter(pl.col("effective_to").is_null())
        dim_closed = dim.filter(pl.col("effective_to").is_not_null())
        dim_open = dim_open.join(to_close, on=on, how="left")
        dim_open = dim_open.with_columns(
            pl.when(pl.col("effective_to").is_null() & pl.col("effective_to_right").is_not_null())
              .then(pl.col("effective_to_right"))
              .otherwise(pl.col("effective_to")).alias("effective_to")
        ).drop("effective_to_right")
        dim = pl.concat([dim_closed, dim_open], how="vertical_relaxed")

    # añadir nuevos y nuevas altas
    additions = pl.concat([to_open, new_rows], how="vertical_relaxed") if (to_open.height or new_rows.height) else None
    if additions is not None and additions.height:
        dim = pl.concat([dim, additions], how="vertical_relaxed")

    return dim.sort(on + ["effective_from"])

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--dimdir", required=True, help="Salida: processed/ref/tickers_dim")
    ap.add_argument("--prev-snapshot", required=False, help="raw/polygon/reference/tickers_snapshot/snapshot_date=YYYY-MM-DD (opcional la 1ª vez)")
    ap.add_argument("--curr-snapshot", required=True, help="raw/polygon/reference/tickers_snapshot/snapshot_date=YYYY-MM-DD")
    args = ap.parse_args()

    dimdir = Path(args.dimdir); dimdir.mkdir(parents=True, exist_ok=True)

    curr = load_snapshot(Path(args.curr_snapshot))
    if args.prev_snapshot:
        prev = load_snapshot(Path(args.prev_snapshot))
        # cargar dim si existe; si no, iníciala con prev
        dim_path = dimdir / "tickers_dim.parquet"
        if dim_path.exists():
            dim = pl.read_parquet(dim_path)
        else:
            dim = initial_dim(prev)
        dim = scd2_merge(dim, prev, curr)
    else:
        dim = initial_dim(curr)

    out = dimdir / "tickers_dim.parquet"
    dim.write_parquet(out)
    print(f"Escrito: {out} ({dim.height:,} filas)")

if __name__ == "__main__":
    from pathlib import Path
    main()
```

**Ejemplos:**

```bash
# 1ª vez (sin snapshot previo)
python build_tickers_dim_scd2.py \
  --dimdir processed/ref/tickers_dim \
  --curr-snapshot raw/polygon/reference/tickers_snapshot/snapshot_date=2025-10-19

# Siguientes días (con snapshots N-1 y N)
python build_tickers_dim_scd2.py \
  --dimdir processed/ref/tickers_dim \
  --prev-snapshot raw/polygon/reference/tickers_snapshot/snapshot_date=2025-10-18 \
  --curr-snapshot raw/polygon/reference/tickers_snapshot/snapshot_date=2025-10-19
```